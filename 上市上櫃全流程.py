#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°ç£è‚¡å¸‚è³‡æ–™å®Œæ•´è™•ç†æµç¨‹ - GitHub Actions ç‰ˆ
"""

import os
import glob
import shutil
import requests
import pandas as pd
import numpy as np
import time
from datetime import datetime, timedelta
from io import StringIO
import re
from openpyxl import load_workbook
from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# åŸºç¤ç›®éŒ„
BASE_DIR = os.getcwd()

def delete_folders(folder_names):
    """åˆªé™¤æŒ‡å®šçš„è³‡æ–™å¤¾"""
    print(f"\n{'='*80}")
    print("æ¸…ç†è³‡æ–™å¤¾...")
    print(f"{'='*80}")
    for folder_name in folder_names:
        folder_path = os.path.join(BASE_DIR, folder_name)
        if os.path.exists(folder_path):
            try:
                shutil.rmtree(folder_path)
                print(f"âœ“ å·²åˆªé™¤: {folder_name}")
            except Exception as e:
                print(f"âœ— åˆªé™¤å¤±æ•— {folder_name}: {e}")
        else:
            print(f"âŠ˜ è³‡æ–™å¤¾ä¸å­˜åœ¨: {folder_name}")
    print(f"{'='*80}\n")


# ============================================================================
# ç¬¬ä¸€æ­¥:çˆ¬èŸ²ç¨‹å¼
# ============================================================================

def filter_csv_content(csv_bytes):
    """éæ¿¾ CSV å…§å®¹,åªä¿ç•™è‚¡ç¥¨è³‡æ–™"""
    try:
        content = csv_bytes.decode('cp950')
        lines = content.split('\r\n')

        filtered_lines = []
        header_found = False
        stock_count = 0

        for line in lines:
            if 'è­‰åˆ¸ä»£è™Ÿ' in line and not header_found:
                filtered_lines.append(line)
                header_found = True
                continue

            if header_found:
                match = re.match(r'^=?"?(\d{4})"?', line)
                if match:
                    filtered_lines.append(line)
                    stock_count += 1

        filtered_content = '\r\n'.join(filtered_lines)
        filtered_bytes = filtered_content.encode('cp950')
        print(f"   âœ‚ï¸  éæ¿¾å®Œæˆ:ä¿ç•™ {stock_count} æª”è‚¡ç¥¨")
        return filtered_bytes

    except Exception as e:
        print(f"   âš ï¸  éæ¿¾å¤±æ•—: {e},å°‡å„²å­˜åŸå§‹è³‡æ–™")
        return csv_bytes

def download_twse_daily(date_str):
    """ä¸‹è¼‰ä¸Šå¸‚æ¯æ—¥äº¤æ˜“è³‡æ–™"""
    if '-' in date_str:
        date_str = date_str.replace('-', '')

    url = f"https://www.twse.com.tw/rwd/zh/afterTrading/MI_INDEX?date={date_str}&type=ALL&response=csv"

    try:
        response = requests.get(url, timeout=30)
        if response.status_code == 200 and len(response.content) > 100:
            return response.content
        return None
    except Exception as e:
        print(f"   âŒ ä¸‹è¼‰éŒ¯èª¤: {e}")
        return None

def crawl_twse_daily(start_date, end_date, save_dir):
    """æŠ“å–ä¸Šå¸‚æ¯æ—¥äº¤æ˜“è³‡æ–™"""
    print("="*60)
    print("ğŸ“Š [1/4] ä¸Šå¸‚æ¯æ—¥äº¤æ˜“è³‡æ–™ (TWSE Daily)")
    print("="*60)

    os.makedirs(save_dir, exist_ok=True)

    missing_dates = []
    curr = end_date

    # å¾ä»Šå¤©å¾€å›æª¢æŸ¥
    while curr >= start_date:
        if curr.weekday() < 5:  # åªæª¢æŸ¥å¹³æ—¥
            date_formatted = curr.strftime('%Y-%m-%d')
            file_path = os.path.join(save_dir, f'{date_formatted}.csv')

            if os.path.exists(file_path):
                print(f"  {date_formatted}... [å·²å­˜åœ¨,åœæ­¢æª¢æŸ¥] âœ“")
                break
            else:
                missing_dates.append(curr)

        curr -= timedelta(days=1)

    if not missing_dates:
        print("âœ“ ç„¡ç¼ºå¤±è³‡æ–™\n")
        return 0

    print(f"éœ€è¦ä¸‹è¼‰ {len(missing_dates)} å€‹äº¤æ˜“æ—¥")
    print("-"*60)

    success_count = 0

    for idx, date_dt in enumerate(missing_dates, 1):
        date_str = date_dt.strftime('%Y%m%d')
        date_formatted = date_dt.strftime('%Y-%m-%d')
        file_path = os.path.join(save_dir, f'{date_formatted}.csv')

        print(f"  [{idx:2d}/{len(missing_dates)}] {date_formatted}...", end='', flush=True)

        csv_bytes = download_twse_daily(date_str)

        if csv_bytes:
            filtered_bytes = filter_csv_content(csv_bytes)
            with open(file_path, 'wb') as f:
                f.write(filtered_bytes)
            print(" âœ“")
            success_count += 1
        else:
            print(" âœ—")

        time.sleep(1)

    print(f"âœ“ æˆåŠŸä¸‹è¼‰: {success_count} å€‹æª”æ¡ˆ\n")
    return success_count

# ============================================================================
# 2. ä¸Šå¸‚ä¸‰å¤§æ³•äººè²·è³£è¶… (TWSE Institutional)
# ============================================================================

def download_twse_institutional(date_str):
    """ä¸‹è¼‰ä¸Šå¸‚ä¸‰å¤§æ³•äººè³‡æ–™"""
    url = 'https://www.twse.com.tw/rwd/zh/fund/T86'
    params = {'date': date_str, 'selectType': 'ALL', 'response': 'json'}
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}

    try:
        response = requests.get(url, params=params, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()

        if data.get('stat') == 'OK' and 'data' in data:
            return pd.DataFrame(data['data'], columns=data['fields'])
        return None
    except Exception as e:
        print(f"   âŒ éŒ¯èª¤: {e}")
        return None

def crawl_twse_institutional(start_date, end_date, save_dir):
    """æŠ“å–ä¸Šå¸‚ä¸‰å¤§æ³•äººè²·è³£è¶…è³‡æ–™"""
    print("="*60)
    print("ğŸ“Š [2/4] ä¸Šå¸‚ä¸‰å¤§æ³•äººè²·è³£è¶… (TWSE Institutional)")
    print("="*60)

    os.makedirs(save_dir, exist_ok=True)

    missing_dates = []
    curr = end_date

    while curr >= start_date:
        if curr.weekday() < 5:
            date_formatted = curr.strftime('%Y-%m-%d')
            file_path = os.path.join(save_dir, f'{date_formatted}.csv')

            if os.path.exists(file_path):
                print(f"  {date_formatted}... [å·²å­˜åœ¨,åœæ­¢æª¢æŸ¥] âœ“")
                break
            else:
                missing_dates.append(curr)

        curr -= timedelta(days=1)

    if not missing_dates:
        print("âœ“ ç„¡ç¼ºå¤±è³‡æ–™\n")
        return 0

    print(f"éœ€è¦ä¸‹è¼‰ {len(missing_dates)} å€‹äº¤æ˜“æ—¥")
    print("-"*60)

    success_count = 0

    for idx, date_dt in enumerate(missing_dates, 1):
        date_str = date_dt.strftime('%Y%m%d')
        date_formatted = date_dt.strftime('%Y-%m-%d')
        file_path = os.path.join(save_dir, f'{date_formatted}.csv')

        print(f"  [{idx:2d}/{len(missing_dates)}] {date_formatted}...", end='', flush=True)

        df = download_twse_institutional(date_str)

        if df is not None and not df.empty:
            df.to_csv(file_path, index=False, encoding='utf-8-sig')
            print(" âœ“")
            success_count += 1
        else:
            print(" âœ—")

        time.sleep(3)

    print(f"âœ“ æˆåŠŸä¸‹è¼‰: {success_count} å€‹æª”æ¡ˆ\n")
    return success_count

# ============================================================================
# 3. ä¸Šæ«ƒæ¯æ—¥äº¤æ˜“è³‡æ–™ (OTC Daily)
# ============================================================================

def process_otc_daily_columns(df):
    """è™•ç†ä¸Šæ«ƒæ¯æ—¥äº¤æ˜“è³‡æ–™æ¬„ä½"""
    rename_mapping = {
        'ä»£è™Ÿ': 'è­‰åˆ¸ä»£è™Ÿ',
        'åç¨±': 'è­‰åˆ¸åç¨±',
        'æ”¶ç›¤': 'æ”¶ç›¤åƒ¹',
        'é–‹ç›¤': 'é–‹ç›¤åƒ¹',
        'æœ€é«˜': 'æœ€é«˜åƒ¹',
        'æœ€ä½': 'æœ€ä½åƒ¹',
        'æˆäº¤è‚¡æ•¸': 'æˆäº¤è‚¡æ•¸',
        'æˆäº¤ç­†æ•¸': 'æˆäº¤ç­†æ•¸',
        'æˆäº¤é‡‘é¡(å…ƒ)': 'æˆäº¤é‡‘é¡',
        'æ¼²è·Œ': 'æ¼²è·Œåƒ¹å·®',
        'æœ€å¾Œè²·åƒ¹': 'æœ€å¾Œæ­ç¤ºè²·åƒ¹',
        'æœ€å¾Œè²·é‡(åƒè‚¡)': 'æœ€å¾Œæ­ç¤ºè²·é‡',
        'æœ€å¾Œè³£åƒ¹': 'æœ€å¾Œæ­ç¤ºè³£åƒ¹',
        'æœ€å¾Œè³£é‡(åƒè‚¡)': 'æœ€å¾Œæ­ç¤ºè³£é‡'
    }

    df = df.rename(columns=rename_mapping)

    numeric_columns = ['æ”¶ç›¤åƒ¹', 'é–‹ç›¤åƒ¹', 'æœ€é«˜åƒ¹', 'æœ€ä½åƒ¹',
                      'æˆäº¤è‚¡æ•¸', 'æˆäº¤ç­†æ•¸', 'æˆäº¤é‡‘é¡']

    for col in numeric_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce')

    if 'æ¼²è·Œåƒ¹å·®' in df.columns:
        def parse_change(val):
            if pd.isna(val) or val == '':
                return 0
            val_str = str(val).replace(',', '').strip()
            if val_str == '-' or val_str == 'é™¤æ¬Šæ¯' or val_str == 'é™¤æ¯' or val_str == 'é™¤æ¬Š':
                return 0
            try:
                return float(val_str)
            except:
                return 0

        df['æ¼²è·Œåƒ¹å·®'] = df['æ¼²è·Œåƒ¹å·®'].apply(parse_change)

    return df

def download_otc_daily(date_str, max_retries=3):
    """ä¸‹è¼‰ä¸Šæ«ƒæ¯æ—¥äº¤æ˜“è³‡æ–™"""
    if '-' in date_str:
        date_str = date_str.replace('-', '')

    minguo_year = str(int(date_str[:4]) - 1911)
    date_formatted = f"{minguo_year}/{date_str[4:6]}/{date_str[6:8]}"
    url = f"https://www.tpex.org.tw/web/stock/aftertrading/otc_quotes_no1430/stk_wn1430_result.php?l=zh-tw&d={date_formatted}"

    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=30)
            if response.status_code == 200:
                data = response.json()
                if 'aaData' in data and len(data['aaData']) > 0:
                    df = pd.DataFrame(data['aaData'])
                    if len(df.columns) >= 14:
                        df.columns = ['ä»£è™Ÿ', 'åç¨±', 'æ”¶ç›¤', 'æ¼²è·Œ', 'é–‹ç›¤', 'æœ€é«˜', 'æœ€ä½',
                                    'æˆäº¤è‚¡æ•¸', 'æˆäº¤é‡‘é¡(å…ƒ)', 'æˆäº¤ç­†æ•¸', 'æœ€å¾Œè²·åƒ¹',
                                    'æœ€å¾Œè²·é‡(åƒè‚¡)', 'æœ€å¾Œè³£åƒ¹', 'æœ€å¾Œè³£é‡(åƒè‚¡)']
                        return process_otc_daily_columns(df)
            return None
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
            print(f"   âŒ éŒ¯èª¤: {e}")
            return None

def crawl_otc_daily(start_date, end_date, save_dir):
    """æŠ“å–ä¸Šæ«ƒæ¯æ—¥äº¤æ˜“è³‡æ–™"""
    print("="*60)
    print("ğŸ“Š [3/4] ä¸Šæ«ƒæ¯æ—¥äº¤æ˜“è³‡æ–™ (OTC Daily)")
    print("="*60)

    os.makedirs(save_dir, exist_ok=True)

    missing_dates = []
    curr = end_date

    while curr >= start_date:
        if curr.weekday() < 5:
            date_formatted = curr.strftime('%Y-%m-%d')
            file_path = os.path.join(save_dir, f'{date_formatted}.csv')

            if os.path.exists(file_path):
                print(f"  {date_formatted}... [å·²å­˜åœ¨,åœæ­¢æª¢æŸ¥] âœ“")
                break
            else:
                missing_dates.append(curr)

        curr -= timedelta(days=1)

    if not missing_dates:
        print("âœ“ ç„¡ç¼ºå¤±è³‡æ–™\n")
        return 0

    print(f"éœ€è¦ä¸‹è¼‰ {len(missing_dates)} å€‹äº¤æ˜“æ—¥")
    print("-"*60)

    success_count = 0

    for idx, date_dt in enumerate(missing_dates, 1):
        date_str = date_dt.strftime('%Y%m%d')
        date_formatted = date_dt.strftime('%Y-%m-%d')
        file_path = os.path.join(save_dir, f'{date_formatted}.csv')

        print(f"  [{idx:2d}/{len(missing_dates)}] {date_formatted}...", end='', flush=True)

        df = download_otc_daily(date_str)

        if df is not None and not df.empty:
            df.to_csv(file_path, index=False, encoding='utf-8-sig')
            print(" âœ“")
            success_count += 1
        else:
            print(" âœ—")

        time.sleep(3)

    print(f"âœ“ æˆåŠŸä¸‹è¼‰: {success_count} å€‹æª”æ¡ˆ\n")
    return success_count

# ============================================================================
# 4. ä¸Šæ«ƒä¸‰å¤§æ³•äººè²·è³£è¶… (OTC Institutional)
# ============================================================================

def download_otc_institutional(date_str):
    """ä¸‹è¼‰ä¸Šæ«ƒä¸‰å¤§æ³•äººè³‡æ–™"""
    if '-' in date_str:
        date_str = date_str.replace('-', '')

    minguo_year = str(int(date_str[:4]) - 1911)
    date_formatted = f"{minguo_year}/{date_str[4:6]}/{date_str[6:8]}"
    url = "https://www.tpex.org.tw/web/stock/3insti/daily_trade/3itrade_hedge_result.php"
    params = {'l': 'zh-tw', 'd': date_formatted, 't': 'D'}
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}

    try:
        response = requests.get(url, params=params, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()

        if 'aaData' in data and len(data['aaData']) > 0:
            df = pd.DataFrame(data['aaData'])
            columns = ['ä»£è™Ÿ', 'åç¨±', 'å¤–è³‡åŠé™¸è³‡(ä¸å«å¤–è³‡è‡ªç‡Ÿå•†)-è²·é€²è‚¡æ•¸', 'å¤–è³‡åŠé™¸è³‡(ä¸å«å¤–è³‡è‡ªç‡Ÿå•†)-è³£å‡ºè‚¡æ•¸',
                      'å¤–è³‡åŠé™¸è³‡(ä¸å«å¤–è³‡è‡ªç‡Ÿå•†)-è²·è³£è¶…è‚¡æ•¸', 'å¤–è³‡è‡ªç‡Ÿå•†-è²·é€²è‚¡æ•¸', 'å¤–è³‡è‡ªç‡Ÿå•†-è³£å‡ºè‚¡æ•¸',
                      'å¤–è³‡è‡ªç‡Ÿå•†-è²·è³£è¶…è‚¡æ•¸', 'æŠ•ä¿¡-è²·é€²è‚¡æ•¸', 'æŠ•ä¿¡-è³£å‡ºè‚¡æ•¸', 'æŠ•ä¿¡-è²·è³£è¶…è‚¡æ•¸',
                      'è‡ªç‡Ÿå•†-è²·é€²è‚¡æ•¸(è‡ªè¡Œè²·è³£)', 'è‡ªç‡Ÿå•†-è³£å‡ºè‚¡æ•¸(è‡ªè¡Œè²·è³£)', 'è‡ªç‡Ÿå•†-è²·è³£è¶…è‚¡æ•¸(è‡ªè¡Œè²·è³£)',
                      'è‡ªç‡Ÿå•†-è²·é€²è‚¡æ•¸(é¿éšª)', 'è‡ªç‡Ÿå•†-è³£å‡ºè‚¡æ•¸(é¿éšª)', 'è‡ªç‡Ÿå•†-è²·è³£è¶…è‚¡æ•¸(é¿éšª)',
                      'ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸']

            if len(df.columns) == len(columns):
                df.columns = columns
                return df

        return None
    except Exception as e:
        print(f"   âŒ éŒ¯èª¤: {e}")
        return None

def crawl_otc_institutional(start_date, end_date, save_dir):
    """æŠ“å–ä¸Šæ«ƒä¸‰å¤§æ³•äººè²·è³£è¶…è³‡æ–™"""
    print("="*60)
    print("ğŸ“Š [4/4] ä¸Šæ«ƒä¸‰å¤§æ³•äººè²·è³£è¶… (OTC Institutional)")
    print("="*60)

    os.makedirs(save_dir, exist_ok=True)

    missing_dates = []
    curr = end_date

    while curr >= start_date:
        if curr.weekday() < 5:
            date_formatted = curr.strftime('%Y-%m-%d')
            file_path = os.path.join(save_dir, f'{date_formatted}.csv')

            if os.path.exists(file_path):
                print(f"  {date_formatted}... [å·²å­˜åœ¨,åœæ­¢æª¢æŸ¥] âœ“")
                break
            else:
                missing_dates.append(curr)

        curr -= timedelta(days=1)

    if not missing_dates:
        print("âœ“ ç„¡ç¼ºå¤±è³‡æ–™\n")
        return 0

    print(f"éœ€è¦ä¸‹è¼‰ {len(missing_dates)} å€‹äº¤æ˜“æ—¥")
    print("-"*60)

    success_count = 0

    for idx, date_dt in enumerate(missing_dates, 1):
        date_str = date_dt.strftime('%Y%m%d')
        date_formatted = date_dt.strftime('%Y-%m-%d')
        file_path = os.path.join(save_dir, f'{date_formatted}.csv')

        print(f"  [{idx:2d}/{len(missing_dates)}] {date_formatted}...", end='', flush=True)

        df = download_otc_institutional(date_str)

        if df is not None and not df.empty:
            df.to_csv(file_path, index=False, encoding='utf-8-sig')
            print(" âœ“")
            success_count += 1
        else:
            print(" âœ—")

        time.sleep(3)

    print(f"âœ“ æˆåŠŸä¸‹è¼‰: {success_count} å€‹æª”æ¡ˆ\n")
    return success_count


# ============================================================================
# ç¬¬äºŒæ­¥:åˆ†æç¨‹å¼ - è¼”åŠ©å‡½æ•¸
# ============================================================================

def setup_config(market_type='TSE'):
    """è¨­å®šé…ç½®"""
    if market_type == 'TSE':
        return {
            'market_type': market_type,
            'market_name': 'ä¸Šå¸‚',
            'folder_path': os.path.join(BASE_DIR, 'StockShares'),
            'stock_daily_folder': os.path.join(BASE_DIR, 'StockDaily'),
            'history_folder': os.path.join(BASE_DIR, 'StockHistory'),
            'market_list_path': os.path.join(BASE_DIR, 'StockList', 'stockListTSE.csv'),
            'output_path': os.path.join(BASE_DIR, 'TSE.xlsx'),
            'sigma_threshold': 2,
            'aggregate_threshold': 10000,
            'show_top_n': None
        }
    else:  # OTC
        return {
            'market_type': market_type,
            'market_name': 'ä¸Šæ«ƒ',
            'folder_path': os.path.join(BASE_DIR, 'StockOTCShares'),
            'stock_daily_folder': os.path.join(BASE_DIR, 'StockOTCDaily'),
            'history_folder': os.path.join(BASE_DIR, 'StockOTCHistory'),
            'market_list_path': os.path.join(BASE_DIR, 'StockList', 'stockListOTC.csv'),
            'output_path': os.path.join(BASE_DIR, 'OTC.xlsx'),
            'sigma_threshold': 2,
            'aggregate_threshold': 10000,
            'show_top_n': None
        }

def load_stock_list(filepath):
    """è¼‰å…¥è‚¡ç¥¨æ¸…å–®"""
    try:
        df = pd.read_csv(filepath, encoding='utf-8-sig')
        allowed_stock_codes = set(df['ä»£è™Ÿ'].astype(str))

        stock_sector_map = dict(zip(df['ä»£è™Ÿ'].astype(str), df['ç”¢æ¥­åˆ¥']))

        etf_stock_codes = set()
        if 'ç”¢æ¥­åˆ¥' in df.columns:
            etf_df = df[df['ç”¢æ¥­åˆ¥'].str.contains('ETF', na=False)]
            etf_stock_codes = set(etf_df['ä»£è™Ÿ'].astype(str))

        return allowed_stock_codes, stock_sector_map, etf_stock_codes
    except Exception as e:
        print(f"âŒ ç„¡æ³•è¼‰å…¥è‚¡ç¥¨æ¸…å–®: {e}")
        return set(), {}, set()

def load_stock_daily_prices(folder_path, allowed_stock_codes):
    """è¼‰å…¥è‚¡ç¥¨æ¯æ—¥åƒ¹æ ¼"""
    stock_daily_prices = {}

    if not os.path.exists(folder_path):
        print(f"âš ï¸  æ¯æ—¥åƒ¹æ ¼è³‡æ–™å¤¾ä¸å­˜åœ¨: {folder_path}")
        return stock_daily_prices

    csv_files = glob.glob(os.path.join(folder_path, '*.csv'))

    if not csv_files:
        print(f"âš ï¸  æ‰¾ä¸åˆ°æ¯æ—¥åƒ¹æ ¼è³‡æ–™")
        return stock_daily_prices

    latest_file = max(csv_files, key=os.path.getmtime)

    try:
        df = pd.read_csv(latest_file, encoding='cp950', dtype=str)

        if 'è­‰åˆ¸ä»£è™Ÿ' in df.columns and 'æ”¶ç›¤åƒ¹' in df.columns:
            df = df[df['è­‰åˆ¸ä»£è™Ÿ'].isin(allowed_stock_codes)]
            df['æ”¶ç›¤åƒ¹'] = df['æ”¶ç›¤åƒ¹'].str.replace(',', '').str.replace('+', '').str.replace('-', '')
            df['æ”¶ç›¤åƒ¹'] = pd.to_numeric(df['æ”¶ç›¤åƒ¹'], errors='coerce')
            stock_daily_prices = dict(zip(df['è­‰åˆ¸ä»£è™Ÿ'], df['æ”¶ç›¤åƒ¹']))

    except Exception as e:
        print(f"âš ï¸  è®€å–æ¯æ—¥åƒ¹æ ¼å¤±æ•—: {e}")

    return stock_daily_prices

def get_latest_files(folder_path, num_files=61):
    """å–å¾—æœ€è¿‘çš„æª”æ¡ˆ"""
    csv_files = glob.glob(os.path.join(folder_path, '*.csv'))

    if not csv_files:
        return []

    csv_files_with_time = [(f, os.path.getmtime(f)) for f in csv_files]
    csv_files_with_time.sort(key=lambda x: x[1], reverse=True)
    latest_files = [f for f, _ in csv_files_with_time[:num_files]]

    return latest_files

def read_shares_file(filepath):
    """è®€å–å–®ä¸€ä¸‰å¤§æ³•äººæª”æ¡ˆ"""
    try:
        df = pd.read_csv(filepath, encoding='utf-8-sig')
        return df
    except Exception as e:
        print(f"âŒ è®€å–å¤±æ•— {os.path.basename(filepath)}: {e}")
        return pd.DataFrame()

def process_shares_files(latest_files, allowed_stock_codes, stock_daily_prices,
                        stock_sector_map, etf_stock_codes):
    """è™•ç†ä¸‰å¤§æ³•äººæª”æ¡ˆ"""
    all_data = []
    daily_buy_sell_data = {}
    etf_daily_data = {}
    buy_top20_tracker = {}
    sell_top20_tracker = {}
    daily_buy_stocks = {}
    daily_sell_stocks = {}
    daily_all_stocks = {}
    all_historical_data = {}
    statistics = {'processed': 0, 'skipped': 0}

    for filepath in latest_files:
        date_str = os.path.basename(filepath).replace('.csv', '')

        df = read_shares_file(filepath)

        if df.empty:
            statistics['skipped'] += 1
            continue

        if 'è­‰åˆ¸ä»£è™Ÿ' not in df.columns or 'ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸' not in df.columns:
            statistics['skipped'] += 1
            continue

        df = df[df['è­‰åˆ¸ä»£è™Ÿ'].isin(allowed_stock_codes)].copy()

        df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'] = df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'].astype(str).str.replace(',', '')
        df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'] = pd.to_numeric(df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'], errors='coerce')
        df = df.dropna(subset=['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'])

        df['ç”¢æ¥­åˆ¥'] = df['è­‰åˆ¸ä»£è™Ÿ'].map(stock_sector_map)
        df['æ”¶ç›¤åƒ¹'] = df['è­‰åˆ¸ä»£è™Ÿ'].map(stock_daily_prices)

        # è¨˜éŒ„æ‰€æœ‰è‚¡ç¥¨
        daily_all_stocks[date_str] = set(df['è­‰åˆ¸ä»£è™Ÿ'])

        # è²·è¶…
        buy_df = df[df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'] > 0].copy()
        daily_buy_stocks[date_str] = set(buy_df['è­‰åˆ¸ä»£è™Ÿ'])

        # ETF è²·è¶…
        etf_buy_df = buy_df[buy_df['è­‰åˆ¸ä»£è™Ÿ'].isin(etf_stock_codes)].copy()
        if not etf_buy_df.empty:
            etf_buy_df = etf_buy_df.sort_values('ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸', ascending=False).head(50)
            etf_daily_data[date_str] = {'buy': etf_buy_df}

        # é ETF è²·è¶…
        buy_df = buy_df[~buy_df['è­‰åˆ¸ä»£è™Ÿ'].isin(etf_stock_codes)]

        if not buy_df.empty:
            buy_df = buy_df.sort_values('ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸', ascending=False).head(50)
            daily_buy_sell_data[date_str] = {'buy': buy_df}

            top20_buy = set(buy_df.head(20)['è­‰åˆ¸ä»£è™Ÿ'])
            for stock_code in top20_buy:
                if stock_code not in buy_top20_tracker:
                    buy_top20_tracker[stock_code] = []
                buy_top20_tracker[stock_code].append(date_str)

        # è³£è¶…
        sell_df = df[df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'] < 0].copy()
        daily_sell_stocks[date_str] = set(sell_df['è­‰åˆ¸ä»£è™Ÿ'])

        sell_df = sell_df[~sell_df['è­‰åˆ¸ä»£è™Ÿ'].isin(etf_stock_codes)]

        if not sell_df.empty:
            sell_df = sell_df.sort_values('ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸').head(50)

            if date_str in daily_buy_sell_data:
                daily_buy_sell_data[date_str]['sell'] = sell_df
            else:
                daily_buy_sell_data[date_str] = {'sell': sell_df}

            top20_sell = set(sell_df.head(20)['è­‰åˆ¸ä»£è™Ÿ'])
            for stock_code in top20_sell:
                if stock_code not in sell_top20_tracker:
                    sell_top20_tracker[stock_code] = []
                sell_top20_tracker[stock_code].append(date_str)

        all_data.append(df)

        # æ­·å²è³‡æ–™
        for _, row in df.iterrows():
            stock_code = row['è­‰åˆ¸ä»£è™Ÿ']
            if stock_code not in all_historical_data:
                all_historical_data[stock_code] = []
            all_historical_data[stock_code].append(row['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'])

        statistics['processed'] += 1

    return (all_data, daily_buy_sell_data, etf_daily_data, buy_top20_tracker,
            sell_top20_tracker, daily_buy_stocks, daily_sell_stocks,
            daily_all_stocks, all_historical_data, statistics)

def calculate_stock_statistics(all_historical_data, sigma_threshold):
    """è¨ˆç®—è‚¡ç¥¨çµ±è¨ˆæ•¸æ“š"""
    stock_statistics = {}

    for stock_code, values in all_historical_data.items():
        mean = np.mean(values)
        std = np.std(values, ddof=1)

        stock_statistics[stock_code] = {
            'mean': mean,
            'std': std,
            'upper_threshold': mean + sigma_threshold * std,
            'lower_threshold': mean - sigma_threshold * std
        }

    return stock_statistics

def analyze_new_entries_and_observables(latest_file, daily_buy_stocks, daily_sell_stocks,
                                       daily_all_stocks, stock_statistics,
                                       allowed_stock_codes, sigma_threshold):
    """åˆ†ææ–°é€²æ¦œèˆ‡è§€å¯Ÿè‚¡"""
    latest_date = os.path.basename(latest_file).replace('.csv', '')
    sorted_dates = sorted(daily_buy_stocks.keys(), reverse=True)

    if len(sorted_dates) < 2:
        return set(), set(), set(), set(), latest_date, [], []

    latest_buy = daily_buy_stocks.get(sorted_dates[0], set())
    latest_sell = daily_sell_stocks.get(sorted_dates[0], set())

    previous_dates = sorted_dates[1:6]
    previous_buy = set()
    previous_sell = set()

    for date in previous_dates:
        previous_buy.update(daily_buy_stocks.get(date, set()))
        previous_sell.update(daily_sell_stocks.get(date, set()))

    new_buy_stocks = latest_buy - previous_buy
    new_sell_stocks = latest_sell - previous_sell

    # è§€å¯Ÿè‚¡
    observable_buy_stocks = set()
    observable_sell_stocks = set()

    df = read_shares_file(latest_file)
    if not df.empty and 'è­‰åˆ¸ä»£è™Ÿ' in df.columns and 'ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸' in df.columns:
        df = df[df['è­‰åˆ¸ä»£è™Ÿ'].isin(allowed_stock_codes)]
        df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'] = pd.to_numeric(
            df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'].astype(str).str.replace(',', ''),
            errors='coerce'
        )
        df = df.dropna(subset=['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'])

        for _, row in df.iterrows():
            stock_code = row['è­‰åˆ¸ä»£è™Ÿ']
            value = row['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸']

            if stock_code in stock_statistics:
                stats = stock_statistics[stock_code]
                if value > stats['upper_threshold'] and stock_code not in new_buy_stocks:
                    observable_buy_stocks.add(stock_code)
                elif value < stats['lower_threshold'] and stock_code not in new_sell_stocks:
                    observable_sell_stocks.add(stock_code)

    # è²·è¶…å‰ 50
    latest_buy_stocks_50 = []
    if not df.empty:
        buy_df = df[df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'] > 0].copy()
        buy_df = buy_df.sort_values('ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸', ascending=False).head(50)
        latest_buy_stocks_50 = buy_df['è­‰åˆ¸ä»£è™Ÿ'].tolist()

    # è³£è¶…å‰ 50
    latest_sell_stocks_50 = []
    if not df.empty:
        sell_df = df[df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'] < 0].copy()
        sell_df = sell_df.sort_values('ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸').head(50)
        latest_sell_stocks_50 = sell_df['è­‰åˆ¸ä»£è™Ÿ'].tolist()

    return (new_buy_stocks, new_sell_stocks, observable_buy_stocks, observable_sell_stocks,
            latest_date, latest_buy_stocks_50, latest_sell_stocks_50)

def collect_stock_history(stock_list, shares_folder, daily_folder, output_folder, allowed_stock_codes):
    """æ”¶é›†è‚¡ç¥¨æ­·å²è³‡æ–™"""
    os.makedirs(output_folder, exist_ok=True)

    for stock_code in stock_list:
        if stock_code not in allowed_stock_codes:
            continue

        output_file = os.path.join(output_folder, f"{stock_code}.csv")

        if os.path.exists(output_file):
            continue

        history_data = []
        shares_files = glob.glob(os.path.join(shares_folder, '*.csv'))

        for filepath in shares_files:
            date_str = os.path.basename(filepath).replace('.csv', '')

            try:
                df = pd.read_csv(filepath, encoding='utf-8-sig')

                if 'è­‰åˆ¸ä»£è™Ÿ' not in df.columns:
                    continue

                stock_df = df[df['è­‰åˆ¸ä»£è™Ÿ'] == stock_code]

                if stock_df.empty:
                    continue

                for _, row in stock_df.iterrows():
                    history_data.append({
                        'æ—¥æœŸ': date_str,
                        'è­‰åˆ¸ä»£è™Ÿ': stock_code,
                        'è­‰åˆ¸åç¨±': row.get('è­‰åˆ¸åç¨±', ''),
                        'ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸': row.get('ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸', 0)
                    })

            except Exception:
                continue

        # åŠ å…¥æ¯æ—¥åƒ¹æ ¼
        daily_files = glob.glob(os.path.join(daily_folder, '*.csv'))

        for filepath in daily_files:
            date_str = os.path.basename(filepath).replace('.csv', '')

            try:
                df = pd.read_csv(filepath, encoding='cp950', dtype=str)

                if 'è­‰åˆ¸ä»£è™Ÿ' not in df.columns or 'æ”¶ç›¤åƒ¹' not in df.columns:
                    continue

                stock_df = df[df['è­‰åˆ¸ä»£è™Ÿ'] == stock_code]

                if stock_df.empty:
                    continue

                close_price = stock_df['æ”¶ç›¤åƒ¹'].iloc[0]
                close_price = close_price.replace(',', '').replace('+', '').replace('-', '')

                try:
                    close_price = float(close_price)
                except:
                    close_price = None

                # æ›´æ–°æ­·å²è³‡æ–™
                for item in history_data:
                    if item['æ—¥æœŸ'] == date_str:
                        item['æ”¶ç›¤åƒ¹'] = close_price
                        break

            except Exception:
                continue

        if history_data:
            history_df = pd.DataFrame(history_data)
            history_df = history_df.sort_values('æ—¥æœŸ')
            history_df.to_csv(output_file, index=False, encoding='utf-8-sig')

def aggregate_analysis(buy_top20_tracker, sell_top20_tracker, stock_sector_map,
                      aggregate_threshold=10000, show_top_n=None):
    """å½™æ•´åˆ†æ"""
    # è²·è¶…
    buy_analysis = []

    for stock_code, dates in buy_top20_tracker.items():
        appearance_count = len(dates)
        sector = stock_sector_map.get(stock_code, 'æœªçŸ¥')

        buy_analysis.append({
            'è­‰åˆ¸ä»£è™Ÿ': stock_code,
            'ç”¢æ¥­åˆ¥': sector,
            'å‡ºç¾æ¬¡æ•¸': appearance_count
        })

    buy_stocks = pd.DataFrame(buy_analysis)

    if not buy_stocks.empty:
        buy_stocks = buy_stocks.sort_values('å‡ºç¾æ¬¡æ•¸', ascending=False)

        if show_top_n:
            buy_stocks = buy_stocks.head(show_top_n)

    # è³£è¶…
    sell_analysis = []

    for stock_code, dates in sell_top20_tracker.items():
        appearance_count = len(dates)
        sector = stock_sector_map.get(stock_code, 'æœªçŸ¥')

        sell_analysis.append({
            'è­‰åˆ¸ä»£è™Ÿ': stock_code,
            'ç”¢æ¥­åˆ¥': sector,
            'å‡ºç¾æ¬¡æ•¸': appearance_count
        })

    sell_stocks = pd.DataFrame(sell_analysis)

    if not sell_stocks.empty:
        sell_stocks = sell_stocks.sort_values('å‡ºç¾æ¬¡æ•¸', ascending=False)

        if show_top_n:
            sell_stocks = sell_stocks.head(show_top_n)

    # åŒæ™‚è²·è³£è¶…
    both_stocks_set = set()
    both_stocks_df = pd.DataFrame()

    if not buy_stocks.empty and not sell_stocks.empty:
        buy_set = set(buy_stocks['è­‰åˆ¸ä»£è™Ÿ'])
        sell_set = set(sell_stocks['è­‰åˆ¸ä»£è™Ÿ'])
        both_stocks_set = buy_set & sell_set

        if both_stocks_set:
            both_data = []

            for stock_code in both_stocks_set:
                buy_count = buy_stocks[buy_stocks['è­‰åˆ¸ä»£è™Ÿ'] == stock_code]['å‡ºç¾æ¬¡æ•¸'].values[0]
                sell_count = sell_stocks[sell_stocks['è­‰åˆ¸ä»£è™Ÿ'] == stock_code]['å‡ºç¾æ¬¡æ•¸'].values[0]
                sector = stock_sector_map.get(stock_code, 'æœªçŸ¥')

                both_data.append({
                    'è­‰åˆ¸ä»£è™Ÿ': stock_code,
                    'ç”¢æ¥­åˆ¥': sector,
                    'è²·è¶…æ¬¡æ•¸': buy_count,
                    'è³£è¶…æ¬¡æ•¸': sell_count
                })

            both_stocks_df = pd.DataFrame(both_data)
            both_stocks_df = both_stocks_df.sort_values('è²·è¶…æ¬¡æ•¸', ascending=False)

    return buy_stocks, sell_stocks, both_stocks_set, both_stocks_df

def export_to_excel(output_path, buy_stocks, sell_stocks, both_stocks_set, both_stocks_df,
                   daily_buy_sell_data, etf_daily_data, latest_date, new_buy_stocks,
                   new_sell_stocks, observable_buy_stocks, observable_sell_stocks,
                   stock_sector_map, etf_stock_codes):
    """åŒ¯å‡ºåˆ° Excel"""
    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
        # å·¥ä½œè¡¨ 1: å½™æ•´è²·è¶…
        if not buy_stocks.empty:
            buy_stocks.to_excel(writer, sheet_name='å½™æ•´è²·è¶…', index=False)

        # å·¥ä½œè¡¨ 2: å½™æ•´è³£è¶…
        if not sell_stocks.empty:
            sell_stocks.to_excel(writer, sheet_name='å½™æ•´è³£è¶…', index=False)

        # å·¥ä½œè¡¨ 3: åŒæ™‚è²·è³£è¶…
        if not both_stocks_df.empty:
            both_stocks_df.to_excel(writer, sheet_name='åŒæ™‚è²·è³£è¶…', index=False)

        # å·¥ä½œè¡¨ 4: æ¯æ—¥è²·è¶…å‰ 50
        sorted_dates = sorted(daily_buy_sell_data.keys(), reverse=True)
        all_daily_buy = []

        for date in sorted_dates:
            if 'buy' in daily_buy_sell_data[date]:
                df = daily_buy_sell_data[date]['buy'].copy()
                df.insert(0, 'æ—¥æœŸ', date)
                all_daily_buy.append(df)

        if all_daily_buy:
            combined_buy = pd.concat(all_daily_buy, ignore_index=True)
            combined_buy.to_excel(writer, sheet_name='æ¯æ—¥è²·è¶…å‰50', index=False)

        # å·¥ä½œè¡¨ 5: æ¯æ—¥è³£è¶…å‰ 50
        all_daily_sell = []

        for date in sorted_dates:
            if 'sell' in daily_buy_sell_data[date]:
                df = daily_buy_sell_data[date]['sell'].copy()
                df.insert(0, 'æ—¥æœŸ', date)
                all_daily_sell.append(df)

        if all_daily_sell:
            combined_sell = pd.concat(all_daily_sell, ignore_index=True)
            combined_sell.to_excel(writer, sheet_name='æ¯æ—¥è³£è¶…å‰50', index=False)

        # å·¥ä½œè¡¨ 6: ETF è²·è¶…å‰ 50
        sorted_etf_dates = sorted(etf_daily_data.keys(), reverse=True)
        all_etf_daily = []

        for date in sorted_etf_dates:
            if 'buy' in etf_daily_data[date]:
                df = etf_daily_data[date]['buy'].copy()
                df.insert(0, 'æ—¥æœŸ', date)
                all_etf_daily.append(df)

        if all_etf_daily:
            combined_etf = pd.concat(all_etf_daily, ignore_index=True)
            combined_etf.to_excel(writer, sheet_name='ETFè²·è¶…å‰50', index=False)

        # å·¥ä½œè¡¨ 7: æ–°é€²è²·è¶…æ¦œ
        if new_buy_stocks:
            new_buy_data = []

            latest_buy_df = daily_buy_sell_data.get(latest_date, {}).get('buy')

            if latest_buy_df is not None:
                for stock_code in new_buy_stocks:
                    stock_df = latest_buy_df[latest_buy_df['è­‰åˆ¸ä»£è™Ÿ'] == stock_code]

                    if not stock_df.empty:
                        new_buy_data.append(stock_df.iloc[0].to_dict())

            if new_buy_data:
                new_buy_df = pd.DataFrame(new_buy_data)
                new_buy_df = new_buy_df.sort_values('ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸', ascending=False)
                new_buy_df.to_excel(writer, sheet_name='æ–°é€²è²·è¶…æ¦œ', index=False)

        # å·¥ä½œè¡¨ 8: æ–°é€²è³£è¶…æ¦œ
        if new_sell_stocks:
            new_sell_data = []

            latest_sell_df = daily_buy_sell_data.get(latest_date, {}).get('sell')

            if latest_sell_df is not None:
                for stock_code in new_sell_stocks:
                    stock_df = latest_sell_df[latest_sell_df['è­‰åˆ¸ä»£è™Ÿ'] == stock_code]

                    if not stock_df.empty:
                        new_sell_data.append(stock_df.iloc[0].to_dict())

            if new_sell_data:
                new_sell_df = pd.DataFrame(new_sell_data)
                new_sell_df = new_sell_df.sort_values('ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸')
                new_sell_df.to_excel(writer, sheet_name='æ–°é€²è³£è¶…æ¦œ', index=False)

        # å·¥ä½œè¡¨ 9: è§€å¯Ÿè²·è¶…è‚¡
        if observable_buy_stocks:
            observable_buy_data = []

            latest_file_df = daily_buy_sell_data.get(latest_date, {}).get('buy')

            if latest_file_df is not None:
                for stock_code in observable_buy_stocks:
                    stock_df = latest_file_df[latest_file_df['è­‰åˆ¸ä»£è™Ÿ'] == stock_code]

                    if not stock_df.empty:
                        observable_buy_data.append(stock_df.iloc[0].to_dict())

            if observable_buy_data:
                observable_buy_df = pd.DataFrame(observable_buy_data)
                observable_buy_df = observable_buy_df.sort_values('ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸', ascending=False)
                observable_buy_df.to_excel(writer, sheet_name='è§€å¯Ÿè²·è¶…è‚¡', index=False)

        # å·¥ä½œè¡¨ 10: è§€å¯Ÿè³£è¶…è‚¡
        if observable_sell_stocks:
            observable_sell_data = []

            latest_file_df = daily_buy_sell_data.get(latest_date, {}).get('sell')

            if latest_file_df is not None:
                for stock_code in observable_sell_stocks:
                    stock_df = latest_file_df[latest_file_df['è­‰åˆ¸ä»£è™Ÿ'] == stock_code]

                    if not stock_df.empty:
                        observable_sell_data.append(stock_df.iloc[0].to_dict())

            if observable_sell_data:
                observable_sell_df = pd.DataFrame(observable_sell_data)
                observable_sell_df = observable_sell_df.sort_values('ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸')
                observable_sell_df.to_excel(writer, sheet_name='è§€å¯Ÿè³£è¶…è‚¡', index=False)

def beautify_excel(file_path):
    """ç¾åŒ– Excel"""
    wb = load_workbook(file_path)

    header_font = Font(bold=True, size=11, color='FFFFFF')
    header_fill = PatternFill(start_color='4472C4', end_color='4472C4', fill_type='solid')
    header_alignment = Alignment(horizontal='center', vertical='center')

    thin_border = Border(
        left=Side(style='thin'),
        right=Side(style='thin'),
        top=Side(style='thin'),
        bottom=Side(style='thin')
    )

    for sheet in wb.sheetnames:
        ws = wb[sheet]

        for cell in ws[1]:
            cell.font = header_font
            cell.fill = header_fill
            cell.alignment = header_alignment
            cell.border = thin_border

        for row in ws.iter_rows(min_row=2):
            for cell in row:
                cell.border = thin_border
                cell.alignment = Alignment(horizontal='left', vertical='center')

        for column in ws.columns:
            max_length = 0
            column_letter = column[0].column_letter

            for cell in column:
                if cell.value:
                    max_length = max(max_length, len(str(cell.value)))

            adjusted_width = min(max_length + 2, 50)
            ws.column_dimensions[column_letter].width = adjusted_width

    wb.save(file_path)


# ============================================================================
# ç¬¬ä¸‰æ­¥:åœ–è¡¨ç”Ÿæˆç¨‹å¼
# ============================================================================

class Config:
    """é…ç½®ç®¡ç†é¡åˆ¥"""

    OVERWRITE_EXISTING = True
    MARKET_TYPE = 'TSE'
    RUN_ALL = True

    FONT_PATH = None

    @staticmethod
    def setup_config(market_type='TSE'):
        """è¨­å®šæ‰€æœ‰è·¯å¾‘è®Šæ•¸"""
        base_path = BASE_DIR

        if market_type == 'TSE':
            config = {
                'market_type': market_type,
                'market_name': 'ä¸Šå¸‚',
                'history_folder': os.path.join(base_path, 'StockHistory'),
                'html_output_folder': os.path.join(base_path, 'StockHTML'),
                'png_output_folder': os.path.join(base_path, 'StockPNG'),
                'stocklist_folder': os.path.join(base_path, 'StockList'),
            }
        else:  # OTC
            config = {
                'market_type': market_type,
                'market_name': 'ä¸Šæ«ƒ',
                'history_folder': os.path.join(base_path, 'StockOTCHistory'),
                'html_output_folder': os.path.join(base_path, 'StockOTCHTML'),
                'png_output_folder': os.path.join(base_path, 'StockOTCPNG'),
                'stocklist_folder': os.path.join(base_path, 'StockList'),
            }

        os.makedirs(config['html_output_folder'], exist_ok=True)
        os.makedirs(config['png_output_folder'], exist_ok=True)

        print(f"{'='*80}")
        print(f"å¸‚å ´é¡å‹: {market_type} ({config['market_name']})")
        print(f"åœ–è¡¨æ ¼å¼: HTML + PNG (é›™æ ¼å¼è¼¸å‡º)")
        print(f"æ­·å²æ•¸æ“šè³‡æ–™å¤¾: {config['history_folder']}")
        print(f"HTMLè¼¸å‡ºè³‡æ–™å¤¾: {config['html_output_folder']}")
        print(f"PNGè¼¸å‡ºè³‡æ–™å¤¾: {config['png_output_folder']}")
        print(f"{'='*80}\n")

        return config


class Utils:
    """å·¥å…·å‡½æ•¸é¡åˆ¥"""

    @staticmethod
    def setup_chinese_font(base_dir):
        """è¨­å®šä¸­æ–‡å­—é«”"""
        font_path = os.path.join(base_dir, 'StockList', 'Font.ttf')

        if os.path.exists(font_path):
            Config.FONT_PATH = font_path
            print(f"âœ“ æ‰¾åˆ°å­—é«”æª”æ¡ˆ: {font_path}")
        else:
            print(f"âš  æ‰¾ä¸åˆ°å­—é«”æª”æ¡ˆ: {font_path}")
            print("  HTML åœ–è¡¨å°‡ä½¿ç”¨é è¨­å­—é«”")
            Config.FONT_PATH = None

        return Config.FONT_PATH

    @staticmethod
    def read_csv_auto_encoding(file_path):
        """è‡ªå‹•åµæ¸¬ç·¨ç¢¼è®€å– CSV"""
        encodings = ['utf-8-sig', 'utf-8', 'cp950', 'big5']

        for encoding in encodings:
            try:
                df = pd.read_csv(file_path, encoding=encoding)
                return df
            except:
                continue

        raise ValueError(f"ç„¡æ³•è®€å–æª”æ¡ˆ: {file_path}")


class ChartGenerator:
    """åœ–è¡¨ç”Ÿæˆé¡åˆ¥"""

    @staticmethod
    def create_chart(stock_code, stock_name, df, sector=""):
        """å»ºç«‹äº’å‹•å¼åœ–è¡¨"""
        if df.empty or len(df) < 2:
            print(f"  âš ï¸  è³‡æ–™ä¸è¶³,ç„¡æ³•ç”Ÿæˆåœ–è¡¨")
            return None

        fig = make_subplots(
            rows=2, cols=1,
            row_heights=[0.7, 0.3],
            vertical_spacing=0.08,
            subplot_titles=(
                f'{stock_code} {stock_name} - ä¸‰å¤§æ³•äººè²·è³£è¶… & è‚¡åƒ¹èµ°å‹¢',
                'ä¸‰å¤§æ³•äººè²·è³£è¶…é‡'
            ),
            specs=[[{"secondary_y": True}], [{"secondary_y": False}]]
        )

        # åœ– 1: è‚¡åƒ¹ç·šåœ–
        fig.add_trace(
            go.Scatter(
                x=df['æ—¥æœŸ'],
                y=df['æ”¶ç›¤åƒ¹'],
                name='æ”¶ç›¤åƒ¹',
                line=dict(color='#2E86DE', width=2),
                mode='lines+markers',
                marker=dict(size=4),
                hovertemplate='<b>æ—¥æœŸ</b>: %{x}<br><b>æ”¶ç›¤åƒ¹</b>: %{y:.2f}<extra></extra>'
            ),
            row=1, col=1, secondary_y=False
        )

        # åœ– 1: è²·è³£è¶…æŸ±ç‹€åœ–
        colors = ['#10AC84' if x > 0 else '#EE5A6F' for x in df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸']]

        fig.add_trace(
            go.Bar(
                x=df['æ—¥æœŸ'],
                y=df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'],
                name='è²·è³£è¶…',
                marker_color=colors,
                opacity=0.6,
                hovertemplate='<b>æ—¥æœŸ</b>: %{x}<br><b>è²·è³£è¶…</b>: %{y:,}<extra></extra>'
            ),
            row=1, col=1, secondary_y=True
        )

        # åœ– 2: è²·è³£è¶…æŸ±ç‹€åœ–
        fig.add_trace(
            go.Bar(
                x=df['æ—¥æœŸ'],
                y=df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'],
                name='è²·è³£è¶…',
                marker_color=colors,
                showlegend=False,
                hovertemplate='<b>æ—¥æœŸ</b>: %{x}<br><b>è²·è³£è¶…</b>: %{y:,}<extra></extra>'
            ),
            row=2, col=1
        )

        # æ›´æ–° Y è»¸
        fig.update_yaxes(title_text="è‚¡åƒ¹ (å…ƒ)", row=1, col=1, secondary_y=False)
        fig.update_yaxes(title_text="è²·è³£è¶… (å¼µ)", row=1, col=1, secondary_y=True)
        fig.update_yaxes(title_text="è²·è³£è¶… (å¼µ)", row=2, col=1)

        # æ›´æ–° X è»¸
        fig.update_xaxes(title_text="æ—¥æœŸ", row=2, col=1)

        # æ•´é«”ä½ˆå±€
        fig.update_layout(
            height=900,
            hovermode='x unified',
            showlegend=True,
            legend=dict(
                orientation="h",
                yanchor="bottom",
                y=1.02,
                xanchor="right",
                x=1
            ),
            margin=dict(l=80, r=80, t=100, b=80)
        )

        return fig


class HtmlToPng:
    """HTML è½‰ PNG é¡åˆ¥"""

    _driver = None

    @classmethod
    def get_driver(cls):
        """å–å¾—æˆ–å»ºç«‹ WebDriver"""
        if cls._driver is None:
            try:
                from selenium import webdriver
                from selenium.webdriver.chrome.options import Options

                chrome_options = Options()
                chrome_options.add_argument('--headless')
                chrome_options.add_argument('--no-sandbox')
                chrome_options.add_argument('--disable-dev-shm-usage')
                chrome_options.add_argument('--disable-gpu')
                chrome_options.add_argument('--window-size=1920,1080')

                cls._driver = webdriver.Chrome(options=chrome_options)
                print("âœ“ WebDriver åˆå§‹åŒ–æˆåŠŸ")

            except Exception as e:
                print(f"âš ï¸  WebDriver åˆå§‹åŒ–å¤±æ•—: {e}")
                cls._driver = None

        return cls._driver

    @classmethod
    def cleanup(cls):
        """æ¸…ç† WebDriver"""
        if cls._driver:
            try:
                cls._driver.quit()
                print("âœ“ WebDriver å·²é—œé–‰")
            except:
                pass
            cls._driver = None

    @staticmethod
    def convert(html_path, png_path):
        """å°‡ HTML è½‰æ›ç‚º PNG"""
        driver = HtmlToPng.get_driver()

        if driver is None:
            return False

        try:
            driver.get(f'file://{html_path}')
            time.sleep(2)

            driver.save_screenshot(png_path)
            return True

        except Exception as e:
            print(f"  âš ï¸  è½‰æ›å¤±æ•—: {e}")
            return False


class Processor:
    """è™•ç†å™¨é¡åˆ¥"""

    @staticmethod
    def process_stock(stock_code, base_dir, config):
        """è™•ç†å–®ä¸€è‚¡ç¥¨"""
        print(f"\nè™•ç†è‚¡ç¥¨: {stock_code}")

        csv_path = os.path.join(config['history_folder'], f"{stock_code}.csv")

        if not os.path.exists(csv_path):
            print(f"  âš ï¸  æ‰¾ä¸åˆ°æ­·å²è³‡æ–™: {csv_path}")
            return None

        html_output = os.path.join(config['html_output_folder'], f"{stock_code}.html")
        png_output = os.path.join(config['png_output_folder'], f"{stock_code}.png")

        if not Config.OVERWRITE_EXISTING:
            if os.path.exists(html_output) and os.path.exists(png_output):
                print(f"  âŠ™ æª”æ¡ˆå·²å­˜åœ¨,è·³é")
                return None

        try:
            df = Utils.read_csv_auto_encoding(csv_path)

            if df.empty:
                print(f"  âš ï¸  CSV æª”æ¡ˆç‚ºç©º")
                return False

            if 'æ—¥æœŸ' not in df.columns:
                print(f"  âš ï¸  ç¼ºå°‘'æ—¥æœŸ'æ¬„ä½")
                return False

            df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
            df = df.dropna(subset=['æ—¥æœŸ'])

            if df.empty:
                print(f"  âš ï¸  æ—¥æœŸè½‰æ›å¾Œç„¡æœ‰æ•ˆè³‡æ–™")
                return False

            df = df.sort_values('æ—¥æœŸ')

            if 'ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸' in df.columns:
                df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'] = pd.to_numeric(
                    df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'].astype(str).str.replace(',', ''),
                    errors='coerce'
                )

            if 'æ”¶ç›¤åƒ¹' in df.columns:
                df['æ”¶ç›¤åƒ¹'] = pd.to_numeric(df['æ”¶ç›¤åƒ¹'], errors='coerce')

            stock_name = ""
            if 'è­‰åˆ¸åç¨±' in df.columns and not df.empty:
                stock_name = df['è­‰åˆ¸åç¨±'].iloc[0]

            sector = ""

            fig = ChartGenerator.create_chart(stock_code, stock_name, df, sector)

            if fig is None:
                return False

            fig.write_html(html_output)
            print(f"  âœ“ HTML å·²å„²å­˜: {os.path.basename(html_output)}")

            success = HtmlToPng.convert(html_output, png_output)

            if success:
                print(f"  âœ“ PNG å·²å„²å­˜: {os.path.basename(png_output)}")
                return True
            else:
                print(f"  âš ï¸  PNG è½‰æ›å¤±æ•—")
                return True

        except Exception as e:
            print(f"  âŒ è™•ç†å¤±æ•—: {e}")
            return False

    @staticmethod
    def batch_process_all_stocks(base_dir, config):
        """æ‰¹æ¬¡è™•ç†æ‰€æœ‰è‚¡ç¥¨"""
        print(f"\n{'='*80}")
        print(f"æ‰¹æ¬¡è™•ç†æ¨¡å¼ - {config['market_name']}")
        print(f"{'='*80}\n")

        history_folder = config['history_folder']

        if not os.path.exists(history_folder):
            print(f"âŒ æ­·å²è³‡æ–™å¤¾ä¸å­˜åœ¨: {history_folder}")
            return

        csv_files = glob.glob(os.path.join(history_folder, '*.csv'))

        if not csv_files:
            print(f"âŒ æ‰¾ä¸åˆ°æ­·å²è³‡æ–™æª”æ¡ˆ")
            return

        stock_codes = [os.path.basename(f).replace('.csv', '') for f in csv_files]
        stock_codes = sorted(stock_codes)

        if not stock_codes:
            print("âŒ ç„¡æ³•å–å¾—è‚¡ç¥¨æ¸…å–®")
            return

        print(f"âœ“ æ‰¾åˆ° {len(stock_codes)} æ”¯è‚¡ç¥¨")

        success_count = 0
        fail_count = 0
        skip_count = 0

        start_time = datetime.now()

        for idx, stock_code in enumerate(stock_codes, 1):
            print(f"\n{'='*70}")
            print(f"é€²åº¦: [{idx}/{len(stock_codes)}] ({idx/len(stock_codes)*100:.1f}%)")
            print(f"{'='*70}")

            result = Processor.process_stock(stock_code, base_dir, config)

            if result is True:
                success_count += 1
            elif result is False:
                fail_count += 1
            elif result is None:
                skip_count += 1

        end_time = datetime.now()
        elapsed_time = (end_time - start_time).total_seconds()

        print("\n" + "="*70)
        print("æ‰¹æ¬¡è™•ç†å®Œæˆ")
        print("="*70)
        print(f"ç¸½è‚¡ç¥¨æ•¸: {len(stock_codes)}")
        print(f"æˆåŠŸè™•ç†: {success_count}")
        print(f"è·³éè™•ç†: {skip_count}")
        print(f"è™•ç†å¤±æ•—: {fail_count}")
        print(f"è™•ç†æ™‚é–“: {elapsed_time:.1f} ç§’ ({elapsed_time/60:.1f} åˆ†é˜)")
        print("="*70)

        HtmlToPng.cleanup()


# ============================================================================
# ä¸»ç¨‹å¼åŸ·è¡Œå‡½æ•¸
# ============================================================================

def run_step1_crawler():
    """åŸ·è¡Œç¬¬ä¸€æ­¥:çˆ¬èŸ²ç¨‹å¼"""
    print("\n" + "ğŸ”¥"*40)
    print("ç¬¬ä¸€æ­¥:åŸ·è¡Œçˆ¬èŸ²ç¨‹å¼")
    print("ğŸ”¥"*40 + "\n")
    
    start_date = datetime(2025, 1, 1)
    end_date = datetime.now()
    print(f"æ—¥æœŸç¯„åœ: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
    print(f"å„²å­˜ä½ç½®: {BASE_DIR}/")
    print()

    start_time = time.time()
    dirs = {
        'StockDaily': os.path.join(BASE_DIR, 'StockDaily'),
        'StockShares': os.path.join(BASE_DIR, 'StockShares'),
        'StockOTCDaily': os.path.join(BASE_DIR, 'StockOTCDaily'),
        'StockOTCShares': os.path.join(BASE_DIR, 'StockOTCShares')
    }

    results = {}
    results['twse_daily'] = crawl_twse_daily(start_date, end_date, dirs['StockDaily'])
    results['twse_inst'] = crawl_twse_institutional(start_date, end_date, dirs['StockShares'])
    results['otc_daily'] = crawl_otc_daily(start_date, end_date, dirs['StockOTCDaily'])
    results['otc_inst'] = crawl_otc_institutional(start_date, end_date, dirs['StockOTCShares'])

    elapsed_time = time.time() - start_time
    print("="*60)
    print("ğŸ“Š ç¬¬ä¸€æ­¥åŸ·è¡Œçµæœæ‘˜è¦")
    print("="*60)
    print(f"âœ“ ä¸Šå¸‚æ¯æ—¥äº¤æ˜“:  {results['twse_daily']} å€‹æª”æ¡ˆ")
    print(f"âœ“ ä¸Šå¸‚ä¸‰å¤§æ³•äºº:  {results['twse_inst']} å€‹æª”æ¡ˆ")
    print(f"âœ“ ä¸Šæ«ƒæ¯æ—¥äº¤æ˜“:  {results['otc_daily']} å€‹æª”æ¡ˆ")
    print(f"âœ“ ä¸Šæ«ƒä¸‰å¤§æ³•äºº:  {results['otc_inst']} å€‹æª”æ¡ˆ")
    print("-"*60)
    print(f"ç¸½è¨ˆä¸‹è¼‰:{sum(results.values())} å€‹æª”æ¡ˆ")
    print(f"åŸ·è¡Œæ™‚é–“:{elapsed_time:.1f} ç§’")
    print("="*60)

def run_step2_analysis(market_type='TSE'):
    """åŸ·è¡Œç¬¬äºŒæ­¥:åˆ†æç¨‹å¼"""
    print(f"\n{'ğŸ”¥'*40}")
    print(f"ç¬¬äºŒæ­¥åˆ†æ:{market_type} ({'ä¸Šå¸‚' if market_type == 'TSE' else 'ä¸Šæ«ƒ'})")
    print(f"{'ğŸ”¥'*40}\n")
    
    config = setup_config(market_type=market_type)
    allowed_stock_codes, stock_sector_map, etf_stock_codes = load_stock_list(config['market_list_path'])
    stock_daily_prices = load_stock_daily_prices(config['stock_daily_folder'], allowed_stock_codes)
    latest_61_files = get_latest_files(config['folder_path'], num_files=61)
    
    (all_data, daily_buy_sell_data, etf_daily_data, buy_top20_tracker,
     sell_top20_tracker, daily_buy_stocks, daily_sell_stocks,
     daily_all_stocks, all_historical_data, statistics) = process_shares_files(
        latest_61_files, allowed_stock_codes, stock_daily_prices,
        stock_sector_map, etf_stock_codes
    )
    
    stock_statistics = calculate_stock_statistics(all_historical_data, config['sigma_threshold'])
    
    (new_buy_stocks, new_sell_stocks, observable_buy_stocks, observable_sell_stocks,
     latest_date, latest_buy_stocks_50, latest_sell_stocks_50) = analyze_new_entries_and_observables(
        latest_61_files[0], daily_buy_stocks, daily_sell_stocks,
        daily_all_stocks, stock_statistics, allowed_stock_codes,
        config['sigma_threshold']
    )
    
    collect_stock_history(latest_buy_stocks_50, config['folder_path'],
                          config['stock_daily_folder'], config['history_folder'],
                          allowed_stock_codes)
    
    buy_stocks, sell_stocks, both_stocks_set, both_stocks_df = aggregate_analysis(
        buy_top20_tracker, sell_top20_tracker, stock_sector_map,
        aggregate_threshold=config.get('aggregate_threshold', 10000),
        show_top_n=config.get('show_top_n', None)
    )
    
    if buy_stocks is not None and sell_stocks is not None:
        export_to_excel(config['output_path'], buy_stocks, sell_stocks, both_stocks_set,
                       both_stocks_df, daily_buy_sell_data, etf_daily_data, latest_date,
                       new_buy_stocks, new_sell_stocks, observable_buy_stocks,
                       observable_sell_stocks, stock_sector_map, etf_stock_codes)
        beautify_excel(config['output_path'])
        print(f"\nâœ“ {market_type} åˆ†æå®Œæˆ")
        print(f"âœ“ Excel å·²å„²å­˜: {config['output_path']}")

def run_step3_charts(market_type='TSE'):
    """åŸ·è¡Œç¬¬ä¸‰æ­¥:åœ–è¡¨ç”Ÿæˆ"""
    print(f"\n{'ğŸ”¥'*40}")
    print(f"ç¬¬ä¸‰æ­¥åœ–è¡¨:{market_type} ({'ä¸Šå¸‚' if market_type == 'TSE' else 'ä¸Šæ«ƒ'})")
    print(f"{'ğŸ”¥'*40}\n")
    
    config = Config.setup_config(market_type=market_type)
    Utils.setup_chinese_font(BASE_DIR)
    Processor.batch_process_all_stocks(BASE_DIR, config)
    print(f"\nâœ“ {market_type} åœ–è¡¨å®Œæˆ")

def main():
    """ä¸»ç¨‹å¼"""
    print("\n" + "="*80)
    print("å°ç£è‚¡å¸‚è³‡æ–™å®Œæ•´è™•ç†æµç¨‹ - GitHub Actions ç‰ˆ")
    print("="*80)
    print("åŸ·è¡Œç’°å¢ƒ:", BASE_DIR)
    print("="*80 + "\n")
    
    # æ­¥é©Ÿ 1:çˆ¬èŸ²
    run_step1_crawler()
    
    # æ­¥é©Ÿ 2:æ¸…ç† History
    print("\n" + "ğŸ”¥"*40)
    print("æ­¥é©Ÿ 2:æ¸…ç† History è³‡æ–™å¤¾")
    print("ğŸ”¥"*40)
    delete_folders(['StockHistory', 'StockOTCHistory'])
    
    # æ­¥é©Ÿ 3-4:åˆ†æ
    run_step2_analysis('TSE')
    run_step2_analysis('OTC')
    
    # æ­¥é©Ÿ 5:æ¸…ç†åœ–è¡¨
    print("\n" + "ğŸ”¥"*40)
    print("æ­¥é©Ÿ 5:æ¸…ç†åœ–è¡¨è³‡æ–™å¤¾")
    print("ğŸ”¥"*40)
    delete_folders(['StockHTML', 'StockPNG', 'StockOTCHTML', 'StockOTCPNG'])
    
    # æ­¥é©Ÿ 6-7:åœ–è¡¨
    run_step3_charts('TSE')
    run_step3_charts('OTC')
    
    # å®Œæˆ
    print("\n" + "ğŸ‰"*40)
    print("æ‰€æœ‰æµç¨‹å·²å®Œæˆ!")
    print("ğŸ‰"*40 + "\n")

if __name__ == "__main__":
    main()
