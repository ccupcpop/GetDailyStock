#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°ç£è‚¡å¸‚è³‡æ–™å®Œæ•´è™•ç†æµç¨‹ - GitHub Actions ç‰ˆæœ¬
æ•´åˆçˆ¬èŸ²ã€åˆ†æã€åœ–è¡¨ç”Ÿæˆçš„å®Œæ•´è‡ªå‹•åŒ–æµç¨‹

ä½œè€…: Frank
ç‰ˆæœ¬: 2.0 (GitHub Actions)
åŠŸèƒ½:
1. çˆ¬å–ä¸Šå¸‚/ä¸Šæ«ƒæ¯æ—¥äº¤æ˜“èˆ‡ä¸‰å¤§æ³•äººè³‡æ–™
2. æ¸…ç†èˆŠçš„ History è³‡æ–™å¤¾
3. ç”Ÿæˆåˆ†æå ±å‘Š (Excel) - åˆ†åˆ¥è™•ç† TSE å’Œ OTC
4. æ¸…ç†èˆŠçš„åœ–è¡¨è³‡æ–™å¤¾
5. ç”ŸæˆæŠ€è¡“åˆ†æåœ–è¡¨ (HTML + PNG) - åˆ†åˆ¥è™•ç† TSE å’Œ OTC
"""

import os
import glob
import shutil
import requests
import pandas as pd
import numpy as np
import time
from datetime import datetime, timedelta
from io import StringIO
import re
from openpyxl import load_workbook
from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import argparse

# ============================================================================
# å…¨åŸŸè¨­å®š
# ============================================================================

# æ§åˆ¶æ˜¯å¦åªåˆ†æç†±é–€è‚¡ç¥¨ (è²·è¶…å‰150 + è³£è¶…å‰50)
# True:  åªåˆ†æè²·è¶…å‰150 + è³£è¶…å‰50
# False: åˆ†ææ‰€æœ‰ CSV å…§çš„è‚¡ç¥¨
TOP_STOCKS_ONLY = True
<<<<<<< HEAD

=======
>>>>>>> 3c0490187701b8052424463ee13effe7be83bfec
# ============================================================================
# å…±ç”¨å·¥å…·å‡½æ•¸
# ============================================================================

def clean_excel_keep_second_sheet(input_file):
    """
    åªä¿ç•™ Excel çš„ç¬¬äºŒå€‹åˆ†é ï¼ˆæœ€è¿‘äº¤æ˜“æ—¥ï¼‰ï¼Œä¸¦ä»¥è©²åˆ†é çš„æ—¥æœŸé‡æ–°å‘½åæª”æ¡ˆ
    
    Args:
        input_file: è¼¸å…¥çš„ Excel æª”æ¡ˆè·¯å¾‘
        
    Returns:
        æ–°æª”æ¡ˆè·¯å¾‘æˆ– None (å¦‚æœå¤±æ•—)
    """
    
    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_file):
        print(f"  âŒ æª”æ¡ˆä¸å­˜åœ¨: {input_file}")
        return None
    
    try:
        # è¼‰å…¥ Excel æª”æ¡ˆ
        wb = load_workbook(input_file)
        sheet_names = wb.sheetnames
        
        print(f"  ğŸ“‹ åŸå§‹åˆ†é æ•¸: {len(sheet_names)}")
        
        # æª¢æŸ¥æ˜¯å¦è‡³å°‘æœ‰ 2 å€‹åˆ†é 
        if len(sheet_names) < 2:
            print(f"  âš ï¸  åªæœ‰ {len(sheet_names)} å€‹åˆ†é ï¼Œè·³éæ¸…ç†")
            wb.close()
            return None
        
        # å–å¾—ç¬¬äºŒå€‹åˆ†é çš„åç¨±ï¼ˆé€™æ˜¯è¦ä¿ç•™çš„ï¼‰
        second_sheet_name = sheet_names[1]
        print(f"  âœ“ ä¿ç•™åˆ†é : {second_sheet_name}")
        
        # å¾åˆ†é åç¨±æå–æ—¥æœŸ (å‡è¨­æ ¼å¼ç‚º YYYYMMDD)
        match = re.search(r'(\d{8})', second_sheet_name)
        if not match:
            print(f"  âš ï¸  ç„¡æ³•å¾åˆ†é åç¨±æå–æ—¥æœŸ: {second_sheet_name}")
            wb.close()
            return None
        
        new_date_str = match.group(1)
        print(f"  ğŸ“… æå–æ—¥æœŸ: {new_date_str}")
        
        # åˆªé™¤å…¶ä»–æ‰€æœ‰åˆ†é ï¼ˆé™¤äº†ç¬¬äºŒå€‹ï¼‰
        sheets_to_delete = [name for i, name in enumerate(sheet_names) if i != 1]
        for sheet_name in sheets_to_delete:
            wb.remove(wb[sheet_name])
            print(f"  ğŸ—‘ï¸  å·²åˆªé™¤åˆ†é : {sheet_name}")
        
        # ç”Ÿæˆæ–°æª”æ¡ˆåç¨±
        dir_name = os.path.dirname(input_file)
        base_name = os.path.basename(input_file)
        
        # æå–æª”æ¡ˆå‰ç¶´ (tse_analysis_result æˆ– otc_analysis_result)
        if 'tse_analysis_result' in base_name.lower():
            prefix = 'tse_analysis_result'
        elif 'otc_analysis_result' in base_name.lower():
            prefix = 'otc_analysis_result'
        else:
            # ä½¿ç”¨åŸå§‹æª”æ¡ˆåå»æ‰æ—¥æœŸéƒ¨åˆ†
            prefix = re.sub(r'_\d{8}', '', base_name.replace('.xlsx', ''))
        
        new_file_name = f"{prefix}_{new_date_str}.xlsx"
        new_file_path = os.path.join(dir_name, new_file_name)
        
        # å„²å­˜æ–°æª”æ¡ˆ
        wb.save(new_file_path)
        wb.close()
        
        file_size = os.path.getsize(new_file_path) / 1024  # KB
        print(f"  âœ… å·²ç”Ÿæˆ: {new_file_name} ({file_size:.1f} KB)")
        
        # å¦‚æœæ–°èˆŠæª”æ¡ˆåä¸åŒï¼Œåˆªé™¤èˆŠæª”æ¡ˆ
        if new_file_path != input_file:
            try:
                os.remove(input_file)
                print(f"  ğŸ—‘ï¸  å·²åˆªé™¤èˆŠæª”: {base_name}")
            except Exception as e:
                print(f"  âš ï¸  ç„¡æ³•åˆªé™¤èˆŠæª”: {e}")
        
        return new_file_path
        
    except Exception as e:
        print(f"  âŒ è™•ç†å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return None


def setup_base_directory():
    """
    è¨­å®šåŸºç¤å·¥ä½œç›®éŒ„
    åœ¨ GitHub Actions ä¸­ä½¿ç”¨ç•¶å‰å·¥ä½œç›®éŒ„æˆ–æŒ‡å®šçš„è³‡æ–™ç›®éŒ„
    """
    # å„ªå…ˆä½¿ç”¨ç’°å¢ƒè®Šæ•¸æŒ‡å®šçš„ç›®éŒ„
    base_dir = os.environ.get('STOCK_DATA_DIR', os.getcwd())
    
    # ç¢ºä¿ç›®éŒ„å­˜åœ¨
    os.makedirs(base_dir, exist_ok=True)
    
    print(f"âœ“ å·¥ä½œç›®éŒ„: {base_dir}\n")
    return base_dir

def delete_folders(base_dir, folder_names):
    """åˆªé™¤ä¸¦é‡å»ºæŒ‡å®šçš„è³‡æ–™å¤¾"""
    print(f"\n{'='*80}")
    print("æ¸…ç†è³‡æ–™å¤¾...")
    print(f"{'='*80}")
    
    for folder_name in folder_names:
        folder_path = os.path.join(base_dir, folder_name)
        
        # çµ±è¨ˆç¾æœ‰æª”æ¡ˆæ•¸é‡
        file_count = 0
        if os.path.exists(folder_path):
            try:
                files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]
                file_count = len(files)
                print(f"ğŸ“‚ {folder_name}: ç™¼ç¾ {file_count} å€‹ CSV æª”æ¡ˆ")
            except Exception as e:
                print(f"âš ï¸  ç„¡æ³•è®€å– {folder_name}: {e}")
        
        # åˆªé™¤è³‡æ–™å¤¾
        if os.path.exists(folder_path):
            try:
                shutil.rmtree(folder_path)
                print(f"âœ“ å·²åˆªé™¤: {folder_name} ({file_count} å€‹æª”æ¡ˆ)")
                
                # ç­‰å¾…æª”æ¡ˆç³»çµ±å®Œæˆæ“ä½œ
                import time
                time.sleep(0.5)
                
            except Exception as e:
                print(f"âœ— åˆªé™¤å¤±æ•— {folder_name}: {e}")
                continue
        else:
            print(f"âŠ˜ è³‡æ–™å¤¾ä¸å­˜åœ¨: {folder_name}")
        
        # é‡æ–°å»ºç«‹ç©ºè³‡æ–™å¤¾
        try:
            os.makedirs(folder_path, exist_ok=True)
            print(f"âœ“ å·²é‡å»ºç©ºè³‡æ–™å¤¾: {folder_name}")
            
            # é©—è­‰è³‡æ–™å¤¾æ˜¯ç©ºçš„
            remaining = os.listdir(folder_path)
            if remaining:
                print(f"âš ï¸  è­¦å‘Š: {folder_name} å…§é‚„æœ‰ {len(remaining)} å€‹é …ç›®ï¼")
            
        except Exception as e:
            print(f"âœ— é‡å»ºè³‡æ–™å¤¾å¤±æ•— {folder_name}: {e}")
    
    print(f"{'='*80}\n")

def create_required_directories(base_dir):
    """å»ºç«‹æ‰€éœ€çš„è³‡æ–™å¤¾çµæ§‹"""
    required_dirs = [
        'StockList',       # è‚¡ç¥¨æ¸…å–®å’Œå­—é«”
        'StockTSEDaily',
        'StockTSEShares',
        'StockOTCDaily',
        'StockOTCShares',
        'StockInfo',       # åˆ†æå ±å‘Š
        'StockTSEHistory',
        'StockOTCHistory',
        'StockTSEHTML',
        'StockOTCHTML',
        'local_StockTSEHistory',  # æ–°å¢ local è³‡æ–™å¤¾
        'local_StockOTCHistory',
        'local_StockTSEHTML',
        'local_StockOTCHTML'
    ]
    
    print(f"\n{'='*80}")
    print("å»ºç«‹è³‡æ–™å¤¾çµæ§‹...")
    print(f"{'='*80}")
    
    for dir_name in required_dirs:
        dir_path = os.path.join(base_dir, dir_name)
        os.makedirs(dir_path, exist_ok=True)
        print(f"âœ“ {dir_name}")
    
    print(f"{'='*80}\n")

# ============================================================================
# ç¬¬ä¸€æ­¥ï¼šçˆ¬èŸ²ç¨‹å¼çš„æ‰€æœ‰å‡½æ•¸
# ============================================================================

# ã€ç¬¬ä¸€æ­¥-filter_csv_contentã€‘
# å¾ç¬¬ä¸€æ­¥ç¨‹å¼è¤‡è£½ filter_csv_content å‡½æ•¸
def filter_csv_content(csv_bytes):
    """éæ¿¾ CSV å…§å®¹ï¼Œåªä¿ç•™è‚¡ç¥¨è³‡æ–™"""
    try:
        content = csv_bytes.decode('cp950')
        lines = content.split('\r\n')

        filtered_lines = []
        header_found = False
        stock_count = 0

        for line in lines:
            if 'è­‰åˆ¸ä»£è™Ÿ' in line and not header_found:
                filtered_lines.append(line)
                header_found = True
                continue

            if header_found:
                match = re.match(r'^=?"?(\d{4})"?', line)
                if match:
                    filtered_lines.append(line)
                    stock_count += 1

        filtered_content = '\r\n'.join(filtered_lines)
        filtered_bytes = filtered_content.encode('cp950')
        print(f"   âœ‚ï¸  éæ¿¾å®Œæˆï¼šä¿ç•™ {stock_count} æª”è‚¡ç¥¨")
        return filtered_bytes

    except Exception as e:
        print(f"   âš ï¸  éæ¿¾å¤±æ•—: {e}ï¼Œå°‡å„²å­˜åŸå§‹è³‡æ–™")
        return csv_bytes

# ã€ç¬¬ä¸€æ­¥-download_twse_dailyã€‘
# å¾ç¬¬ä¸€æ­¥ç¨‹å¼è¤‡è£½ download_twse_daily å‡½æ•¸
def download_twse_daily(date_str):
    """ä¸‹è¼‰ä¸Šå¸‚æ¯æ—¥äº¤æ˜“è³‡æ–™"""
    if '-' in date_str:
        date_str = date_str.replace('-', '')

    url = f"https://www.twse.com.tw/rwd/zh/afterTrading/MI_INDEX?date={date_str}&type=ALL&response=csv"

    try:
        response = requests.get(url, timeout=30)
        if response.status_code == 200 and len(response.content) > 100:
            return response.content
        return None
    except Exception as e:
        print(f"   âŒ ä¸‹è¼‰éŒ¯èª¤: {e}")
        return None
# ã€ç¬¬ä¸€æ­¥-crawl_twse_dailyã€‘
# å¾ç¬¬ä¸€æ­¥ç¨‹å¼è¤‡è£½ crawl_twse_daily å‡½æ•¸
def crawl_twse_daily(start_date, end_date, save_dir):
    """æŠ“å–ä¸Šå¸‚æ¯æ—¥äº¤æ˜“è³‡æ–™"""
    print("="*60)
    print("ğŸ“Š [1/4] ä¸Šå¸‚æ¯æ—¥äº¤æ˜“è³‡æ–™ (TWSE Daily)")
    print("="*60)

    os.makedirs(save_dir, exist_ok=True)

    missing_dates = []
    curr = end_date

    # å¾ä»Šå¤©å¾€å›æª¢æŸ¥
    while curr >= start_date:
        if curr.weekday() < 5:  # åªæª¢æŸ¥å¹³æ—¥
            date_formatted = curr.strftime('%Y-%m-%d')
            file_path = os.path.join(save_dir, f'{date_formatted}.csv')

            if os.path.exists(file_path):
                print(f"  {date_formatted}... [å·²å­˜åœ¨ï¼Œåœæ­¢æª¢æŸ¥] âœ“")
                break
            else:
                missing_dates.append(curr)

        curr -= timedelta(days=1)

    if not missing_dates:
        print("âœ“ ç„¡ç¼ºå¤±è³‡æ–™\n")
        return 0

    print(f"éœ€è¦ä¸‹è¼‰ {len(missing_dates)} å€‹äº¤æ˜“æ—¥")
    print("-"*60)

    success_count = 0

    for idx, date_dt in enumerate(missing_dates, 1):
        date_str = date_dt.strftime('%Y%m%d')
        date_formatted = date_dt.strftime('%Y-%m-%d')
        file_path = os.path.join(save_dir, f'{date_formatted}.csv')

        print(f"  [{idx:2d}/{len(missing_dates)}] {date_formatted}...", end='', flush=True)

        csv_bytes = download_twse_daily(date_str)

        if csv_bytes:
            filtered_bytes = filter_csv_content(csv_bytes)
            with open(file_path, 'wb') as f:
                f.write(filtered_bytes)
            print(" âœ“")
            success_count += 1
        else:
            print(" âœ—")

        time.sleep(1)

    print(f"âœ“ æˆåŠŸä¸‹è¼‰: {success_count} å€‹æª”æ¡ˆ\n")
    return success_count
# ã€ç¬¬ä¸€æ­¥-download_twse_institutionalã€‘
# å¾ç¬¬ä¸€æ­¥ç¨‹å¼è¤‡è£½ download_twse_institutional å‡½æ•¸
def download_twse_institutional(date_str):
    """ä¸‹è¼‰ä¸Šå¸‚ä¸‰å¤§æ³•äººè³‡æ–™"""
    url = 'https://www.twse.com.tw/rwd/zh/fund/T86'
    params = {'date': date_str, 'selectType': 'ALL', 'response': 'json'}
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}

    try:
        response = requests.get(url, params=params, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()

        if data.get('stat') == 'OK' and 'data' in data:
            return pd.DataFrame(data['data'], columns=data['fields'])
        return None
    except Exception as e:
        print(f"   âŒ éŒ¯èª¤: {e}")
        return None
# ã€ç¬¬ä¸€æ­¥-crawl_twse_institutionalã€‘
# å¾ç¬¬ä¸€æ­¥ç¨‹å¼è¤‡è£½ crawl_twse_institutional å‡½æ•¸
def crawl_twse_institutional(start_date, end_date, save_dir):
    """æŠ“å–ä¸Šå¸‚ä¸‰å¤§æ³•äººè²·è³£è¶…è³‡æ–™"""
    print("="*60)
    print("ğŸ“Š [2/4] ä¸Šå¸‚ä¸‰å¤§æ³•äººè²·è³£è¶… (TWSE Institutional)")
    print("="*60)

    os.makedirs(save_dir, exist_ok=True)

    missing_dates = []
    curr = end_date

    while curr >= start_date:
        if curr.weekday() < 5:
            date_formatted = curr.strftime('%Y-%m-%d')
            file_path = os.path.join(save_dir, f'{date_formatted}.csv')

            if os.path.exists(file_path):
                print(f"  {date_formatted}... [å·²å­˜åœ¨ï¼Œåœæ­¢æª¢æŸ¥] âœ“")
                break
            else:
                missing_dates.append(curr)

        curr -= timedelta(days=1)

    if not missing_dates:
        print("âœ“ ç„¡ç¼ºå¤±è³‡æ–™\n")
        return 0

    print(f"éœ€è¦ä¸‹è¼‰ {len(missing_dates)} å€‹äº¤æ˜“æ—¥")
    print("-"*60)

    success_count = 0

    for idx, date_dt in enumerate(missing_dates, 1):
        date_str = date_dt.strftime('%Y%m%d')
        date_formatted = date_dt.strftime('%Y-%m-%d')
        file_path = os.path.join(save_dir, f'{date_formatted}.csv')

        print(f"  [{idx:2d}/{len(missing_dates)}] {date_formatted}...", end='', flush=True)

        df = download_twse_institutional(date_str)

        if df is not None and not df.empty:
            df.to_csv(file_path, index=False, encoding='utf-8-sig')
            print(" âœ“")
            success_count += 1
        else:
            print(" âœ—")

        time.sleep(3)

    print(f"âœ“ æˆåŠŸä¸‹è¼‰: {success_count} å€‹æª”æ¡ˆ\n")
    return success_count
# ã€ç¬¬ä¸€æ­¥-process_otc_daily_columnsã€‘
# å¾ç¬¬ä¸€æ­¥ç¨‹å¼è¤‡è£½ process_otc_daily_columns å‡½æ•¸
def process_otc_daily_columns(df):
    """è™•ç†ä¸Šæ«ƒæ¯æ—¥äº¤æ˜“è³‡æ–™æ¬„ä½"""
    rename_mapping = {
        'ä»£è™Ÿ': 'è­‰åˆ¸ä»£è™Ÿ',
        'åç¨±': 'è­‰åˆ¸åç¨±',
        'æ”¶ç›¤': 'æ”¶ç›¤åƒ¹',
        'é–‹ç›¤': 'é–‹ç›¤åƒ¹',
        'æœ€é«˜': 'æœ€é«˜åƒ¹',
        'æœ€ä½': 'æœ€ä½åƒ¹',
        'æˆäº¤è‚¡æ•¸': 'æˆäº¤è‚¡æ•¸',
        'æˆäº¤ç­†æ•¸': 'æˆäº¤ç­†æ•¸',
        'æˆäº¤é‡‘é¡(å…ƒ)': 'æˆäº¤é‡‘é¡',
        'æ¼²è·Œ': 'æ¼²è·Œåƒ¹å·®',
        'æœ€å¾Œè²·åƒ¹': 'æœ€å¾Œæ­ç¤ºè²·åƒ¹',
        'æœ€å¾Œè²·é‡(åƒè‚¡)': 'æœ€å¾Œæ­ç¤ºè²·é‡',
        'æœ€å¾Œè³£åƒ¹': 'æœ€å¾Œæ­ç¤ºè³£åƒ¹',
        'æœ€å¾Œè³£é‡(åƒè‚¡)': 'æœ€å¾Œæ­ç¤ºè³£é‡'
    }

    df = df.rename(columns=rename_mapping)

    # åˆªé™¤ä¸éœ€è¦çš„æ¬„ä½
    columns_to_drop = ['å‡åƒ¹', 'ç™¼è¡Œè‚¡æ•¸', 'æ¬¡æ—¥åƒè€ƒåƒ¹', 'æ¬¡æ—¥æ¼²åœåƒ¹', 'æ¬¡æ—¥è·Œåœåƒ¹']
    existing_cols_to_drop = [col for col in columns_to_drop if col in df.columns]
    if existing_cols_to_drop:
        df = df.drop(columns=existing_cols_to_drop)

    # æ–°å¢æ¼²è·Œ(+/-)æ¬„ä½
    if 'æ¼²è·Œåƒ¹å·®' in df.columns:
        df['æ¼²è·Œåƒ¹å·®'] = pd.to_numeric(df['æ¼²è·Œåƒ¹å·®'], errors='coerce')
        df['æ¼²è·Œ(+/-)'] = df['æ¼²è·Œåƒ¹å·®'].apply(lambda x: '+' if x > 0 else '-' if pd.notna(x) else '')
        df['æ¼²è·Œåƒ¹å·®'] = df['æ¼²è·Œåƒ¹å·®'].abs()
    else:
        df['æ¼²è·Œ(+/-)'] = ''

    # æ–°å¢æœ¬ç›Šæ¯”æ¬„ä½
    df['æœ¬ç›Šæ¯”'] = ''

    # èª¿æ•´æ¬„ä½é †åº
    desired_order = [
        'è­‰åˆ¸ä»£è™Ÿ', 'è­‰åˆ¸åç¨±', 'æˆäº¤è‚¡æ•¸', 'æˆäº¤ç­†æ•¸', 'æˆäº¤é‡‘é¡',
        'é–‹ç›¤åƒ¹', 'æœ€é«˜åƒ¹', 'æœ€ä½åƒ¹', 'æ”¶ç›¤åƒ¹', 'æ¼²è·Œ(+/-)', 'æ¼²è·Œåƒ¹å·®',
        'æœ€å¾Œæ­ç¤ºè²·åƒ¹', 'æœ€å¾Œæ­ç¤ºè²·é‡', 'æœ€å¾Œæ­ç¤ºè³£åƒ¹', 'æœ€å¾Œæ­ç¤ºè³£é‡', 'æœ¬ç›Šæ¯”'
    ]

    existing_desired_cols = [col for col in desired_order if col in df.columns]
    other_cols = [col for col in df.columns if col not in desired_order]
    final_order = existing_desired_cols + other_cols
    df = df[final_order]

    return df
# ã€ç¬¬ä¸€æ­¥-download_otc_dailyã€‘
# å¾ç¬¬ä¸€æ­¥ç¨‹å¼è¤‡è£½ download_otc_daily å‡½æ•¸
def download_otc_daily(date_str):
    """ä¸‹è¼‰ä¸Šæ«ƒæ¯æ—¥äº¤æ˜“è³‡æ–™"""
    date_formatted = f"{date_str[:4]}%2F{date_str[4:6]}%2F{date_str[6:]}"
    url = f'https://www.tpex.org.tw/www/zh-tw/afterTrading/dailyQuotes?date={date_formatted}&id=&response=csv'

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
        'Referer': 'https://www.tpex.org.tw/zh-tw/aftertrading/quotes/daily.html'
    }

    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()

        if not response.content or len(response.content) < 100:
            return None

        encodings = ['big5', 'cp950', 'utf-8', 'utf-8-sig']

        for encoding in encodings:
            try:
                text = response.content.decode(encoding)

                if 'æŸ¥ç„¡è³‡æ–™' in text or 'ç›®å‰ç„¡è³‡æ–™' in text:
                    return None

                csv_data = StringIO(text)
                df = pd.read_csv(csv_data, skiprows=2)

                if df.empty:
                    continue

                df = df.dropna(how='all')

                if len(df.columns) > 0:
                    first_col = df.columns[0]
                    df = df[df[first_col].notna()]
                    df = df[~df[first_col].astype(str).str.contains('ä¸Šæ«ƒ|ç¸½æˆäº¤|è¨»:', na=False)]

                if len(df) == 0:
                    continue

                first_col = df.columns[0] if len(df.columns) > 0 else ''
                if any('\u4e00' <= c <= '\u9fff' for c in first_col):
                    df = process_otc_daily_columns(df)
                    return df

            except:
                continue

        return None

    except Exception as e:
        return None
# ã€ç¬¬ä¸€æ­¥-crawl_otc_dailyã€‘
# å¾ç¬¬ä¸€æ­¥ç¨‹å¼è¤‡è£½ crawl_otc_daily å‡½æ•¸
def crawl_otc_daily(start_date, end_date, save_dir):
    """æŠ“å–ä¸Šæ«ƒæ¯æ—¥äº¤æ˜“è³‡æ–™"""
    print("="*60)
    print("ğŸ“Š [3/4] ä¸Šæ«ƒæ¯æ—¥äº¤æ˜“è³‡æ–™ (OTC Daily)")
    print("="*60)

    os.makedirs(save_dir, exist_ok=True)

    missing_dates = []
    curr = end_date

    while curr >= start_date:
        if curr.weekday() < 5:
            date_formatted = curr.strftime('%Y-%m-%d')
            file_path = os.path.join(save_dir, f'{date_formatted}.csv')

            if os.path.exists(file_path):
                try:
                    df_check = pd.read_csv(file_path)
                    if len(df_check) > 1:
                        print(f"  {date_formatted}... [å·²å­˜åœ¨ï¼Œåœæ­¢æª¢æŸ¥] âœ“")
                        break
                    else:
                        missing_dates.append(curr)
                except:
                    missing_dates.append(curr)
            else:
                missing_dates.append(curr)

        curr -= timedelta(days=1)

    if not missing_dates:
        print("âœ“ ç„¡ç¼ºå¤±è³‡æ–™\n")
        return 0

    print(f"éœ€è¦ä¸‹è¼‰ {len(missing_dates)} å€‹äº¤æ˜“æ—¥")
    print("-"*60)

    success_count = 0

    for idx, date_dt in enumerate(missing_dates, 1):
        date_str = date_dt.strftime('%Y%m%d')
        date_formatted = date_dt.strftime('%Y-%m-%d')
        file_path = os.path.join(save_dir, f'{date_formatted}.csv')

        print(f"  [{idx:2d}/{len(missing_dates)}] {date_formatted}...", end='', flush=True)

        df = download_otc_daily(date_str)

        if df is not None and not df.empty:
            df.to_csv(file_path, index=False, encoding='utf-8-sig')
            print(f" âœ“ ({len(df)} ç­†)")
            success_count += 1
        else:
            print(" âœ—")

        if idx % 5 == 0:
            time.sleep(4)
        else:
            time.sleep(2)

    print(f"âœ“ æˆåŠŸä¸‹è¼‰: {success_count} å€‹æª”æ¡ˆ\n")
    return success_count

# ã€ç¬¬ä¸€æ­¥-process_otc_institutional_columnsã€‘
# å¾ç¬¬ä¸€æ­¥ç¨‹å¼è¤‡è£½ process_otc_institutional_columns å‡½æ•¸
def process_otc_institutional_columns(df):
    """è™•ç†ä¸Šæ«ƒä¸‰å¤§æ³•äººè³‡æ–™æ¬„ä½"""
    column_rename_map = {
        'ä»£è™Ÿ': 'è­‰åˆ¸ä»£è™Ÿ',
        'åç¨±': 'è­‰åˆ¸åç¨±',
        'å¤–è³‡åŠé™¸è³‡(ä¸å«å¤–è³‡è‡ªç‡Ÿå•†)-è²·é€²è‚¡æ•¸': 'å¤–é™¸è³‡è²·é€²è‚¡æ•¸(ä¸å«å¤–è³‡è‡ªç‡Ÿå•†)',
        'å¤–è³‡åŠé™¸è³‡(ä¸å«å¤–è³‡è‡ªç‡Ÿå•†)-è³£å‡ºè‚¡æ•¸': 'å¤–é™¸è³‡è³£å‡ºè‚¡æ•¸(ä¸å«å¤–è³‡è‡ªç‡Ÿå•†)',
        'å¤–è³‡åŠé™¸è³‡(ä¸å«å¤–è³‡è‡ªç‡Ÿå•†)-è²·è³£è¶…è‚¡æ•¸': 'å¤–é™¸è³‡è²·è³£è¶…è‚¡æ•¸(ä¸å«å¤–è³‡è‡ªç‡Ÿå•†)',
        'å¤–è³‡è‡ªç‡Ÿå•†-è²·é€²è‚¡æ•¸': 'å¤–è³‡è‡ªç‡Ÿå•†è²·é€²è‚¡æ•¸',
        'å¤–è³‡è‡ªç‡Ÿå•†-è³£å‡ºè‚¡æ•¸': 'å¤–è³‡è‡ªç‡Ÿå•†è³£å‡ºè‚¡æ•¸',
        'å¤–è³‡è‡ªç‡Ÿå•†-è²·è³£è¶…è‚¡æ•¸': 'å¤–è³‡è‡ªç‡Ÿå•†è²·è³£è¶…è‚¡æ•¸',
        'æŠ•ä¿¡-è²·é€²è‚¡æ•¸': 'æŠ•ä¿¡è²·é€²è‚¡æ•¸',
        'æŠ•ä¿¡-è³£å‡ºè‚¡æ•¸': 'æŠ•ä¿¡è³£å‡ºè‚¡æ•¸',
        'æŠ•ä¿¡-è²·è³£è¶…è‚¡æ•¸': 'æŠ•ä¿¡è²·è³£è¶…è‚¡æ•¸',
        'è‡ªç‡Ÿå•†(è‡ªè¡Œè²·è³£)-è²·é€²è‚¡æ•¸': 'è‡ªç‡Ÿå•†è²·é€²è‚¡æ•¸(è‡ªè¡Œè²·è³£)',
        'è‡ªç‡Ÿå•†(è‡ªè¡Œè²·è³£)-è³£å‡ºè‚¡æ•¸': 'è‡ªç‡Ÿå•†è³£å‡ºè‚¡æ•¸(è‡ªè¡Œè²·è³£)',
        'è‡ªç‡Ÿå•†(è‡ªè¡Œè²·è³£)-è²·è³£è¶…è‚¡æ•¸': 'è‡ªç‡Ÿå•†è²·è³£è¶…è‚¡æ•¸(è‡ªè¡Œè²·è³£)',
        'è‡ªç‡Ÿå•†(é¿éšª)-è²·é€²è‚¡æ•¸': 'è‡ªç‡Ÿå•†è²·é€²è‚¡æ•¸(é¿éšª)',
        'è‡ªç‡Ÿå•†(é¿éšª)-è³£å‡ºè‚¡æ•¸': 'è‡ªç‡Ÿå•†è³£å‡ºè‚¡æ•¸(é¿éšª)',
        'è‡ªç‡Ÿå•†(é¿éšª)-è²·è³£è¶…è‚¡æ•¸': 'è‡ªç‡Ÿå•†è²·è³£è¶…è‚¡æ•¸(é¿éšª)',
        'è‡ªç‡Ÿå•†-è²·è³£è¶…è‚¡æ•¸': 'è‡ªç‡Ÿå•†è²·è³£è¶…è‚¡æ•¸',
        'ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸åˆè¨ˆ': 'ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'
    }

    df = df.rename(columns=column_rename_map)

    # åˆªé™¤æŒ‡å®šæ¬„ä½
    columns_to_drop_indices = [8, 9, 10, 20, 21]
    all_columns = list(df.columns)
    columns_to_keep = [col for idx, col in enumerate(all_columns) if idx not in columns_to_drop_indices]
    df = df[columns_to_keep]

    # èª¿æ•´æ¬„ä½é †åº
    current_columns = list(df.columns)
    if 'è‡ªç‡Ÿå•†è²·è³£è¶…è‚¡æ•¸' in current_columns and 'æŠ•ä¿¡è²·è³£è¶…è‚¡æ•¸' in current_columns:
        current_columns.remove('è‡ªç‡Ÿå•†è²·è³£è¶…è‚¡æ•¸')
        invest_trust_idx = current_columns.index('æŠ•ä¿¡è²·è³£è¶…è‚¡æ•¸')
        current_columns.insert(invest_trust_idx + 1, 'è‡ªç‡Ÿå•†è²·è³£è¶…è‚¡æ•¸')
        df = df[current_columns]

    return df

# ã€ç¬¬ä¸€æ­¥-download_otc_institutionalã€‘
# å¾ç¬¬ä¸€æ­¥ç¨‹å¼è¤‡è£½ download_otc_institutional å‡½æ•¸
def download_otc_institutional(date_str):
    """ä¸‹è¼‰ä¸Šæ«ƒä¸‰å¤§æ³•äººè³‡æ–™"""
    date_formatted = f"{date_str[:4]}%2F{date_str[4:6]}%2F{date_str[6:]}"
    url = f'https://www.tpex.org.tw/www/zh-tw/insti/dailyTrade?type=Daily&sect=AL&date={date_formatted}&id=&response=csv'

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
        'Referer': 'https://www.tpex.org.tw/zh-tw/mainboard/trading/major-institutional/detail/day.html'
    }

    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()

        if not response.content or len(response.content) < 100:
            return None

        encodings = ['big5', 'cp950', 'utf-8', 'utf-8-sig']

        for encoding in encodings:
            try:
                text = response.content.decode(encoding)

                if 'æŸ¥ç„¡è³‡æ–™' in text or 'ç›®å‰ç„¡è³‡æ–™' in text:
                    return None

                csv_data = StringIO(text)
                df = pd.read_csv(csv_data, skiprows=1)

                if df.empty or len(df) == 0:
                    continue

                df = df.dropna(how='all')

                if len(df) == 0:
                    continue

                first_col = df.columns[0] if len(df.columns) > 0 else ''
                if any('\u4e00' <= c <= '\u9fff' for c in first_col):
                    df = process_otc_institutional_columns(df)
                    return df

            except:
                continue

        return None

    except Exception as e:
        return None
# ã€ç¬¬ä¸€æ­¥-crawl_otc_institutionalã€‘
# å¾ç¬¬ä¸€æ­¥ç¨‹å¼è¤‡è£½ crawl_otc_institutional å‡½æ•¸
def crawl_otc_institutional(start_date, end_date, save_dir):
    """æŠ“å–ä¸Šæ«ƒä¸‰å¤§æ³•äººè²·è³£è¶…è³‡æ–™"""
    print("="*60)
    print("ğŸ“Š [4/4] ä¸Šæ«ƒä¸‰å¤§æ³•äººè²·è³£è¶… (OTC Institutional)")
    print("="*60)

    os.makedirs(save_dir, exist_ok=True)

    missing_dates = []
    curr = end_date

    while curr >= start_date:
        if curr.weekday() < 5:
            date_formatted = curr.strftime('%Y-%m-%d')
            file_path = os.path.join(save_dir, f'{date_formatted}.csv')

            if os.path.exists(file_path):
                try:
                    df_check = pd.read_csv(file_path)
                    if len(df_check) > 1:
                        print(f"  {date_formatted}... [å·²å­˜åœ¨ï¼Œåœæ­¢æª¢æŸ¥] âœ“")
                        break
                    else:
                        missing_dates.append(curr)
                except:
                    missing_dates.append(curr)
            else:
                missing_dates.append(curr)

        curr -= timedelta(days=1)

    if not missing_dates:
        print("âœ“ ç„¡ç¼ºå¤±è³‡æ–™\n")
        return 0

    print(f"éœ€è¦ä¸‹è¼‰ {len(missing_dates)} å€‹äº¤æ˜“æ—¥")
    print("-"*60)

    success_count = 0

    for idx, date_dt in enumerate(missing_dates, 1):
        date_str = date_dt.strftime('%Y%m%d')
        date_formatted = date_dt.strftime('%Y-%m-%d')
        file_path = os.path.join(save_dir, f'{date_formatted}.csv')

        print(f"  [{idx:2d}/{len(missing_dates)}] {date_formatted}...", end='', flush=True)

        df = download_otc_institutional(date_str)

        if df is not None and not df.empty:
            df.to_csv(file_path, index=False, encoding='utf-8-sig')
            print(f" âœ“ ({len(df)} ç­†)")
            success_count += 1
        else:
            print(" âœ—")

        if idx % 5 == 0:
            time.sleep(4)
        else:
            time.sleep(2)

    print(f"âœ“ æˆåŠŸä¸‹è¼‰: {success_count} å€‹æª”æ¡ˆ\n")
    return success_count

def run_step1_crawler(base_dir, start_date=None, end_date=None):
    """åŸ·è¡Œç¬¬ä¸€æ­¥ï¼šçˆ¬èŸ²ç¨‹å¼"""
    print("\n" + "ğŸ”¥"*40)
    print("ç¬¬ä¸€æ­¥ï¼šåŸ·è¡Œçˆ¬èŸ²ç¨‹å¼")
    print("ğŸ”¥"*40 + "\n")
    
    if start_date is None:
        start_date = datetime(2025, 1, 1)
    if end_date is None:
        end_date = datetime.now()

    print(f"æ—¥æœŸç¯„åœ: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
    print(f"å„²å­˜ä½ç½®: {base_dir}/")
    print()

    start_time = time.time()

    dirs = {
        'StockTSEDaily': os.path.join(base_dir, 'StockTSEDaily'),
        'StockTSEShares': os.path.join(base_dir, 'StockTSEShares'),
        'StockOTCDaily': os.path.join(base_dir, 'StockOTCDaily'),
        'StockOTCShares': os.path.join(base_dir, 'StockOTCShares')
    }

    results = {}
    results['twse_daily'] = crawl_twse_daily(start_date, end_date, dirs['StockTSEDaily'])
    results['twse_inst'] = crawl_twse_institutional(start_date, end_date, dirs['StockTSEShares'])
    results['otc_daily'] = crawl_otc_daily(start_date, end_date, dirs['StockOTCDaily'])
    results['otc_inst'] = crawl_otc_institutional(start_date, end_date, dirs['StockOTCShares'])

    elapsed_time = time.time() - start_time

    print("="*60)
    print("ğŸ“Š ç¬¬ä¸€æ­¥åŸ·è¡Œçµæœæ‘˜è¦")
    print("="*60)
    print(f"âœ“ ä¸Šå¸‚æ¯æ—¥äº¤æ˜“ï¼š  {results['twse_daily']} å€‹æª”æ¡ˆ")
    print(f"âœ“ ä¸Šå¸‚ä¸‰å¤§æ³•äººï¼š  {results['twse_inst']} å€‹æª”æ¡ˆ")
    print(f"âœ“ ä¸Šæ«ƒæ¯æ—¥äº¤æ˜“ï¼š  {results['otc_daily']} å€‹æª”æ¡ˆ")
    print(f"âœ“ ä¸Šæ«ƒä¸‰å¤§æ³•äººï¼š  {results['otc_inst']} å€‹æª”æ¡ˆ")
    print("-"*60)
    print(f"ç¸½è¨ˆä¸‹è¼‰ï¼š{sum(results.values())} å€‹æª”æ¡ˆ")
    print(f"åŸ·è¡Œæ™‚é–“ï¼š{elapsed_time:.1f} ç§’")
    print("="*60)

# ============================================================================
# ç¬¬äºŒæ­¥ï¼šåˆ†æç¨‹å¼çš„æ‰€æœ‰å‡½æ•¸
# ============================================================================

# ã€ç¬¬äºŒæ­¥-normalize_stock_codeã€‘
# å¾ç¬¬äºŒæ­¥ç¨‹å¼è¤‡è£½ normalize_stock_code å‡½æ•¸
def normalize_stock_code(code):
    """
    æ¨™æº–åŒ–è‚¡ç¥¨ä»£ç¢¼ï¼Œç¢ºä¿åƒ'56'æœƒè¢«è½‰æ›æˆ'0056'
    è¦å‰‡ï¼šå¦‚æœæ˜¯ç´”æ•¸å­—ä¸”é•·åº¦å°æ–¼4ï¼Œå‰‡è£œ0åˆ°4ä½æ•¸
    """
    if pd.isna(code) or code == '':
        return ''

    code_str = str(code).strip()
    code_str = code_str.replace('="', '').replace('"', '').replace("'", '')

    if code_str.isdigit() and len(code_str) < 4:
        return code_str.zfill(4)

    return code_str

# ã€ç¬¬äºŒæ­¥-shares_to_lotsã€‘
# å¾ç¬¬äºŒæ­¥ç¨‹å¼è¤‡è£½ shares_to_lots å‡½æ•¸
def shares_to_lots(value):
    """
    å°‡è‚¡æ•¸è½‰æ›ç‚ºå¼µæ•¸(é™¤ä»¥1000å¾Œå–æ•´æ•¸)
    å°æ–¼1000è‚¡è¦–ç‚º0å¼µ
    """
    try:
        if pd.isna(value) or value == '':
            return 0
        if isinstance(value, str):
            value = value.replace(',', '')
        num_value = float(value)
        return int(num_value / 1000)
    except:
        return 0
    
# ã€ç¬¬äºŒæ­¥-format_date_shortã€‘
# å¾ç¬¬äºŒæ­¥ç¨‹å¼è¤‡è£½ format_date_short å‡½æ•¸
def format_date_short(date_str):
    """å°‡ YYYY-MM-DD æ ¼å¼è½‰æ›ç‚º DD (åªé¡¯ç¤ºæ—¥)"""
    try:
        parts = date_str.split('-')
        if len(parts) == 3:
            return f"{parts[2]}"
        return date_str
    except:
        return date_str
    
# ã€ç¬¬äºŒæ­¥-setup_configã€‘
# å¾ç¬¬äºŒæ­¥ç¨‹å¼è¤‡è£½ setup_config å‡½æ•¸ (éœ€è¦ä¿®æ”¹è·¯å¾‘)
def setup_config(market_type='TSE'):
    """
    è¨­å®šæ‰€æœ‰è·¯å¾‘è®Šæ•¸ (GitHub Actions ç‰ˆæœ¬)

    Args:
        market_type: 'TSE' (ä¸Šå¸‚) æˆ– 'OTC' (ä¸Šæ«ƒ)

    Returns:
        dict: åŒ…å«æ‰€æœ‰è·¯å¾‘é…ç½®çš„å­—å…¸
    """
    # GitHub Actions ä½¿ç”¨ç•¶å‰ç›®éŒ„
    base_path = os.getcwd()

    if market_type == 'TSE':
        config = {
            'market_type': market_type,
            'folder_path': os.path.join(base_path, 'StockTSEShares'),
            'stock_daily_folder': os.path.join(base_path, 'StockTSEDaily'),
            'output_folder': os.path.join(base_path, 'StockInfo'),
            'history_folder': os.path.join(base_path, 'StockTSEHistory'),
            'market_list_filename': 'tse_company_list.csv',
            'output_filename': 'tse_analysis_result.xlsx',
            'sigma_threshold': 2.5,
            'aggregate_threshold': None,
            'show_top_n': 100,
            'top_buy_count': 100,   # è²·è¶…å‰nå
            'top_sell_count': 50   # è³£è¶…å‰nå
        }
    else:  # OTC
        config = {
            'market_type': market_type,
            'folder_path': os.path.join(base_path, 'StockOTCShares'),
            'stock_daily_folder': os.path.join(base_path, 'StockOTCDaily'),
            'output_folder': os.path.join(base_path, 'StockInfo'),
            'history_folder': os.path.join(base_path, 'StockOTCHistory'),
            'market_list_filename': 'otc_company_list.csv',
            'output_filename': 'otc_analysis_result.xlsx',
            'sigma_threshold': 2.5,
            'aggregate_threshold': None,
            'show_top_n': 100,
            'top_buy_count': 100,   # è²·è¶…å‰nå
            'top_sell_count': 50   # è³£è¶…å‰nå
        }

    # å»ºç«‹å®Œæ•´è·¯å¾‘
    config['market_list_path'] = os.path.join(config['output_folder'], config['market_list_filename'])
    config['output_path'] = os.path.join(config['output_folder'], config['output_filename'])

    # å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾
    os.makedirs(config['output_folder'], exist_ok=True)
    os.makedirs(config['history_folder'], exist_ok=True)

    print(f"{'='*80}")
    print(f"å¸‚å ´é¡å‹: {market_type} ({'ä¸Šå¸‚' if market_type == 'TSE' else 'ä¸Šæ«ƒ'})")
    print(f"ä¸‰å¤§æ³•äººè³‡æ–™å¤¾: {config['folder_path']}")
    print(f"å€‹è‚¡æ—¥ç·šè³‡æ–™å¤¾: {config['stock_daily_folder']}")
    print(f"è¼¸å‡ºè³‡æ–™å¤¾: {config['output_folder']}")
    print(f"æ­·å²æ•¸æ“šè³‡æ–™å¤¾: {config['history_folder']}")
    print(f"è‚¡ç¥¨æ¸…å–®æª”æ¡ˆ: {config['market_list_path']}")
    print(f"è¼¸å‡ºExcelæª”æ¡ˆ: {config['output_path']}")
    print(f"è²·è¶…åˆ†ææ•¸é‡: å‰ {config['top_buy_count']} å")
    print(f"è³£è¶…åˆ†ææ•¸é‡: å‰ {config['top_sell_count']} å")
    if config['show_top_n'] is not None:
        print(f"å½™æ•´åˆ†ææ¨¡å¼: é¡¯ç¤ºå‰ {config['show_top_n']} å")
    else:
        print(f"å½™æ•´åˆ†æé–¾å€¼: {config['aggregate_threshold']} å¼µ")
    print(f"{'='*80}\n")

    return config
# ã€ç¬¬äºŒæ­¥-load_stock_listã€‘
# å¾ç¬¬äºŒæ­¥ç¨‹å¼è¤‡è£½ load_stock_list å‡½æ•¸
def load_stock_list(market_list_path):
    """
    è®€å–å…è¨±çš„è‚¡ç¥¨ä»£ç¢¼æ¸…å–®å’Œé ˜åŸŸè³‡è¨Š

    Returns:
        tuple: (allowed_stock_codes, stock_sector_map, etf_stock_codes)
    """
    allowed_stock_codes = set()
    stock_sector_map = {}
    etf_stock_codes = set()

    try:
        market_df = pd.read_csv(market_list_path, encoding='utf-8')
        first_column = market_df.iloc[:, 0].apply(normalize_stock_code)
        allowed_stock_codes = set(first_column.tolist())

        if len(market_df.columns) >= 3:
            for idx, row in market_df.iterrows():
                stock_code = normalize_stock_code(row.iloc[0])
                sector = str(row.iloc[2]).strip() if pd.notna(row.iloc[2]) else ''
                stock_sector_map[stock_code] = sector

                if sector.upper() == 'ETF':
                    etf_stock_codes.add(stock_code)

            print(f"{'='*80}")
            print(f"å·²è¼‰å…¥å…è¨±çš„è‚¡ç¥¨ä»£ç¢¼æ¸…å–®: {len(allowed_stock_codes)} æª”")
            print(f"å·²å»ºç«‹é ˜åŸŸæ˜ å°„: {len(stock_sector_map)} æª”")
            print(f"è­˜åˆ¥ETFè‚¡ç¥¨: {len(etf_stock_codes)} æª”")
            print(f"æ¸…å–®æª”æ¡ˆ: {market_list_path}")
            print(f"å‰10å€‹ä»£ç¢¼: {list(allowed_stock_codes)[:10]}")
            if etf_stock_codes:
                print(f"ETFä»£ç¢¼: {sorted(etf_stock_codes)}")
            print(f"{'='*80}\n")
        else:
            print(f"{'='*80}")
            print(f"å·²è¼‰å…¥å…è¨±çš„è‚¡ç¥¨ä»£ç¢¼æ¸…å–®: {len(allowed_stock_codes)} æª”")
            print(f"è­¦å‘Š: æ¬„ä½ä¸è¶³ï¼Œç„¡æ³•è®€å–é ˜åŸŸè³‡è¨Šï¼ˆéœ€è¦è‡³å°‘3æ¬„ï¼‰")
            print(f"{'='*80}\n")

    except FileNotFoundError:
        print(f"è­¦å‘Š: æ‰¾ä¸åˆ° {market_list_path}")
        print("å°‡è™•ç†æ‰€æœ‰è‚¡ç¥¨ä»£ç¢¼")
        return None, {}, set()
    except Exception as e:
        print(f"è®€å–è‚¡ç¥¨æ¸…å–®æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        print("å°‡è™•ç†æ‰€æœ‰è‚¡ç¥¨ä»£ç¢¼")
        return None, {}, set()

    return allowed_stock_codes, stock_sector_map, etf_stock_codes


# ã€ç¬¬äºŒæ­¥-is_allowed_stockã€‘
# å¾ç¬¬äºŒæ­¥ç¨‹å¼è¤‡è£½ is_allowed_stock å‡½æ•¸
def is_allowed_stock(stock_code, allowed_stock_codes):
    """æª¢æŸ¥è‚¡ç¥¨ä»£ç¢¼æ˜¯å¦åœ¨å…è¨±æ¸…å–®ä¸­"""
    if allowed_stock_codes is None:
        return True
    normalized_code = normalize_stock_code(stock_code)
    return normalized_code in allowed_stock_codes

# ã€ç¬¬äºŒæ­¥-get_stock_sectorã€‘
# å¾ç¬¬äºŒæ­¥ç¨‹å¼è¤‡è£½ get_stock_sector å‡½æ•¸
def get_stock_sector(stock_code, stock_sector_map):
    """ç²å–è‚¡ç¥¨ä»£ç¢¼å°æ‡‰çš„é ˜åŸŸ"""
    normalized_code = normalize_stock_code(stock_code)
    return stock_sector_map.get(normalized_code, '')

# ã€ç¬¬äºŒæ­¥-load_stock_daily_pricesã€‘
# å¾ç¬¬äºŒæ­¥ç¨‹å¼è¤‡è£½ load_stock_daily_prices å‡½æ•¸
def load_stock_daily_prices(stock_daily_folder, allowed_stock_codes, num_days=5):
    """
    è®€å–StockTSEDailyçš„æ”¶ç›¤åƒ¹å’Œæ¼²è·Œåƒ¹å·®

    Returns:
        dict: {æ—¥æœŸ: {è­‰åˆ¸ä»£è™Ÿ: {'æ”¶ç›¤åƒ¹': x, 'æ¼²è·Œåƒ¹å·®': y}}}
    """
    stock_daily_prices = {}

    print(f"\n{'='*80}")
    print("é–‹å§‹è®€å– StockTSEDaily çš„æ”¶ç›¤åƒ¹å’Œæ¼²è·Œåƒ¹å·®è³‡æ–™...")
    print(f"{'='*80}")

    if not os.path.exists(stock_daily_folder):
        print(f"è­¦å‘Š: StockTSEDaily è³‡æ–™å¤¾ä¸å­˜åœ¨: {stock_daily_folder}")
        print("å°‡ç„¡æ³•é¡¯ç¤ºæ”¶ç›¤åƒ¹å’Œæ¼²è·Œåƒ¹å·®")
        print(f"{'='*80}\n")
        return stock_daily_prices

    all_daily_files = glob.glob(os.path.join(stock_daily_folder, '*.csv'))
    daily_files_sorted = sorted(all_daily_files, key=lambda x: os.path.basename(x).replace('.csv', ''), reverse=True)
    latest_files = daily_files_sorted[:num_days]

    print(f"æ‰¾åˆ° {len(all_daily_files)} å€‹ StockTSEDaily æª”æ¡ˆ")
    print(f"å°‡è®€å–æœ€è¿‘ {num_days} å€‹æª”æ¡ˆçš„åƒ¹æ ¼è³‡æ–™")

    for daily_file in latest_files:
        try:
            # å…ˆå˜—è©¦ cp950 ç·¨ç¢¼,å¤±æ•—å‰‡ç”¨ utf-8
            try:
                df_daily = pd.read_csv(daily_file, encoding='cp950', low_memory=False)
            except:
                df_daily = pd.read_csv(daily_file, encoding='utf-8', low_memory=False)

            file_date = os.path.basename(daily_file).replace('.csv', '')

            if 'è­‰åˆ¸ä»£è™Ÿ' in df_daily.columns:
                df_daily['è­‰åˆ¸ä»£è™Ÿ'] = df_daily['è­‰åˆ¸ä»£è™Ÿ'].apply(normalize_stock_code)

            if allowed_stock_codes is not None:
                df_daily = df_daily[df_daily['è­‰åˆ¸ä»£è™Ÿ'].isin(allowed_stock_codes)]

            stock_daily_prices[file_date] = {}

            for _, row in df_daily.iterrows():
                stock_code = normalize_stock_code(row['è­‰åˆ¸ä»£è™Ÿ'])
                close_price = row.get('æ”¶ç›¤åƒ¹', '')

                price_sign = ''
                if len(df_daily.columns) > 9:
                    j_col_name = df_daily.columns[9]
                    price_sign = str(row.get(j_col_name, '')).strip()

                price_value = ''
                if len(df_daily.columns) > 10:
                    k_col_name = df_daily.columns[10]
                    price_value = str(row.get(k_col_name, '')).strip()

                if price_sign and price_value and price_value not in ['', '--', 'X']:
                    clean_value = price_value.replace(',', '')
                    price_diff = f"{price_sign}{clean_value}"
                else:
                    price_diff = ''

                stock_daily_prices[file_date][stock_code] = {
                    'æ”¶ç›¤åƒ¹': close_price,
                    'æ¼²è·Œåƒ¹å·®': price_diff
                }

            print(f"  å·²è®€å–: {os.path.basename(daily_file)} - {len(stock_daily_prices[file_date])} æª”è‚¡ç¥¨")

        except Exception as e:
            print(f"è®€å–StockTSEDailyæª”æ¡ˆ {daily_file} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    print(f"å®Œæˆè®€å–åƒ¹æ ¼è³‡æ–™,å…± {len(stock_daily_prices)} å¤©")
    print(f"{'='*80}\n")

    return stock_daily_prices

# ã€ç¬¬äºŒæ­¥-get_latest_filesã€‘
# å¾ç¬¬äºŒæ­¥ç¨‹å¼è¤‡è£½ get_latest_files å‡½æ•¸
def get_latest_files(folder_path, num_files=61):
    """å–å¾—æœ€æ–°çš„Nå€‹æª”æ¡ˆ"""
    csv_files = glob.glob(os.path.join(folder_path, '*.csv'))
    csv_files_sorted = sorted(csv_files, key=lambda x: os.path.basename(x).replace('.csv', ''), reverse=True)
    return csv_files_sorted[:num_files]

# ã€ç¬¬äºŒæ­¥-process_shares_filesã€‘
# å¾ç¬¬äºŒæ­¥ç¨‹å¼è¤‡è£½ process_shares_files å‡½æ•¸
def process_shares_files(latest_files, allowed_stock_codes, stock_daily_prices,
                         stock_sector_map, etf_stock_codes, top_buy_count=50, top_sell_count=20):
    """
    è™•ç†ä¸‰å¤§æ³•äººè²·è³£è¶…æª”æ¡ˆ

    Args:
        top_buy_count: è²·è¶…é¡¯ç¤ºå‰Nå (é è¨­50)
        top_sell_count: è³£è¶…é¡¯ç¤ºå‰Nå (é è¨­20)

    Returns:
        tuple: (all_data, daily_buy_sell_data, etf_daily_data, buy_top20_tracker,
                sell_top20_tracker, daily_buy_stocks, daily_sell_stocks,
                daily_all_stocks, all_historical_data, statistics)
    """
    all_data = []
    daily_buy_sell_data = []
    etf_daily_data = []
    buy_top20_tracker = []
    sell_top20_tracker = []
    daily_buy_stocks = {}
    daily_sell_stocks = {}
    daily_all_stocks = {}
    all_historical_data = {}

    filtered_out_count = 0
    processed_count = 0

    print(f"æ‰¾åˆ° {len(latest_files)} å€‹ CSV æª”æ¡ˆ")
    print(f"å°‡è™•ç†æœ€æ–°çš„ {len(latest_files)} å€‹æª”æ¡ˆç”¨æ–¼æ¨™æº–å·®è¨ˆç®—")
    print(f"æœ€è¿‘5å€‹æª”æ¡ˆ:")
    for i, file in enumerate(latest_files[:5], 1):
        print(f"{i}. {os.path.basename(file)}")

    for file_path in latest_files:
        try:
            df = pd.read_csv(file_path, encoding='utf-8')

            if 'è­‰åˆ¸ä»£è™Ÿ' in df.columns:
                df['è­‰åˆ¸ä»£è™Ÿ'] = df['è­‰åˆ¸ä»£è™Ÿ'].apply(normalize_stock_code)

            if allowed_stock_codes is not None:
                original_count = len(df)
                df = df[df['è­‰åˆ¸ä»£è™Ÿ'].isin(allowed_stock_codes)]
                filtered_count = original_count - len(df)
                filtered_out_count += filtered_count
                processed_count += len(df)

            file_date = os.path.basename(file_path).replace('.csv', '')

            if 'ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸' in df.columns:
                df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'] = pd.to_numeric(
                    df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'].astype(str).str.replace(',', ''),
                    errors='coerce'
                )
                df['è²·è³£è¶…å¼µæ•¸'] = (df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'] / 1000).fillna(0).astype(int)

                # è¨˜éŒ„æ¯å¤©æ‰€æœ‰è‚¡ç¥¨çš„è²·è³£è¶…ç‹€æ…‹
                daily_all_stocks[file_date] = {}
                for _, row in df.iterrows():
                    if pd.notna(row['è­‰åˆ¸ä»£è™Ÿ']) and pd.notna(row['è²·è³£è¶…å¼µæ•¸']):
                        stock_code = normalize_stock_code(row['è­‰åˆ¸ä»£è™Ÿ'])
                        if is_allowed_stock(stock_code, allowed_stock_codes):
                            buy_sell_value = int(row['è²·è³£è¶…å¼µæ•¸'])
                            daily_all_stocks[file_date][stock_code] = buy_sell_value

                            if stock_code not in all_historical_data:
                                all_historical_data[stock_code] = []
                            all_historical_data[stock_code].append((file_date, buy_sell_value))

                # åªè™•ç†å‰5å¤©çš„è©³ç´°è³‡æ–™
                if file_path in latest_files[:5]:
                    print(f"\n{'='*80}")
                    print(f"æª”æ¡ˆ:{os.path.basename(file_path)}")
                    print(f"{'='*80}")

                    # è²·è¶…è™•ç† - ä½¿ç”¨åƒæ•¸æ§åˆ¶æ•¸é‡
                    buy_top = df[df['è²·è³£è¶…å¼µæ•¸'] > 0].nlargest(top_buy_count, 'è²·è³£è¶…å¼µæ•¸')
                    print(f"\nã€è²·è¶… TOP {top_buy_count}ã€‘")
                    print("-" * 80)

                    if len(buy_top) > 0:
                        display_df = buy_top[['è­‰åˆ¸ä»£è™Ÿ', 'è­‰åˆ¸åç¨±', 'è²·è³£è¶…å¼µæ•¸']].copy()
                        print(display_df.to_string(index=False))

                        buy_top20 = df[df['è²·è³£è¶…å¼µæ•¸'] > 0].nlargest(20, 'è²·è³£è¶…å¼µæ•¸')
                        daily_buy_stocks[file_date] = set(buy_top20['è­‰åˆ¸ä»£è™Ÿ'].tolist())

                        buy_output = buy_top[['è­‰åˆ¸ä»£è™Ÿ', 'è­‰åˆ¸åç¨±', 'è²·è³£è¶…å¼µæ•¸']].copy()
                        buy_output['æ—¥æœŸ'] = file_date
                        buy_output['é¡åˆ¥'] = 'è²·è¶…'
                        buy_output['æ’å'] = range(1, len(buy_output) + 1)

                        if file_date in stock_daily_prices:
                            buy_output['æ”¶ç›¤åƒ¹'] = buy_output['è­‰åˆ¸ä»£è™Ÿ'].apply(
                                lambda x: stock_daily_prices[file_date].get(normalize_stock_code(x), {}).get('æ”¶ç›¤åƒ¹', '')
                            )
                            buy_output['æ¼²è·Œåƒ¹å·®'] = buy_output['è­‰åˆ¸ä»£è™Ÿ'].apply(
                                lambda x: stock_daily_prices[file_date].get(normalize_stock_code(x), {}).get('æ¼²è·Œåƒ¹å·®', '')
                            )
                        else:
                            buy_output['æ”¶ç›¤åƒ¹'] = ''
                            buy_output['æ¼²è·Œåƒ¹å·®'] = ''

                        daily_buy_sell_data.append(buy_output)

                        for _, row in buy_top20.iterrows():
                            buy_top20_tracker.append({
                                'è­‰åˆ¸ä»£è™Ÿ': normalize_stock_code(row['è­‰åˆ¸ä»£è™Ÿ']),
                                'è­‰åˆ¸åç¨±': row['è­‰åˆ¸åç¨±'],
                                'æ—¥æœŸ': file_date,
                                'è²·è³£è¶…å¼µæ•¸': int(row['è²·è³£è¶…å¼µæ•¸'])
                            })
                    else:
                        print("ç„¡è²·è¶…è³‡æ–™")
                        daily_buy_stocks[file_date] = set()

                    # è³£è¶…è™•ç† - ä½¿ç”¨åƒæ•¸æ§åˆ¶æ•¸é‡
                    sell_top = df[df['è²·è³£è¶…å¼µæ•¸'] < 0].nsmallest(top_sell_count, 'è²·è³£è¶…å¼µæ•¸')
                    print(f"\nã€è³£è¶… TOP {top_sell_count}ã€‘")
                    print("-" * 80)

                    if len(sell_top) > 0:
                        display_df = sell_top[['è­‰åˆ¸ä»£è™Ÿ', 'è­‰åˆ¸åç¨±', 'è²·è³£è¶…å¼µæ•¸']].copy()
                        print(display_df.to_string(index=False))

                        sell_top20 = df[df['è²·è³£è¶…å¼µæ•¸'] < 0].nsmallest(20, 'è²·è³£è¶…å¼µæ•¸')
                        daily_sell_stocks[file_date] = set(sell_top20['è­‰åˆ¸ä»£è™Ÿ'].tolist())

                        sell_output = sell_top[['è­‰åˆ¸ä»£è™Ÿ', 'è­‰åˆ¸åç¨±', 'è²·è³£è¶…å¼µæ•¸']].copy()
                        sell_output['æ—¥æœŸ'] = file_date
                        sell_output['é¡åˆ¥'] = 'è³£è¶…'
                        sell_output['æ’å'] = range(1, len(sell_output) + 1)

                        if file_date in stock_daily_prices:
                            sell_output['æ”¶ç›¤åƒ¹'] = sell_output['è­‰åˆ¸ä»£è™Ÿ'].apply(
                                lambda x: stock_daily_prices[file_date].get(normalize_stock_code(x), {}).get('æ”¶ç›¤åƒ¹', '')
                            )
                            sell_output['æ¼²è·Œåƒ¹å·®'] = sell_output['è­‰åˆ¸ä»£è™Ÿ'].apply(
                                lambda x: stock_daily_prices[file_date].get(normalize_stock_code(x), {}).get('æ¼²è·Œåƒ¹å·®', '')
                            )
                        else:
                            sell_output['æ”¶ç›¤åƒ¹'] = ''
                            sell_output['æ¼²è·Œåƒ¹å·®'] = ''

                        daily_buy_sell_data.append(sell_output)

                        for _, row in sell_top20.iterrows():
                            sell_top20_tracker.append({
                                'è­‰åˆ¸ä»£è™Ÿ': normalize_stock_code(row['è­‰åˆ¸ä»£è™Ÿ']),
                                'è­‰åˆ¸åç¨±': row['è­‰åˆ¸åç¨±'],
                                'æ—¥æœŸ': file_date,
                                'è²·è³£è¶…å¼µæ•¸': int(row['è²·è³£è¶…å¼µæ•¸'])
                            })
                    else:
                        print("ç„¡è³£è¶…è³‡æ–™")
                        daily_sell_stocks[file_date] = set()

                    # ETFè™•ç†
                    if len(etf_stock_codes) > 0:
                        etf_df = df[df['è­‰åˆ¸ä»£è™Ÿ'].isin(etf_stock_codes)].copy()

                        if len(etf_df) > 0:
                            # ETFè²·è¶…
                            etf_buy_top10 = etf_df[etf_df['è²·è³£è¶…å¼µæ•¸'] > 0].nlargest(10, 'è²·è³£è¶…å¼µæ•¸')
                            if len(etf_buy_top10) > 0:
                                etf_buy_output = etf_buy_top10[['è­‰åˆ¸ä»£è™Ÿ', 'è­‰åˆ¸åç¨±', 'è²·è³£è¶…å¼µæ•¸']].copy()
                                etf_buy_output['æ—¥æœŸ'] = file_date
                                etf_buy_output['é¡åˆ¥'] = 'ETFè²·è¶…'
                                etf_buy_output['æ’å'] = range(1, len(etf_buy_output) + 1)

                                if file_date in stock_daily_prices:
                                    etf_buy_output['æ”¶ç›¤åƒ¹'] = etf_buy_output['è­‰åˆ¸ä»£è™Ÿ'].apply(
                                        lambda x: stock_daily_prices[file_date].get(normalize_stock_code(x), {}).get('æ”¶ç›¤åƒ¹', '')
                                    )
                                    etf_buy_output['æ¼²è·Œåƒ¹å·®'] = etf_buy_output['è­‰åˆ¸ä»£è™Ÿ'].apply(
                                        lambda x: stock_daily_prices[file_date].get(normalize_stock_code(x), {}).get('æ¼²è·Œåƒ¹å·®', '')
                                    )
                                else:
                                    etf_buy_output['æ”¶ç›¤åƒ¹'] = ''
                                    etf_buy_output['æ¼²è·Œåƒ¹å·®'] = ''

                                etf_daily_data.append(etf_buy_output)

                            # ETFè³£è¶…
                            etf_sell_top10 = etf_df[etf_df['è²·è³£è¶…å¼µæ•¸'] < 0].nsmallest(10, 'è²·è³£è¶…å¼µæ•¸')
                            if len(etf_sell_top10) > 0:
                                etf_sell_output = etf_sell_top10[['è­‰åˆ¸ä»£è™Ÿ', 'è­‰åˆ¸åç¨±', 'è²·è³£è¶…å¼µæ•¸']].copy()
                                etf_sell_output['æ—¥æœŸ'] = file_date
                                etf_sell_output['é¡åˆ¥'] = 'ETFè³£è¶…'
                                etf_sell_output['æ’å'] = range(1, len(etf_sell_output) + 1)

                                if file_date in stock_daily_prices:
                                    etf_sell_output['æ”¶ç›¤åƒ¹'] = etf_sell_output['è­‰åˆ¸ä»£è™Ÿ'].apply(
                                        lambda x: stock_daily_prices[file_date].get(normalize_stock_code(x), {}).get('æ”¶ç›¤åƒ¹', '')
                                    )
                                    etf_sell_output['æ¼²è·Œåƒ¹å·®'] = etf_sell_output['è­‰åˆ¸ä»£è™Ÿ'].apply(
                                        lambda x: stock_daily_prices[file_date].get(normalize_stock_code(x), {}).get('æ¼²è·Œåƒ¹å·®', '')
                                    )
                                else:
                                    etf_sell_output['æ”¶ç›¤åƒ¹'] = ''
                                    etf_sell_output['æ¼²è·Œåƒ¹å·®'] = ''

                                etf_daily_data.append(etf_sell_output)

                    df_full = df.copy()
                    df_full['æª”æ¡ˆä¾†æº'] = os.path.basename(file_path)
                    all_data.append(df_full)

        except Exception as e:
            print(f"è®€å–æª”æ¡ˆ {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤:{e}")

    statistics = {
        'filtered_out_count': filtered_out_count,
        'processed_count': processed_count
    }

    if allowed_stock_codes is not None:
        print(f"\n{'='*80}")
        print(f"è‚¡ç¥¨ä»£ç¢¼éæ¿¾çµ±è¨ˆ:")
        print(f"  - å…è¨±çš„è‚¡ç¥¨ä»£ç¢¼ç¸½æ•¸: {len(allowed_stock_codes)}")
        print(f"  - è™•ç†çš„è‚¡ç¥¨ç­†æ•¸: {processed_count}")
        print(f"  - éæ¿¾æ‰çš„è‚¡ç¥¨ç­†æ•¸: {filtered_out_count}")
        if len(etf_stock_codes) > 0:
            print(f"  - ETFè‚¡ç¥¨æ•¸é‡: {len(etf_stock_codes)}")
        print(f"{'='*80}")

    return (all_data, daily_buy_sell_data, etf_daily_data, buy_top20_tracker,
            sell_top20_tracker, daily_buy_stocks, daily_sell_stocks,
            daily_all_stocks, all_historical_data, statistics)

def organize_daily_buy_sell_data_for_html(daily_buy_sell_data_list):
    """
    å°‡ daily_buy_sell_data å¾ DataFrame list è½‰æ›ç‚º HTML éœ€è¦çš„å­—å…¸æ ¼å¼
    
    Args:
        daily_buy_sell_data_list: list of DataFrames æˆ– list of dicts
    
    Returns:
        list of dicts: æ¯å€‹å­—å…¸åŒ…å« {'æ—¥æœŸ': date, 'è²·è¶…': [...], 'è³£è¶…': [...]}
    """
    # å¦‚æœå·²ç¶“æ˜¯å­—å…¸åˆ—è¡¨ï¼Œç›´æ¥è¿”å›
    if daily_buy_sell_data_list and isinstance(daily_buy_sell_data_list, list):
        if len(daily_buy_sell_data_list) > 0 and isinstance(daily_buy_sell_data_list[0], dict):
            if 'æ—¥æœŸ' in daily_buy_sell_data_list[0] and 'è²·è¶…' in daily_buy_sell_data_list[0]:
                return daily_buy_sell_data_list
    
    # æŒ‰æ—¥æœŸåˆ†çµ„
    date_data_map = {}
    
    for df in daily_buy_sell_data_list:
        if hasattr(df, 'empty') and df.empty:
            continue
            
        date = df['æ—¥æœŸ'].iloc[0] if 'æ—¥æœŸ' in df.columns else ''
        category = df['é¡åˆ¥'].iloc[0] if 'é¡åˆ¥' in df.columns else ''
        
        if date not in date_data_map:
            date_data_map[date] = {'æ—¥æœŸ': date, 'è²·è¶…': [], 'è³£è¶…': []}
        
        # è½‰æ› DataFrame ç‚ºå­—å…¸åˆ—è¡¨
        for _, row in df.iterrows():
            stock_dict = {
                'è­‰åˆ¸ä»£è™Ÿ': str(row.get('è­‰åˆ¸ä»£è™Ÿ', '')),
                'è­‰åˆ¸åç¨±': str(row.get('è­‰åˆ¸åç¨±', '')),
                'è²·è³£è¶…å¼µæ•¸': int(row.get('è²·è³£è¶…å¼µæ•¸', 0)),
                'æ”¶ç›¤åƒ¹': row.get('æ”¶ç›¤åƒ¹', ''),
                'æ¼²è·Œ': row.get('æ¼²è·Œåƒ¹å·®', '')
            }
            
            # è™•ç†æ¼²è·Œæ•¸å€¼
            price_diff_str = str(stock_dict['æ¼²è·Œ'])
            if price_diff_str and price_diff_str not in ['', '--', 'X', 'nan']:
                try:
                    clean_value = price_diff_str.replace(',', '').replace('+', '')
                    stock_dict['æ¼²è·Œ'] = float(clean_value)
                except:
                    stock_dict['æ¼²è·Œ'] = 0
            else:
                stock_dict['æ¼²è·Œ'] = 0
            
            if category == 'è²·è¶…':
                date_data_map[date]['è²·è¶…'].append(stock_dict)
            elif category == 'è³£è¶…':
                date_data_map[date]['è³£è¶…'].append(stock_dict)
    
    # è½‰æ›ç‚ºåˆ—è¡¨ä¸¦æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰é¢ï¼‰
    result = list(date_data_map.values())
    result.sort(key=lambda x: x['æ—¥æœŸ'], reverse=True)
    
    return result

def organize_daily_buy_sell_data(daily_buy_sell_data_list):
    """
    å°‡ daily_buy_sell_data å¾ DataFrame list è½‰æ›ç‚ºéœ€è¦çš„å­—å…¸æ ¼å¼
    
    Args:
        daily_buy_sell_data_list: list of DataFrames
    
    Returns:
        list of dicts: æ¯å€‹å­—å…¸åŒ…å« {'æ—¥æœŸ': date, 'è²·è¶…': [...], 'è³£è¶…': [...]}
    """
    # æŒ‰æ—¥æœŸåˆ†çµ„
    date_data_map = {}
    
    for df in daily_buy_sell_data_list:
        if df.empty:
            continue
            
        date = df['æ—¥æœŸ'].iloc[0] if 'æ—¥æœŸ' in df.columns else ''
        category = df['é¡åˆ¥'].iloc[0] if 'é¡åˆ¥' in df.columns else ''
        
        if date not in date_data_map:
            date_data_map[date] = {'æ—¥æœŸ': date, 'è²·è¶…': [], 'è³£è¶…': []}
        
        # è½‰æ› DataFrame ç‚ºå­—å…¸åˆ—è¡¨
        for _, row in df.iterrows():
            stock_dict = {
                'è­‰åˆ¸ä»£è™Ÿ': str(row.get('è­‰åˆ¸ä»£è™Ÿ', '')),
                'è­‰åˆ¸åç¨±': str(row.get('è­‰åˆ¸åç¨±', '')),
                'è²·è³£è¶…å¼µæ•¸': int(row.get('è²·è³£è¶…å¼µæ•¸', 0)),
                'æ”¶ç›¤åƒ¹': row.get('æ”¶ç›¤åƒ¹', ''),
                'æ¼²è·Œ': row.get('æ¼²è·Œåƒ¹å·®', '')  # æ”¹åç‚º 'æ¼²è·Œ'
            }
            
            # è™•ç†æ¼²è·Œæ•¸å€¼
            price_diff_str = str(stock_dict['æ¼²è·Œ'])
            if price_diff_str and price_diff_str not in ['', '--', 'X', 'nan']:
                try:
                    # ç§»é™¤é€—è™Ÿä¸¦è½‰æ›ç‚ºæ•¸å€¼
                    clean_value = price_diff_str.replace(',', '')
                    stock_dict['æ¼²è·Œ'] = float(clean_value)
                except:
                    stock_dict['æ¼²è·Œ'] = 0
            else:
                stock_dict['æ¼²è·Œ'] = 0
            
            if category == 'è²·è¶…':
                date_data_map[date]['è²·è¶…'].append(stock_dict)
            elif category == 'è³£è¶…':
                date_data_map[date]['è³£è¶…'].append(stock_dict)
    
    # è½‰æ›ç‚ºåˆ—è¡¨ä¸¦æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰é¢ï¼‰
    result = list(date_data_map.values())
    result.sort(key=lambda x: x['æ—¥æœŸ'], reverse=True)
    
    return result

# ã€ç¬¬äºŒæ­¥-calculate_stock_statisticsã€‘
# å¾ç¬¬äºŒæ­¥ç¨‹å¼è¤‡è£½ calculate_stock_statistics å‡½æ•¸
def calculate_stock_statistics(all_historical_data, sigma_threshold):
    """
    è¨ˆç®—æ¯å€‹è­‰åˆ¸çš„çµ±è¨ˆæ•¸æ“š(ä½¿ç”¨ä»Šå¤©å¾€å‰60å¤©ï¼Œä¸å«ä»Šå¤©)

    Returns:
        dict: {è­‰åˆ¸ä»£è™Ÿ: {'å¹³å‡å€¼': x, 'æ¨™æº–å·®': y, 'æœ€æ–°å€¼': z, 'Zåˆ†æ•¸': w, 'ç•°å¸¸': bool}}
    """
    print(f"\n{'='*80}")
    print("è¨ˆç®—éå»60å¤©çš„æ¨™æº–å·®...")
    print(f"{'='*80}")

    stock_statistics = {}

    for stock_code, date_values in all_historical_data.items():
        if len(date_values) >= 30:
            sorted_values = sorted(date_values, key=lambda x: x[0], reverse=True)
            latest_value = sorted_values[0][1] if len(sorted_values) > 0 else 0
            historical_values = [v[1] for v in sorted_values[1:61]]

            if len(historical_values) >= 30:
                mean = np.mean(historical_values)
                std = np.std(historical_values)

                if std > 0:
                    z_score = abs((latest_value - mean) / std)
                else:
                    z_score = 0

                stock_statistics[stock_code] = {
                    'å¹³å‡å€¼': mean,
                    'æ¨™æº–å·®': std,
                    'æœ€æ–°å€¼': latest_value,
                    'Zåˆ†æ•¸': z_score,
                    'ç•°å¸¸': z_score >= sigma_threshold
                }

    return stock_statistics

# ã€ç¬¬äºŒæ­¥-analyze_new_entries_and_observablesã€‘
# å¾ç¬¬äºŒæ­¥ç¨‹å¼è¤‡è£½ analyze_new_entries_and_observables å‡½æ•¸
def analyze_new_entries_and_observables(latest_file, daily_buy_stocks, daily_sell_stocks,
                                        daily_all_stocks, stock_statistics, allowed_stock_codes,
                                        sigma_threshold, top_buy_count=50, top_sell_count=20):
    """
    æ‰¾å‡ºæœ€æ–°ä¸€å¤©çš„æ–°é€²æ¦œè­‰åˆ¸å’Œå€¼å¾—è§€å¯Ÿè­‰åˆ¸

    Args:
        top_buy_count: è²·è¶…åˆ†æå‰Nå (é è¨­50)
        top_sell_count: è³£è¶…åˆ†æå‰Nå (é è¨­20)

    Returns:
        tuple: (new_buy_stocks, new_sell_stocks, observable_buy_stocks, observable_sell_stocks,
                latest_date, latest_buy_stocks_n, latest_sell_stocks_n)
    """
    sorted_dates = sorted(daily_buy_stocks.keys(), reverse=True)
    observable_buy_stocks = {}
    observable_sell_stocks = {}
    new_buy_stocks = set()
    new_sell_stocks = set()
    latest_buy_stocks_n = set()
    latest_sell_stocks_n = set()
    latest_date = None

    if len(sorted_dates) >= 2:
        latest_date = sorted_dates[0]
        previous_dates = sorted_dates[1:]

        latest_df = pd.read_csv(latest_file, encoding='utf-8')

        if 'è­‰åˆ¸ä»£è™Ÿ' in latest_df.columns:
            latest_df['è­‰åˆ¸ä»£è™Ÿ'] = latest_df['è­‰åˆ¸ä»£è™Ÿ'].apply(normalize_stock_code)

        if allowed_stock_codes is not None:
            latest_df = latest_df[latest_df['è­‰åˆ¸ä»£è™Ÿ'].isin(allowed_stock_codes)]

        latest_df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'] = pd.to_numeric(
            latest_df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'].astype(str).str.replace(',', ''),
            errors='coerce'
        )
        latest_df['è²·è³£è¶…å¼µæ•¸'] = (latest_df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'] / 1000).fillna(0).astype(int)

        # ä½¿ç”¨åƒæ•¸æ§åˆ¶çš„æ•¸é‡
        buy_top_n = latest_df[latest_df['è²·è³£è¶…å¼µæ•¸'] > 0].nlargest(top_buy_count, 'è²·è³£è¶…å¼µæ•¸')
        sell_top_n = latest_df[latest_df['è²·è³£è¶…å¼µæ•¸'] < 0].nsmallest(top_sell_count, 'è²·è³£è¶…å¼µæ•¸')

        latest_buy_stocks_n = set(buy_top_n['è­‰åˆ¸ä»£è™Ÿ'].tolist())
        latest_sell_stocks_n = set(sell_top_n['è­‰åˆ¸ä»£è™Ÿ'].tolist())

        # è¨ˆç®—æ–°é€²æ¦œ
        previous_buy_stocks = set()
        previous_sell_stocks = set()
        for date in previous_dates[:4]:
            if date in daily_buy_stocks:
                previous_buy_stocks.update(daily_buy_stocks[date])
            if date in daily_sell_stocks:
                previous_sell_stocks.update(daily_sell_stocks[date])

        latest_buy_stocks = daily_buy_stocks.get(latest_date, set())
        latest_sell_stocks = daily_sell_stocks.get(latest_date, set())

        new_buy_stocks = latest_buy_stocks - previous_buy_stocks
        new_sell_stocks = latest_sell_stocks - previous_sell_stocks

        # è²·è¶…å€¼å¾—è§€å¯Ÿ
        for stock_code in latest_buy_stocks_n:
            reasons = []
            z_score = 0
            mean_val = 0
            std_val = 0

            if stock_code in stock_statistics and stock_statistics[stock_code]['ç•°å¸¸']:
                z_score = stock_statistics[stock_code]['Zåˆ†æ•¸']
                mean_val = stock_statistics[stock_code]['å¹³å‡å€¼']
                std_val = stock_statistics[stock_code]['æ¨™æº–å·®']
                reasons.append(f'ç•°å¸¸æ³¢å‹•({z_score:.1f}Ïƒ)')

            positive_days = 0
            for date in previous_dates[:4]:
                if date in daily_all_stocks and stock_code in daily_all_stocks[date]:
                    if daily_all_stocks[date][stock_code] > 0:
                        positive_days += 1
            if positive_days >= 3:
                reasons.append('é€£çºŒè²·è¶…')

            if reasons:
                observable_buy_stocks[stock_code] = ('+'.join(reasons), z_score, mean_val, std_val)

        # è³£è¶…å€¼å¾—è§€å¯Ÿ
        for stock_code in latest_sell_stocks_n:
            reasons = []
            z_score = 0
            mean_val = 0
            std_val = 0

            if stock_code in stock_statistics and stock_statistics[stock_code]['ç•°å¸¸']:
                z_score = stock_statistics[stock_code]['Zåˆ†æ•¸']
                mean_val = stock_statistics[stock_code]['å¹³å‡å€¼']
                std_val = stock_statistics[stock_code]['æ¨™æº–å·®']
                reasons.append(f'ç•°å¸¸æ³¢å‹•({z_score:.1f}Ïƒ)')

            negative_days = 0
            for date in previous_dates[:4]:
                if date in daily_all_stocks and stock_code in daily_all_stocks[date]:
                    if daily_all_stocks[date][stock_code] < 0:
                        negative_days += 1
            if negative_days >= 3:
                reasons.append('é€£çºŒè³£è¶…')

            if reasons:
                observable_sell_stocks[stock_code] = ('+'.join(reasons), z_score, mean_val, std_val)

        print(f"\n{'='*80}")
        print(f"ã€{latest_date} åˆ†æçµæœã€‘")
        print(f"{'='*80}")
        print(f"ä½¿ç”¨æ¨™æº–å·®é–¾å€¼: {sigma_threshold} å€‹æ¨™æº–å·®")
        print(f"è²·è¶…å‰20æ–°é€²æ¦œ: {len(new_buy_stocks)} æª”")
        if new_buy_stocks:
            print(f"  è­‰åˆ¸ä»£è™Ÿ: {', '.join(sorted(new_buy_stocks))}")
        print(f"è³£è¶…å‰20æ–°é€²æ¦œ: {len(new_sell_stocks)} æª”")
        if new_sell_stocks:
            print(f"  è­‰åˆ¸ä»£è™Ÿ: {', '.join(sorted(new_sell_stocks))}")
        print(f"\nè²·è¶…å‰{top_buy_count}å€¼å¾—è§€å¯Ÿ: {len(observable_buy_stocks)} æª”")
        if observable_buy_stocks:
            for code, (reason, z, mean_val, std_val) in sorted(observable_buy_stocks.items()):
                print(f"  {code}: {reason}")
        print(f"è³£è¶…å‰{top_sell_count}å€¼å¾—è§€å¯Ÿ: {len(observable_sell_stocks)} æª”")
        if observable_sell_stocks:
            for code, (reason, z, mean_val, std_val) in sorted(observable_sell_stocks.items()):
                print(f"  {code}: {reason}")

    return (new_buy_stocks, new_sell_stocks, observable_buy_stocks, observable_sell_stocks,
            latest_date, latest_buy_stocks_n, latest_sell_stocks_n)


# ã€ç¬¬äºŒæ­¥-collect_stock_historyã€‘
# å¾ç¬¬äºŒæ­¥ç¨‹å¼è¤‡è£½ collect_stock_history å‡½æ•¸
def collect_stock_history(latest_buy_stocks_n, latest_sell_stocks_n, folder_path, stock_daily_folder,
                          history_folder, allowed_stock_codes):
    """æ”¶é›†è²·è¶…å‰Næª”å’Œè³£è¶…å‰Næª”è‚¡ç¥¨çš„æ­·å²æ•¸æ“š"""
    print(f"\n{'='*80}")
    print(f"é–‹å§‹æ”¶é›†è²·è¶…å‰{len(latest_buy_stocks_n)}æª” + è³£è¶…å‰{len(latest_sell_stocks_n)}æª”è‚¡ç¥¨çš„æ­·å²æ•¸æ“š...")
    print(f"{'='*80}")

    # åˆä½µè²·è¶…å’Œè³£è¶…çš„è‚¡ç¥¨ä»£ç¢¼
    all_target_stocks = latest_buy_stocks_n.union(latest_sell_stocks_n)
    
    if len(all_target_stocks) == 0:
        print(f"æ²’æœ‰è‚¡ç¥¨éœ€è¦æ”¶é›†æ­·å²æ•¸æ“š")
        return

    print(f"ç¸½å…±éœ€è¦æ”¶é›† {len(all_target_stocks)} æª”è‚¡ç¥¨çš„æ­·å²æ•¸æ“š")
    print(f"  - è²·è¶…: {len(latest_buy_stocks_n)} æª”")
    print(f"  - è³£è¶…: {len(latest_sell_stocks_n)} æª”")
    print(f"  - é‡è¤‡: {len(latest_buy_stocks_n & latest_sell_stocks_n)} æª”")

    stock_history_data = {}
    for stock_code in all_target_stocks:
        stock_history_data[stock_code] = {}

    # å¾ StockTSEShares è®€å–
    print("\nå¾ StockTSEShares æ”¶é›†æ•¸æ“š(2025-01-01 ä¹‹å¾Œ)...")
    all_shares_files = glob.glob(os.path.join(folder_path, '*.csv'))

    shares_files_2025 = []
    for file_path in all_shares_files:
        file_date = os.path.basename(file_path).replace('.csv', '')
        if file_date >= '2025-01-01':
            shares_files_2025.append(file_path)

    shares_files_2025 = sorted(shares_files_2025, key=lambda x: os.path.basename(x).replace('.csv', ''), reverse=True)
    print(f"æ‰¾åˆ° {len(shares_files_2025)} å€‹ StockTSEShares æª”æ¡ˆ(2025-01-01 ä¹‹å¾Œ)")

    shares_processed = 0
    for file_path in shares_files_2025:
        try:
            df = pd.read_csv(file_path, encoding='utf-8')

            if 'è­‰åˆ¸ä»£è™Ÿ' in df.columns:
                df['è­‰åˆ¸ä»£è™Ÿ'] = df['è­‰åˆ¸ä»£è™Ÿ'].apply(normalize_stock_code)

            if allowed_stock_codes is not None:
                df = df[df['è­‰åˆ¸ä»£è™Ÿ'].isin(allowed_stock_codes)]

            file_date = os.path.basename(file_path).replace('.csv', '')

            for stock_code in all_target_stocks:
                stock_data = df[df['è­‰åˆ¸ä»£è™Ÿ'] == stock_code]
                if len(stock_data) > 0:
                    row = stock_data.iloc[0]

                    if file_date not in stock_history_data[stock_code]:
                        stock_history_data[stock_code][file_date] = {
                            'æ—¥æœŸ': file_date,
                            'è‚¡ç¥¨ä»£ç¢¼': stock_code,
                            'è‚¡ç¥¨åç¨±': row.get('è­‰åˆ¸åç¨±', '').strip()
                        }

                    stock_history_data[stock_code][file_date]['å¤–é™¸è³‡è²·è³£è¶…å¼µæ•¸'] = shares_to_lots(row.get('å¤–é™¸è³‡è²·è³£è¶…è‚¡æ•¸(ä¸å«å¤–è³‡è‡ªç‡Ÿå•†)', 0))
                    stock_history_data[stock_code][file_date]['æŠ•ä¿¡è²·è³£è¶…å¼µæ•¸'] = shares_to_lots(row.get('æŠ•ä¿¡è²·è³£è¶…è‚¡æ•¸', 0))
                    stock_history_data[stock_code][file_date]['è‡ªç‡Ÿå•†è²·è³£è¶…å¼µæ•¸'] = shares_to_lots(row.get('è‡ªç‡Ÿå•†è²·è³£è¶…è‚¡æ•¸', 0))

            shares_processed += 1

        except Exception as e:
            print(f"è®€å–StockTSESharesæª”æ¡ˆ {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    print(f"æˆåŠŸè™•ç† {shares_processed} å€‹ StockTSEShares æª”æ¡ˆ")

    # å¾ StockTSEDaily è®€å–
    if os.path.exists(stock_daily_folder):
        print("\nå¾ StockTSEDaily æ”¶é›†æ•¸æ“š(2025-01-01 ä¹‹å¾Œ)...")

        all_daily_files = glob.glob(os.path.join(stock_daily_folder, '*.csv'))

        daily_files_2025 = []
        for file_path in all_daily_files:
            file_date = os.path.basename(file_path).replace('.csv', '')
            if file_date >= '2025-01-01':
                daily_files_2025.append(file_path)

        daily_files_2025 = sorted(daily_files_2025, key=lambda x: os.path.basename(x).replace('.csv', ''), reverse=True)
        print(f"æ‰¾åˆ° {len(daily_files_2025)} å€‹ StockTSEDaily æª”æ¡ˆ(2025-01-01 ä¹‹å¾Œ)")

        stock_data_count = {code: 0 for code in all_target_stocks}
        daily_processed = 0

        for daily_file in daily_files_2025:
            try:
                # å…ˆå˜—è©¦ cp950 ç·¨ç¢¼ï¼Œå¤±æ•—å‰‡ç”¨ utf-8
                try:
                    df_daily = pd.read_csv(daily_file, encoding='cp950', low_memory=False)
                except:
                    df_daily = pd.read_csv(daily_file, encoding='utf-8', low_memory=False)

                file_date = os.path.basename(daily_file).replace('.csv', '')

                if 'è­‰åˆ¸ä»£è™Ÿ' in df_daily.columns:
                    df_daily['è­‰åˆ¸ä»£è™Ÿ'] = df_daily['è­‰åˆ¸ä»£è™Ÿ'].apply(normalize_stock_code)

                if allowed_stock_codes is not None:
                    df_daily = df_daily[df_daily['è­‰åˆ¸ä»£è™Ÿ'].isin(allowed_stock_codes)]

                for stock_code in all_target_stocks:
                    stock_data = df_daily[df_daily['è­‰åˆ¸ä»£è™Ÿ'] == stock_code]

                    if len(stock_data) > 0:
                        row = stock_data.iloc[0]

                        if file_date not in stock_history_data[stock_code]:
                            stock_history_data[stock_code][file_date] = {
                                'æ—¥æœŸ': file_date,
                                'è‚¡ç¥¨ä»£ç¢¼': stock_code,
                                'è‚¡ç¥¨åç¨±': row.get('è­‰åˆ¸åç¨±', '').strip()
                            }

                        stock_history_data[stock_code][file_date]['æˆäº¤å¼µæ•¸'] = shares_to_lots(row.get('æˆäº¤è‚¡æ•¸', 0))
                        stock_history_data[stock_code][file_date]['æˆäº¤ç­†æ•¸'] = row.get('æˆäº¤ç­†æ•¸', '')
                        stock_history_data[stock_code][file_date]['æˆäº¤é‡‘é¡'] = row.get('æˆäº¤é‡‘é¡', '')
                        stock_history_data[stock_code][file_date]['é–‹ç›¤åƒ¹'] = row.get('é–‹ç›¤åƒ¹', '')
                        stock_history_data[stock_code][file_date]['æœ€é«˜åƒ¹'] = row.get('æœ€é«˜åƒ¹', '')
                        stock_history_data[stock_code][file_date]['æœ€ä½åƒ¹'] = row.get('æœ€ä½åƒ¹', '')
                        stock_history_data[stock_code][file_date]['æ”¶ç›¤åƒ¹'] = row.get('æ”¶ç›¤åƒ¹', '')
                        stock_history_data[stock_code][file_date]['æœ¬ç›Šæ¯”'] = row.get('æœ¬ç›Šæ¯”', '')

                        stock_data_count[stock_code] += 1

                daily_processed += 1

            except Exception as e:
                print(f"è®€å–StockTSEDailyæª”æ¡ˆ {daily_file} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        print(f"æˆåŠŸè™•ç† {daily_processed} å€‹ StockTSEDaily æª”æ¡ˆ")

        print(f"\nè³‡æ–™çµ±è¨ˆ(å‰5æª”è‚¡ç¥¨):")
        for i, code in enumerate(list(all_target_stocks)[:5]):
            shares_count = len([d for d in stock_history_data[code].keys()])
            daily_count = stock_data_count[code]
            print(f"  {code}: ç¸½å…± {shares_count} å¤©è³‡æ–™,å…¶ä¸­ {daily_count} å¤©æœ‰åƒ¹æ ¼è³‡æ–™")
    else:
        print(f"\nè­¦å‘Š: StockTSEDaily è³‡æ–™å¤¾ä¸å­˜åœ¨: {stock_daily_folder}")

    # å„²å­˜æ­·å²æ•¸æ“š
    print("\nå„²å­˜æ­·å²æ•¸æ“šåˆ° StockTSEHistory...")

    if not os.path.exists(history_folder):
        os.makedirs(history_folder, exist_ok=True)
        print(f"å·²å»ºç«‹è³‡æ–™å¤¾: {history_folder}")

    saved_count = 0
    for stock_code, date_dict in stock_history_data.items():
        if len(date_dict) > 0:
            history_list = list(date_dict.values())
            history_df = pd.DataFrame(history_list)

            column_order = [
                'æ—¥æœŸ', 'è‚¡ç¥¨ä»£ç¢¼', 'è‚¡ç¥¨åç¨±',
                'æˆäº¤å¼µæ•¸', 'æˆäº¤ç­†æ•¸', 'æˆäº¤é‡‘é¡',
                'é–‹ç›¤åƒ¹', 'æœ€é«˜åƒ¹', 'æœ€ä½åƒ¹', 'æ”¶ç›¤åƒ¹',
                'æœ¬ç›Šæ¯”', 'å¤–é™¸è³‡è²·è³£è¶…å¼µæ•¸', 'æŠ•ä¿¡è²·è³£è¶…å¼µæ•¸', 'è‡ªç‡Ÿå•†è²·è³£è¶…å¼µæ•¸'
            ]

            existing_columns = [col for col in column_order if col in history_df.columns]
            history_df = history_df[existing_columns]
            history_df = history_df.sort_values('æ—¥æœŸ', ascending=False)

            output_file = os.path.join(history_folder, f"{stock_code}.csv")
            history_df.to_csv(output_file, index=False, encoding='utf-8-sig')
            saved_count += 1

            if saved_count <= 5:
                print(f"  å·²å„²å­˜: {stock_code}.csv ({len(history_list)} ç­†è¨˜éŒ„)")

    print(f"\nå®Œæˆ! å…±å„²å­˜ {saved_count} å€‹è‚¡ç¥¨çš„æ­·å²æ•¸æ“šåˆ°: {history_folder}")
    print(f"æ¯å€‹æª”æ¡ˆåŒ…å«æœ€è¿‘100å¤©çš„åˆä½µæ•¸æ“š(StockTSEDaily + StockTSEShares)")
    print(f"æ³¨æ„: æ‰€æœ‰è‚¡æ•¸æ¬„ä½å·²è½‰æ›ç‚ºå¼µæ•¸(é™¤ä»¥1000å–æ•´æ•¸)")

# ã€ç¬¬äºŒæ­¥-aggregate_analysisã€‘
# å¾ç¬¬äºŒæ­¥ç¨‹å¼è¤‡è£½ aggregate_analysis å‡½æ•¸
def aggregate_analysis(buy_top20_tracker, sell_top20_tracker, stock_sector_map, aggregate_threshold=10000, show_top_n=None):
    """
    å½™æ•´åˆ†æè²·è¶…å‰20å’Œè³£è¶…å‰20

    Args:
        aggregate_threshold: å½™æ•´åˆ†æçš„è²·è³£è¶…å¼µæ•¸é–¾å€¼ (ç•¶ show_top_n ç‚º None æ™‚ä½¿ç”¨)
        show_top_n: ç›´æ¥é¡¯ç¤ºå‰ N å (å¦‚æœè¨­å®šæ­¤åƒæ•¸ï¼Œå‰‡å¿½ç•¥ aggregate_threshold)

    Returns:
        tuple: (buy_stocks, sell_stocks, both_stocks_set, both_stocks_df)
    """
    if not buy_top20_tracker or not sell_top20_tracker:
        return None, None, set(), None

    all_tracker = buy_top20_tracker + sell_top20_tracker
    all_df = pd.DataFrame(all_tracker)

    summary = all_df.groupby(['è­‰åˆ¸ä»£è™Ÿ', 'è­‰åˆ¸åç¨±']).agg({
        'è²·è³£è¶…å¼µæ•¸': 'sum',
        'æ—¥æœŸ': 'count'
    }).reset_index()
    summary.columns = ['è­‰åˆ¸ä»£è™Ÿ', 'è­‰åˆ¸åç¨±', 'è²·è³£è¶…ç¸½å’Œ', 'å‡ºç¾æ¬¡æ•¸']
    summary['è²·è³£è¶…ç¸½å’Œ'] = summary['è²·è³£è¶…ç¸½å’Œ'].astype(int)

    buy_summary = summary[summary['è²·è³£è¶…ç¸½å’Œ'] > 0].copy()
    buy_summary.columns = ['è­‰åˆ¸ä»£è™Ÿ', 'è­‰åˆ¸åç¨±', 'è²·è¶…ç¸½å’Œ', 'è²·è¶…å‡ºç¾æ¬¡æ•¸']

    sell_summary = summary[summary['è²·è³£è¶…ç¸½å’Œ'] < 0].copy()
    sell_summary.columns = ['è­‰åˆ¸ä»£è™Ÿ', 'è­‰åˆ¸åç¨±', 'è³£è¶…ç¸½å’Œ', 'è³£è¶…å‡ºç¾æ¬¡æ•¸']

    # æ‰¾å‡ºåŒæ™‚å‡ºç¾åœ¨è²·è³£è¶…çš„è­‰åˆ¸
    buy_dates_by_stock = {}
    sell_dates_by_stock = {}

    for item in buy_top20_tracker:
        stock_code = item['è­‰åˆ¸ä»£è™Ÿ']
        if stock_code not in buy_dates_by_stock:
            buy_dates_by_stock[stock_code] = []
        buy_dates_by_stock[stock_code].append(item['æ—¥æœŸ'])

    for item in sell_top20_tracker:
        stock_code = item['è­‰åˆ¸ä»£è™Ÿ']
        if stock_code not in sell_dates_by_stock:
            sell_dates_by_stock[stock_code] = []
        sell_dates_by_stock[stock_code].append(item['æ—¥æœŸ'])

    all_buy_stocks = set(buy_dates_by_stock.keys())
    all_sell_stocks = set(sell_dates_by_stock.keys())
    both_stocks_set = all_buy_stocks & all_sell_stocks

    print(f"\n{'='*80}")
    print(f"ç™¼ç¾ {len(both_stocks_set)} æª”è­‰åˆ¸åŒæ™‚å‡ºç¾åœ¨è²·è¶…å‰20å’Œè³£è¶…å‰20")
    print("(åœ¨5å¤©å…§,æœ‰äº›å¤©é€²è²·è¶…æ¦œã€æœ‰äº›å¤©é€²è³£è¶…æ¦œ)")
    print(f"{'='*80}")

    # è²·è¶…åˆ†æ
    print(f"\n{'='*80}")
    if show_top_n is not None:
        print(f"ã€è²·è¶…åˆ†æã€‘æœ€è¿‘5å¤©è²·è³£è¶…æ·¨å€¼æ’åå‰{show_top_n}å")
    elif aggregate_threshold > 0:
        print(f"ã€è²·è¶…åˆ†æã€‘æœ€è¿‘5å¤©è²·è³£è¶…æ·¨å€¼ç‚ºæ­£ä¸”>={aggregate_threshold}å¼µçš„è­‰åˆ¸")
    else:
        print(f"ã€è²·è¶…åˆ†æã€‘æœ€è¿‘5å¤©è²·è³£è¶…æ·¨å€¼ç‚ºæ­£çš„æ‰€æœ‰è­‰åˆ¸")
    print("(è²·è³£è¶…æ·¨å€¼ = 5å¤©å…§æ‰€æœ‰è²·è³£è¶…å¼µæ•¸çš„ç¸½å’Œ)")
    print(f"{'='*80}\n")

    # æ ¹æ“šåƒæ•¸æ±ºå®šç¯©é¸æ–¹å¼
    if show_top_n is not None:
        buy_stocks = buy_summary.sort_values('è²·è¶…ç¸½å’Œ', ascending=False).head(show_top_n).copy()
    else:
        buy_stocks = buy_summary[buy_summary['è²·è¶…ç¸½å’Œ'] >= aggregate_threshold].sort_values('è²·è¶…ç¸½å’Œ', ascending=False).copy()

    buy_stocks['è­‰åˆ¸é ˜åŸŸ'] = buy_stocks['è­‰åˆ¸ä»£è™Ÿ'].apply(lambda x: get_stock_sector(x, stock_sector_map))
    buy_stocks['æ³¨æ„äº‹é …'] = buy_stocks['è­‰åˆ¸ä»£è™Ÿ'].apply(
        lambda x: 'âš ï¸åŒæ™‚å‡ºç¾åœ¨è³£è¶…' if x in both_stocks_set else ''
    )

    display_buy_stocks = buy_stocks.copy()
    display_buy_stocks['è²·è¶…ç¸½å’Œ'] = display_buy_stocks['è²·è¶…ç¸½å’Œ'].apply(lambda x: f"{x:,}")

    if len(buy_stocks) > 0:
        print(display_buy_stocks.to_string(index=False))
        print(f"\nå…±æ‰¾åˆ° {len(buy_stocks)} æª”ç¬¦åˆæ¢ä»¶çš„è­‰åˆ¸")
    else:
        print("æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„è­‰åˆ¸")

    # è³£è¶…åˆ†æ
    print(f"\n{'='*80}")
    if show_top_n is not None:
        print(f"ã€è³£è¶…åˆ†æã€‘æœ€è¿‘5å¤©è²·è³£è¶…æ·¨å€¼æ’åå‰{show_top_n}å(ç”±å¤§åˆ°å°)")
    elif aggregate_threshold > 0:
        print(f"ã€è³£è¶…åˆ†æã€‘æœ€è¿‘5å¤©è²·è³£è¶…æ·¨å€¼ç‚ºè² ä¸”<=-{aggregate_threshold}å¼µçš„è­‰åˆ¸")
    else:
        print(f"ã€è³£è¶…åˆ†æã€‘æœ€è¿‘5å¤©è²·è³£è¶…æ·¨å€¼ç‚ºè² çš„æ‰€æœ‰è­‰åˆ¸")
    print("(è²·è³£è¶…æ·¨å€¼ = 5å¤©å…§æ‰€æœ‰è²·è³£è¶…å¼µæ•¸çš„ç¸½å’Œ)")
    print(f"{'='*80}\n")

    # æ ¹æ“šåƒæ•¸æ±ºå®šç¯©é¸æ–¹å¼
    if show_top_n is not None:
        sell_stocks = sell_summary.sort_values('è³£è¶…ç¸½å’Œ', ascending=True).head(show_top_n).copy()
    else:
        sell_stocks = sell_summary[sell_summary['è³£è¶…ç¸½å’Œ'] <= -aggregate_threshold].sort_values('è³£è¶…ç¸½å’Œ', ascending=True).copy()

    sell_stocks['è­‰åˆ¸é ˜åŸŸ'] = sell_stocks['è­‰åˆ¸ä»£è™Ÿ'].apply(lambda x: get_stock_sector(x, stock_sector_map))
    sell_stocks['æ³¨æ„äº‹é …'] = sell_stocks['è­‰åˆ¸ä»£è™Ÿ'].apply(
        lambda x: 'âš ï¸åŒæ™‚å‡ºç¾åœ¨è²·è¶…' if x in both_stocks_set else ''
    )

    display_sell_stocks = sell_stocks.copy()
    display_sell_stocks['è³£è¶…ç¸½å’Œ'] = display_sell_stocks['è³£è¶…ç¸½å’Œ'].apply(lambda x: f"{x:,}")

    if len(sell_stocks) > 0:
        print(display_sell_stocks.to_string(index=False))
        print(f"\nå…±æ‰¾åˆ° {len(sell_stocks)} æª”ç¬¦åˆæ¢ä»¶çš„è­‰åˆ¸")
    else:
        print("æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„è­‰åˆ¸")

    # åŒæ™‚å‡ºç¾åœ¨è²·è³£è¶…çš„è­‰åˆ¸è©³ç´°åˆ†æ
    both_stocks_df = None
    if len(both_stocks_set) > 0:
        print(f"\n{'='*80}")
        print("ã€ç‰¹åˆ¥æ³¨æ„ã€‘åŒæ™‚å‡ºç¾åœ¨è²·è¶…å‰20å’Œè³£è¶…å‰20çš„è­‰åˆ¸")
        print("(åœ¨5å¤©å…§,æœ‰äº›å¤©é€²è²·è¶…æ¦œã€æœ‰äº›å¤©é€²è³£è¶…æ¦œ)")
        print(f"{'='*80}\n")

        # å–å¾—æ‰€æœ‰æ—¥æœŸä¸¦æ’åºï¼ˆæœ€æ–°åœ¨å‰ï¼‰
        all_available_dates = sorted(list(set([item['æ—¥æœŸ'] for item in all_tracker])), reverse=True)
        
        both_stocks_detail = []
        for stock_code in both_stocks_set:
            stock_all_data = all_df[all_df['è­‰åˆ¸ä»£è™Ÿ'] == stock_code]
            stock_name = stock_all_data.iloc[0]['è­‰åˆ¸åç¨±']
            total_sum = int(stock_all_data['è²·è³£è¶…å¼µæ•¸'].sum())

            buy_dates = buy_dates_by_stock.get(stock_code, [])
            sell_dates = sell_dates_by_stock.get(stock_code, [])

            buy_dates_short = [format_date_short(d) for d in sorted(buy_dates)]
            sell_dates_short = [format_date_short(d) for d in sorted(sell_dates)]

            buy_dates_str = ', '.join(buy_dates_short)
            sell_dates_str = ', '.join(sell_dates_short)

            buy_sum = int(all_df[(all_df['è­‰åˆ¸ä»£è™Ÿ'] == stock_code) & (all_df['è²·è³£è¶…å¼µæ•¸'] > 0)]['è²·è³£è¶…å¼µæ•¸'].sum())
            sell_sum = int(all_df[(all_df['è­‰åˆ¸ä»£è™Ÿ'] == stock_code) & (all_df['è²·è³£è¶…å¼µæ•¸'] < 0)]['è²·è³£è¶…å¼µæ•¸'].sum())

            # å»ºç«‹éå»5å¤©çš„è²·è³£è¶…ç‹€æ…‹ (æœ€æ–°åœ¨å·¦ï¼Œç¢ºä¿é¡¯ç¤ºæ‰€æœ‰5å€‹æ—¥æœŸ)
            date_status = []
            for date in all_available_dates[:5]:  # åªå–å‰5å€‹æ—¥æœŸ
                day_short = format_date_short(date)
                if date in buy_dates:
                    date_status.append(('buy', day_short))
                elif date in sell_dates:
                    date_status.append(('sell', day_short))
                else:
                    date_status.append(('neutral', day_short))
            
            both_stocks_detail.append({
                'è­‰åˆ¸ä»£è™Ÿ': stock_code,
                'è­‰åˆ¸åç¨±': stock_name,
                'è­‰åˆ¸é ˜åŸŸ': get_stock_sector(stock_code, stock_sector_map),
                'è²·è¶…æ¬¡æ•¸': len(buy_dates),
                'è²·è¶…æ—¥æœŸ': buy_dates_str,
                'è²·è¶…ç¸½å’Œ': buy_sum,
                'è³£è¶…æ¬¡æ•¸': len(sell_dates),
                'è³£è¶…æ—¥æœŸ': sell_dates_str,
                'è³£è¶…ç¸½å’Œ': sell_sum,
                'æ·¨è²·è³£è¶…': total_sum,
                'æ—¥æœŸç‹€æ…‹': date_status  # æ–°å¢ï¼šåŒ…å« (ç‹€æ…‹, æ—¥æœŸ) çš„åˆ—è¡¨
            })

        both_stocks_df = pd.DataFrame(both_stocks_detail)
        both_stocks_df = both_stocks_df.sort_values('æ·¨è²·è³£è¶…', ascending=False)

        display_both = both_stocks_df.copy()
        for col in ['è²·è¶…ç¸½å’Œ', 'è³£è¶…ç¸½å’Œ', 'æ·¨è²·è³£è¶…']:
            display_both[col] = display_both[col].apply(lambda x: f"{x:,}")

        print(display_both.to_string(index=False))
        print(f"\nå…± {len(both_stocks_df)} æª”è­‰åˆ¸")

    return buy_stocks, sell_stocks, both_stocks_set, both_stocks_df

# ã€ç¬¬äºŒæ­¥-export_to_excelã€‘
# å¾ç¬¬äºŒæ­¥ç¨‹å¼è¤‡è£½ export_to_excel å‡½æ•¸
def export_to_excel(output_path, buy_stocks, sell_stocks, both_stocks_set, both_stocks_df,
                   daily_buy_sell_data, etf_daily_data, latest_date, new_buy_stocks,
                   new_sell_stocks, observable_buy_stocks, observable_sell_stocks,
                   stock_sector_map, etf_stock_codes):
    """å»ºç«‹ä¸¦ç¾åŒ– Excel æª”æ¡ˆ"""

    if buy_stocks is None and sell_stocks is None:
        print("æ²’æœ‰æ•¸æ“šå¯ä»¥è¼¸å‡ºåˆ°Excel")
        return

    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
        # å·¥ä½œè¡¨1: å½™æ•´åˆ†æ
        startrow = 0

        summary_df = pd.DataFrame([['ã€å½™æ•´è²·è¶…åˆ†æã€‘æœ€è¿‘5å¤©è²·è³£è¶…æ·¨å€¼>=10000å¼µ (æ·¨å€¼=5å¤©è²·è³£è¶…ç¸½å’Œ)']],
                                 columns=[''])
        summary_df.to_excel(writer, sheet_name='å½™æ•´åˆ†æ', index=False, header=False, startrow=startrow)
        startrow += 2

        buy_stocks_output = buy_stocks[['è­‰åˆ¸ä»£è™Ÿ', 'è­‰åˆ¸é ˜åŸŸ', 'è­‰åˆ¸åç¨±', 'è²·è¶…ç¸½å’Œ', 'æ³¨æ„äº‹é …']].copy()
        buy_stocks_output.to_excel(writer, sheet_name='å½™æ•´åˆ†æ', index=False, startrow=startrow)
        startrow += len(buy_stocks_output) + 3

        summary_df2 = pd.DataFrame([['ã€å½™æ•´è³£è¶…åˆ†æã€‘æœ€è¿‘5å¤©è²·è³£è¶…æ·¨å€¼<=-10000å¼µ (æ·¨å€¼=5å¤©è²·è³£è¶…ç¸½å’Œ)']],
                                  columns=[''])
        summary_df2.to_excel(writer, sheet_name='å½™æ•´åˆ†æ', index=False, header=False, startrow=startrow)
        startrow += 2

        sell_stocks_output = sell_stocks[['è­‰åˆ¸ä»£è™Ÿ', 'è­‰åˆ¸é ˜åŸŸ', 'è­‰åˆ¸åç¨±', 'è³£è¶…ç¸½å’Œ', 'æ³¨æ„äº‹é …']].copy()
        sell_stocks_output.to_excel(writer, sheet_name='å½™æ•´åˆ†æ', index=False, startrow=startrow)
        startrow += len(sell_stocks_output) + 3

        if both_stocks_df is not None and len(both_stocks_set) > 0:
            summary_df3 = pd.DataFrame([['ã€ç‰¹åˆ¥æ³¨æ„ã€‘åŒæ™‚å‡ºç¾åœ¨è²·è¶…å‰20å’Œè³£è¶…å‰20çš„è­‰åˆ¸(å«æ—¥æœŸæ˜ç´°)']],
                                      columns=[''])
            summary_df3.to_excel(writer, sheet_name='å½™æ•´åˆ†æ', index=False, header=False, startrow=startrow)
            startrow += 2
            both_stocks_df.to_excel(writer, sheet_name='å½™æ•´åˆ†æ', index=False, startrow=startrow)

        # å·¥ä½œè¡¨2-6: æ¯æ—¥è²·è³£è¶…
        if daily_buy_sell_data:
            # æª¢æŸ¥æ˜¯å¦ç‚º DataFrame list
            if daily_buy_sell_data and len(daily_buy_sell_data) > 0:
                if hasattr(daily_buy_sell_data[0], 'empty'):  # æ˜¯ DataFrame
                    daily_df = pd.concat(daily_buy_sell_data, ignore_index=True)
                else:  # æ˜¯å­—å…¸ï¼Œéœ€è¦è½‰æ›
                    all_rows = []
                    for day_data in daily_buy_sell_data:
                        date = day_data['æ—¥æœŸ']
                        for stock in day_data.get('è²·è¶…', []):
                            all_rows.append({
                                'æ—¥æœŸ': date,
                                'é¡åˆ¥': 'è²·è¶…',
                                'æ’å': stock.get('æ’å', ''),
                                'è­‰åˆ¸ä»£è™Ÿ': stock['è­‰åˆ¸ä»£è™Ÿ'],
                                'è­‰åˆ¸åç¨±': stock['è­‰åˆ¸åç¨±'],
                                'è²·è³£è¶…å¼µæ•¸': stock['è²·è³£è¶…å¼µæ•¸'],
                                'æ”¶ç›¤åƒ¹': stock['æ”¶ç›¤åƒ¹'],
                                'æ¼²è·Œåƒ¹å·®': stock['æ¼²è·Œ']
                            })
                        for stock in day_data.get('è³£è¶…', []):
                            all_rows.append({
                                'æ—¥æœŸ': date,
                                'é¡åˆ¥': 'è³£è¶…',
                                'æ’å': stock.get('æ’å', ''),
                                'è­‰åˆ¸ä»£è™Ÿ': stock['è­‰åˆ¸ä»£è™Ÿ'],
                                'è­‰åˆ¸åç¨±': stock['è­‰åˆ¸åç¨±'],
                                'è²·è³£è¶…å¼µæ•¸': stock['è²·è³£è¶…å¼µæ•¸'],
                                'æ”¶ç›¤åƒ¹': stock['æ”¶ç›¤åƒ¹'],
                                'æ¼²è·Œåƒ¹å·®': stock['æ¼²è·Œ']
                            })
                    daily_df = pd.DataFrame(all_rows) if all_rows else pd.DataFrame()
            else:
                daily_df = pd.DataFrame()

            for date in sorted(daily_df['æ—¥æœŸ'].unique(), reverse=True):
                date_data = daily_df[daily_df['æ—¥æœŸ'] == date]
                sheet_name = date.replace('-', '')[:8]
                startrow = 0
                is_latest = (date == latest_date)

                # è²·è¶…éƒ¨åˆ†
                buy_data = date_data[date_data['é¡åˆ¥'] == 'è²·è¶…'].copy()
                if len(buy_data) > 0:
                    top_count = len(buy_data)
                    title_df = pd.DataFrame([[f'ã€{date} è²·è¶… TOP {top_count}ã€‘']], columns=[''])
                    title_df.to_excel(writer, sheet_name=sheet_name, index=False, header=False, startrow=startrow)
                    startrow += 2

                    buy_data['è­‰åˆ¸é ˜åŸŸ'] = buy_data['è­‰åˆ¸ä»£è™Ÿ'].apply(lambda x: get_stock_sector(x, stock_sector_map))

                    if is_latest:
                        buy_data['æ–°é€²æ¦œ'] = buy_data['è­‰åˆ¸ä»£è™Ÿ'].apply(
                            lambda x: 'ğŸ”¥NEW' if normalize_stock_code(x) in new_buy_stocks else ''
                        )
                        buy_data['å€¼å¾—è§€å¯Ÿ'] = buy_data['è­‰åˆ¸ä»£è™Ÿ'].apply(
                            lambda x: f'ğŸ‘€{observable_buy_stocks[normalize_stock_code(x)][0]}' if normalize_stock_code(x) in observable_buy_stocks else ''
                        )
                        buy_data['çµ±è¨ˆæ•¸æ“š(60å¤©)'] = buy_data['è­‰åˆ¸ä»£è™Ÿ'].apply(
                            lambda x: f'å‡:{observable_buy_stocks[normalize_stock_code(x)][2]:.0f} æ¨™å·®:{observable_buy_stocks[normalize_stock_code(x)][3]:.0f}'
                            if normalize_stock_code(x) in observable_buy_stocks and observable_buy_stocks[normalize_stock_code(x)][2] != 0 else ''
                        )
                        buy_data_output = buy_data[['æ’å', 'è­‰åˆ¸ä»£è™Ÿ', 'è­‰åˆ¸é ˜åŸŸ', 'è­‰åˆ¸åç¨±', 'æ”¶ç›¤åƒ¹', 'æ¼²è·Œåƒ¹å·®', 'è²·è³£è¶…å¼µæ•¸', 'æ–°é€²æ¦œ', 'å€¼å¾—è§€å¯Ÿ', 'çµ±è¨ˆæ•¸æ“š(60å¤©)']].copy()
                    else:
                        buy_data_output = buy_data[['æ’å', 'è­‰åˆ¸ä»£è™Ÿ', 'è­‰åˆ¸é ˜åŸŸ', 'è­‰åˆ¸åç¨±', 'æ”¶ç›¤åƒ¹', 'æ¼²è·Œåƒ¹å·®', 'è²·è³£è¶…å¼µæ•¸']].copy()

                    buy_data_output.to_excel(writer, sheet_name=sheet_name, index=False, startrow=startrow)
                    startrow += len(buy_data_output) + 3

                # è³£è¶…éƒ¨åˆ†
                sell_data = date_data[date_data['é¡åˆ¥'] == 'è³£è¶…'].copy()
                if len(sell_data) > 0:
                    top_count = len(sell_data)
                    title_df2 = pd.DataFrame([[f'ã€{date} è³£è¶… TOP {top_count}ã€‘']], columns=[''])
                    title_df2.to_excel(writer, sheet_name=sheet_name, index=False, header=False, startrow=startrow)
                    startrow += 2

                    sell_data['è­‰åˆ¸é ˜åŸŸ'] = sell_data['è­‰åˆ¸ä»£è™Ÿ'].apply(lambda x: get_stock_sector(x, stock_sector_map))

                    if is_latest:
                        sell_data['æ–°é€²æ¦œ'] = sell_data['è­‰åˆ¸ä»£è™Ÿ'].apply(
                            lambda x: 'ğŸ“‰NEW' if normalize_stock_code(x) in new_sell_stocks else ''
                        )
                        sell_data['å€¼å¾—è§€å¯Ÿ'] = sell_data['è­‰åˆ¸ä»£è™Ÿ'].apply(
                            lambda x: f'ğŸ‘€{observable_sell_stocks[normalize_stock_code(x)][0]}' if normalize_stock_code(x) in observable_sell_stocks else ''
                        )
                        sell_data['çµ±è¨ˆæ•¸æ“š(60å¤©)'] = sell_data['è­‰åˆ¸ä»£è™Ÿ'].apply(
                            lambda x: f'å‡:{observable_sell_stocks[normalize_stock_code(x)][2]:.0f} æ¨™å·®:{observable_sell_stocks[normalize_stock_code(x)][3]:.0f}'
                            if normalize_stock_code(x) in observable_sell_stocks and observable_sell_stocks[normalize_stock_code(x)][2] != 0 else ''
                        )
                        sell_data_output = sell_data[['æ’å', 'è­‰åˆ¸ä»£è™Ÿ', 'è­‰åˆ¸é ˜åŸŸ', 'è­‰åˆ¸åç¨±', 'æ”¶ç›¤åƒ¹', 'æ¼²è·Œåƒ¹å·®', 'è²·è³£è¶…å¼µæ•¸', 'æ–°é€²æ¦œ', 'å€¼å¾—è§€å¯Ÿ', 'çµ±è¨ˆæ•¸æ“š(60å¤©)']].copy()
                    else:
                        sell_data_output = sell_data[['æ’å', 'è­‰åˆ¸ä»£è™Ÿ', 'è­‰åˆ¸é ˜åŸŸ', 'è­‰åˆ¸åç¨±', 'æ”¶ç›¤åƒ¹', 'æ¼²è·Œåƒ¹å·®', 'è²·è³£è¶…å¼µæ•¸']].copy()

                    sell_data_output.to_excel(writer, sheet_name=sheet_name, index=False, startrow=startrow)
                    startrow += len(sell_data_output) + 3

                # ETFæ•¸æ“š
                if len(etf_stock_codes) > 0 and etf_daily_data:
                    etf_df = pd.concat(etf_daily_data, ignore_index=True)
                    etf_date_data = etf_df[etf_df['æ—¥æœŸ'] == date]

                    if len(etf_date_data) > 0:
                        etf_buy = etf_date_data[etf_date_data['é¡åˆ¥'] == 'ETFè²·è¶…']
                        if len(etf_buy) > 0:
                            title_df3 = pd.DataFrame([[f'ã€{date} ETFè²·è¶… TOP 10ã€‘']], columns=[''])
                            title_df3.to_excel(writer, sheet_name=sheet_name, index=False, header=False, startrow=startrow)
                            startrow += 2

                            etf_buy_output = etf_buy[['æ’å', 'è­‰åˆ¸ä»£è™Ÿ', 'è­‰åˆ¸åç¨±', 'æ”¶ç›¤åƒ¹', 'æ¼²è·Œåƒ¹å·®', 'è²·è³£è¶…å¼µæ•¸']].copy()
                            etf_buy_output.to_excel(writer, sheet_name=sheet_name, index=False, startrow=startrow)
                            startrow += len(etf_buy_output) + 3

                        etf_sell = etf_date_data[etf_date_data['é¡åˆ¥'] == 'ETFè³£è¶…']
                        if len(etf_sell) > 0:
                            title_df4 = pd.DataFrame([[f'ã€{date} ETFè³£è¶… TOP 10ã€‘']], columns=[''])
                            title_df4.to_excel(writer, sheet_name=sheet_name, index=False, header=False, startrow=startrow)
                            startrow += 2

                            etf_sell_output = etf_sell[['æ’å', 'è­‰åˆ¸ä»£è™Ÿ', 'è­‰åˆ¸åç¨±', 'æ”¶ç›¤åƒ¹', 'æ¼²è·Œåƒ¹å·®', 'è²·è³£è¶…å¼µæ•¸']].copy()
                            etf_sell_output.to_excel(writer, sheet_name=sheet_name, index=False, startrow=startrow)

# ã€ç¬¬äºŒæ­¥-beautify_excelã€‘
# å¾ç¬¬äºŒæ­¥ç¨‹å¼è¤‡è£½ beautify_excel å‡½æ•¸
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®æ­£ç‰ˆ HTML ç”Ÿæˆå‡½æ•¸ - åŠ å¼·æ‰‹æ©ŸéŸ¿æ‡‰å¼æ”¯æ´
"""

def generate_complete_html(output_path, buy_stocks, sell_stocks, both_stocks_set, both_stocks_df,
                          daily_buy_sell_data, etf_daily_data, latest_date, new_buy_stocks,
                          new_sell_stocks, observable_buy_stocks, observable_sell_stocks,
                          stock_sector_map, etf_stock_codes, market_type='TSE'):
    """ç”Ÿæˆå®Œæ•´çš„ HTML åˆ†æå ±å‘Š - æ‰‹æ©Ÿå„ªåŒ–ç‰ˆ"""
    
    market_name = 'ä¸Šå¸‚' if market_type == 'TSE' else 'ä¸Šæ«ƒ'
    
    # æº–å‚™æ—¥æœŸæ¨™ç±¤
    date_tabs = []
    if daily_buy_sell_data and len(daily_buy_sell_data) > 0:
        for i, day_data in enumerate(daily_buy_sell_data[:5]):
            date = day_data['æ—¥æœŸ']
            if len(date) == 8:
                formatted_date = f"{date[0:4]}/{date[4:6]}/{date[6:8]}"
            else:
                formatted_date = date
            date_tabs.append((i + 1, formatted_date, day_data))
    
    # HTML é–‹å§‹ - åŠ å¼·æ‰‹æ©ŸéŸ¿æ‡‰å¼è¨­è¨ˆ
    html_content = f"""<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>{market_name}ä¸‰å¤§æ³•äººåˆ†æå ±å‘Š</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        
        body {{
            font-family: "Microsoft JhengHei", "Segoe UI", Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 2px;
            min-height: 100vh;
            font-size: 15px; /* åŸºç¤å­—é«”ç¸®å° */
        }}
        
        .container {{
            max-width: 1400px;
            margin: 0 auto;
        }}
        
        .tabs {{
            background: white;
            border-radius: 15px;
            padding: 4px 4px 0 4px; /* ç¸®å°é–“è· */
            margin-bottom: 5px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.2);
        }}
        
        .tab-buttons {{
            display: flex;
            gap: 2px; /* ç¸®å°é–“è· */
            flex-wrap: wrap;
            border-bottom: 2px solid #e2e8f0;
            padding-bottom: 10px;
        }}
        
        .tab-button {{
            padding: 2px 4px; /* ç¸®å°æŒ‰éˆ• */
            border: none;
            background: #f7fafc;
            color: #4a5568;
            cursor: pointer;
            border-radius: 8px 8px 0 0;
            font-size: 1.1em; /* ç¸®å°å­—é«” */
            font-weight: 600;
            transition: all 0.3s ease;
            font-family: "Microsoft JhengHei", sans-serif;
        }}
        
        .tab-button:hover {{
            background: #edf2f7;
        }}
        
        .tab-button.active {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
        }}
        
        .tab-content {{
            display: none;
            padding: 4px 0; /* ç¸®å°é–“è· */
        }}
        
        .tab-content.active {{
            display: block;
        }}
        
        .section {{
            background: white;
            padding: 4px; /* ç¸®å°é–“è· */
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.2);
            margin-bottom: 5px;
        }}
        
        .section-title {{
            font-size: 1.1em; /* ç¸®å°æ¨™é¡Œ */
            margin-bottom: 4px;
            padding-bottom: 2px;
            border-bottom: 3px solid #667eea;
            color: #2d3748;
        }}
        
        .section-title.buy {{
            border-bottom-color: #48bb78;
        }}
        
        .section-title.sell {{
            border-bottom-color: #f56565;
        }}
        
        .section-title.etf {{
            border-bottom-color: #ed8936;
        }}
        
        .section-title.attention {{
            border-bottom-color: #ecc94b;
        }}
        
        /* è¡¨æ ¼å®¹å™¨ - å…è¨±æ°´å¹³æ»¾å‹• */
        .table-container {{
            width: 100%;
            overflow-x: auto;
            -webkit-overflow-scrolling: touch;
            margin-bottom: 4px;
        }}
        
        table {{
            width: 100%;
            min-width: 400px; /* æœ€å°å¯¬åº¦ */
            border-collapse: collapse;
            background: white;
            font-size: 0.95em; /* ç¸®å°è¡¨æ ¼å­—é«” */
        }}
        
        thead {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            position: sticky;
            top: 0;
            z-index: 10;
        }}
        
        thead.buy {{
            background: linear-gradient(135deg, #48bb78 0%, #38a169 100%);
        }}
        
        thead.sell {{
            background: linear-gradient(135deg, #f56565 0%, #e53e3e 100%);
        }}
        
        thead.etf {{
            background: linear-gradient(135deg, #ed8936 0%, #dd6b20 100%);
        }}
        
        thead.attention {{
            background: linear-gradient(135deg, #ecc94b 0%, #d69e2e 100%);
        }}
        
        th {{
            padding: 2px 4px; /* ç¸®å°é–“è· */
            text-align: left;
            font-weight: 600;
            font-size: 1.1em;
            white-space: nowrap; /* æ¨™é¡Œä¸æ›è¡Œ */
        }}
        
        td {{
            padding: 2px 4px; /* ç¸®å°é–“è· */
            border-bottom: 1px solid #e2e8f0;
            font-size: 1.1em;
        }}
        
        tr:hover {{
            background-color: #f7fafc;
        }}
        
        .rank {{
            font-weight: bold;
            color: #667eea;
            font-size: 1em;
        }}
        
        .stock-code {{
            font-weight: 600;
            color: #2d3748;
            white-space: nowrap;
            width: 50px; /* ç¸®å°ä»£è™Ÿæ¬„å¯¬ */
            max-width: 50px;
        }}
        
        .stock-name {{
            font-weight: 600;
            color: #4a5568;
            max-width: 120px; /* å¢åŠ åç¨±å¯¬åº¦ */
            overflow: hidden;
            text-overflow: ellipsis;
            white-space: nowrap;
        }}
        
        .volume-positive {{
            color: #e53e3e;
            font-weight: 600;
            white-space: nowrap;
        }}
        
        .volume-negative {{
            color: #38a169;
            font-weight: 600;
            white-space: nowrap;
        }}
        
        .price-up {{
            color: #e53e3e;
            font-weight: 600;
            width: 60px; /* ç¸®å°æ”¶ç›¤åƒ¹æ¬„å¯¬ */
            max-width: 60px;
        }}
        
        .price-down {{
            color: #38a169;
            font-weight: 600;
            width: 60px; /* ç¸®å°æ”¶ç›¤åƒ¹æ¬„å¯¬ */
            max-width: 60px;
        }}
        
        .badge {{
            display: inline-block;
            padding: 2px 6px; /* ç¸®å°å¾½ç«  */
            border-radius: 10px;
            font-size: 0.75em;
            font-weight: 600;
            margin-left: 3px;
        }}
        
        .badge-new {{
            background-color: #fed7d7;
            color: #c53030;
        }}
        
        .badge-watch {{
            background-color: #fef5e7;
            color: #d69e2e;
        }}
        
        .badge-alert {{
            background-color: #feebc8;
            color: #c05621;
        }}
        
        .footer {{
            background: white;
            padding: 15px;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.2);
            text-align: center;
            color: #718096;
            font-size: 1.1em;
        }}
        
        /* æ‰‹æ©Ÿå°ˆç”¨æ¨£å¼ */
        @media (max-width: 768px) {{
            body {{
                padding: 2px;
                font-size: 13px;
            }}
            
            .section {{
                padding: 2px;
                margin-bottom: 4px;
            }}
            
            .section-title {{
                font-size: 1.1em;
                margin-bottom: 5px;
            }}
            
            table {{
                font-size: 0.75em; /* æ‰‹æ©Ÿé€²ä¸€æ­¥ç¸®å° */
                min-width: 350px;
            }}
            
            th, td {{
                padding: 1px 2px; /* æ‰‹æ©Ÿæ›´ç·Šæ¹Š */
            }}
            
            .tab-button {{
                padding: 3px 6px;
                font-size: 0.85em;
            }}
            
            .stock-name {{
                max-width: 90px; /* æ‰‹æ©Ÿç¸®çŸ­åç¨± */
            }}
            
            .stock-code {{
                width: 45px;
                max-width: 45px;
            }}
            
            .price-up, .price-down {{
                width: 55px;
                max-width: 55px;
            }}
        }}
        
        /* æ¥µå°è¢å¹• */
        @media (max-width: 480px) {{
            table {{
                font-size: 0.7em;
                min-width: 320px;
            }}
            
            th, td {{
                padding: 1px 1px;
            }}
            
            .stock-name {{
                max-width: 70px;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="tabs">
            <div class="tab-buttons">
                <button class="tab-button active" onclick="switchTab(0)">å½™æ•´åˆ†æ</button>"""
    
    # æ·»åŠ æ—¥æœŸæ¨™ç±¤æŒ‰éˆ•
    for tab_idx, formatted_date, _ in date_tabs:
        html_content += f"""
                <button class="tab-button" onclick="switchTab({tab_idx})">{formatted_date}</button>"""
    
    html_content += """
            </div>

            <div id="tab-0" class="tab-content active">"""
    
    # ========== Tab 0: å½™æ•´åˆ†æ ==========
    # è²·è¶…åˆ†æ
    if buy_stocks is not None and len(buy_stocks) > 0:
        html_content += """
                <div class="section">
                    <h2 class="section-title buy">ğŸ“ˆ å½™æ•´è²·è¶…åˆ†æ</h2>
                    <p style="color: #718096; margin-bottom: 5px; font-size: 1.1em;">æœ€è¿‘5å¤©è²·è³£è¶…æ·¨å€¼ >= 10000å¼µ</p>
                    <div class="table-container">
                        <table>
                            <thead class="buy">
                                <tr>
                                    <th>ä»£è™Ÿ</th>
                                    <th>é ˜åŸŸ</th>
                                    <th>åç¨±</th>
                                    <th>è²·è¶…ç¸½å’Œ</th>
                                    <th>æ³¨æ„</th>
                                </tr>
                            </thead>
                            <tbody>
"""
        for _, row in buy_stocks.iterrows():
            code = row["è­‰åˆ¸ä»£è™Ÿ"]
            sector = row.get("è­‰åˆ¸é ˜åŸŸ", "")
            name = row["è­‰åˆ¸åç¨±"]
            total = row["è²·è¶…ç¸½å’Œ"]
            note = row.get("æ³¨æ„äº‹é …", "")
            note_html = f'<span class="badge badge-alert">âš ï¸</span>' if note else ''
            
            html_content += f"""
                                <tr>
                                    <td class="stock-code">{code}</td>
                                    <td>{sector}</td>
                                    <td class="stock-name" title="{name}">{name}</td>
                                    <td class="volume-positive">{total:,}</td>
                                    <td>{note_html}</td>
                                </tr>
"""
        html_content += """
                            </tbody>
                        </table>
                    </div>
                </div>
                """
    
    # è³£è¶…åˆ†æ
    if sell_stocks is not None and len(sell_stocks) > 0:
        html_content += """
                <div class="section">
                    <h2 class="section-title sell">ğŸ“‰ å½™æ•´è³£è¶…åˆ†æ</h2>
                    <p style="color: #718096; margin-bottom: 5px; font-size: 1.1em;">æœ€è¿‘5å¤©è²·è³£è¶…æ·¨å€¼ <= -10000å¼µ</p>
                    <div class="table-container">
                        <table>
                            <thead class="sell">
                                <tr>
                                    <th>ä»£è™Ÿ</th>
                                    <th>é ˜åŸŸ</th>
                                    <th>åç¨±</th>
                                    <th>è³£è¶…ç¸½å’Œ</th>
                                    <th>æ³¨æ„</th>
                                </tr>
                            </thead>
                            <tbody>
"""
        for _, row in sell_stocks.iterrows():
            code = row["è­‰åˆ¸ä»£è™Ÿ"]
            sector = row.get("è­‰åˆ¸é ˜åŸŸ", "")
            name = row["è­‰åˆ¸åç¨±"]
            total = row["è³£è¶…ç¸½å’Œ"]
            note = row.get("æ³¨æ„äº‹é …", "")
            note_html = f'<span class="badge badge-alert">âš ï¸</span>' if note else ''
            
            html_content += f"""
                                <tr>
                                    <td class="stock-code">{code}</td>
                                    <td>{sector}</td>
                                    <td class="stock-name" title="{name}">{name}</td>
                                    <td class="volume-negative">{total:,}</td>
                                    <td>{note_html}</td>
                                </tr>
"""
        html_content += """
                            </tbody>
                        </table>
                    </div>
                </div>
                """
    
    # ç‰¹åˆ¥æ³¨æ„
    if both_stocks_df is not None and len(both_stocks_df) > 0:
        html_content += """
                <div class="section">
                    <h2 class="section-title attention">âš ï¸ ç‰¹åˆ¥æ³¨æ„</h2>
                    <p style="color: #718096; margin-bottom: 5px; font-size: 1.1em;">åŒæ™‚å‡ºç¾åœ¨è²·è¶…èˆ‡è³£è¶…å‰20</p>
                    <div class="table-container">
                        <table>
                            <thead class="attention">
                                <tr>
                                    <th>ä»£è™Ÿ</th>
                                    <th>åç¨±</th>
                                    <th>é ˜åŸŸ</th>
                                    <th>è²·è¶…å’Œ</th>
                                    <th>è³£è¶…å’Œ</th>
                                    <th>è²·è³£è¶…æ—¥æœŸ</th>
                                </tr>
                            </thead>
                            <tbody>
"""
        for _, row in both_stocks_df.iterrows():
            code = row["è­‰åˆ¸ä»£è™Ÿ"]
            name = row["è­‰åˆ¸åç¨±"]
            sector = row.get("è­‰åˆ¸é ˜åŸŸ", "")
            buy_total = row.get("è²·è¶…ç¸½å’Œ", 0)
            sell_total = row.get("è³£è¶…ç¸½å’Œ", 0)
            date_status = row.get("æ—¥æœŸç‹€æ…‹", [])
            
            # ç”Ÿæˆå¸¶é¡è‰²çš„æ—¥æœŸåˆ—è¡¨
            date_html_parts = []
            for status, day in date_status:
                if status == 'buy':
                    date_html_parts.append(f'<span style="color: #e53e3e; font-weight: 600;">{day}</span>')
                elif status == 'sell':
                    date_html_parts.append(f'<span style="color: #38a169; font-weight: 600;">{day}</span>')
                else:
                    date_html_parts.append(f'<span style="color: #4a5568;">{day}</span>')
            
            dates_display = ', '.join(date_html_parts)
            
            html_content += f"""
                                <tr>
                                    <td class="stock-code">{code}</td>
                                    <td class="stock-name" title="{name}">{name}</td>
                                    <td>{sector}</td>
                                    <td class="volume-positive">{buy_total:,}</td>
                                    <td class="volume-negative">{sell_total:,}</td>
                                    <td style="font-size: 0.9em;">{dates_display}</td>
                                </tr>
"""
        html_content += """
                            </tbody>
                        </table>
                    </div>
                </div>
"""
    
    html_content += """
            </div>
"""
    
    # ========== Tab 1-5: æ¯æ—¥è²·è³£è¶… ==========
    for tab_idx, formatted_date, day_data in date_tabs:
        html_content += f"""
            <div id="tab-{tab_idx}" class="tab-content">"""
        
        # è²·è¶… TOP
        buy_data = day_data.get('è²·è¶…', [])
        if buy_data:
            buy_count = len(buy_data)
            html_content += f"""
                <div class="section">
                    <h2 class="section-title buy">ğŸ“ˆ è²·è¶… TOP {buy_count} ({formatted_date})</h2>
                    <div class="table-container">
                        <table>
                            <thead class="buy">
                                <tr>
                                    <th>æ’å</th>
                                    <th>ä»£è™Ÿ</th>
                                    <th>åç¨±</th>
                                    <th>æ”¶ç›¤åƒ¹</th>
                                    <th>æ¼²è·Œ</th>
                                    <th>è²·è³£è¶…</th>
                                </tr>
                            </thead>
                            <tbody>
"""
            for idx, stock in enumerate(buy_data, 1):
                code = stock.get('è­‰åˆ¸ä»£è™Ÿ', '')
                name = stock.get('è­‰åˆ¸åç¨±', '')
                close_price = stock.get('æ”¶ç›¤åƒ¹', 0)
                price_change = stock.get('æ¼²è·Œ', 0)
                volume = stock.get('è²·è³£è¶…å¼µæ•¸', 0)
                
                if isinstance(price_change, (int, float)):
                    if price_change > 0:
                        price_class = 'price-up'
                        price_str = f'+{price_change}'
                    elif price_change < 0:
                        price_class = 'price-down'
                        price_str = str(price_change)
                    else:
                        price_class = ''
                        price_str = '0'
                else:
                    price_class = ''
                    price_str = str(price_change)
                
                html_content += f"""
                                <tr>
                                    <td class="rank">{idx}</td>
                                    <td class="stock-code">{code}</td>
                                    <td class="stock-name" title="{name}">{name}</td>
                                    <td>{close_price}</td>
                                    <td class="{price_class}">{price_str}</td>
                                    <td class="volume-positive">{volume:,}</td>
                                </tr>
"""
            html_content += """
                            </tbody>
                        </table>
                    </div>
                </div>
"""
        
        # è³£è¶… TOP
        sell_data = day_data.get('è³£è¶…', [])
        if sell_data:
            sell_count = len(sell_data)
            html_content += f"""
                <div class="section">
                    <h2 class="section-title sell">ğŸ“‰ è³£è¶… TOP {sell_count} ({formatted_date})</h2>
                    <div class="table-container">
                        <table>
                            <thead class="sell">
                                <tr>
                                    <th>æ’å</th>
                                    <th>ä»£è™Ÿ</th>
                                    <th>åç¨±</th>
                                    <th>æ”¶ç›¤åƒ¹</th>
                                    <th>æ¼²è·Œ</th>
                                    <th>è²·è³£è¶…</th>
                                </tr>
                            </thead>
                            <tbody>
"""
            for idx, stock in enumerate(sell_data, 1):
                code = stock.get('è­‰åˆ¸ä»£è™Ÿ', '')
                name = stock.get('è­‰åˆ¸åç¨±', '')
                close_price = stock.get('æ”¶ç›¤åƒ¹', 0)
                price_change = stock.get('æ¼²è·Œ', 0)
                volume = stock.get('è²·è³£è¶…å¼µæ•¸', 0)
                
                if isinstance(price_change, (int, float)):
                    if price_change > 0:
                        price_class = 'price-up'
                        price_str = f'+{price_change}'
                    elif price_change < 0:
                        price_class = 'price-down'
                        price_str = str(price_change)
                    else:
                        price_class = ''
                        price_str = '0'
                else:
                    price_class = ''
                    price_str = str(price_change)
                
                html_content += f"""
                                <tr>
                                    <td class="rank">{idx}</td>
                                    <td class="stock-code">{code}</td>
                                    <td class="stock-name" title="{name}">{name}</td>
                                    <td>{close_price}</td>
                                    <td class="{price_class}">{price_str}</td>
                                    <td class="volume-negative">{volume:,}</td>
                                </tr>
"""
            html_content += """
                            </tbody>
                        </table>
                    </div>
                </div>
"""
        
        html_content += """
            </div>
"""
    
    # Footer
    from datetime import datetime
    current_time = datetime.now().strftime('%Y-%m-%d')
    
    html_content += f"""
        </div>
        
        <div class="footer">
            <p>è³‡æ–™ä¾†æºï¼šå°ç£è­‰åˆ¸äº¤æ˜“æ‰€ | ç”Ÿæˆæ™‚é–“ï¼š{current_time}</p>
        </div>
    </div>
    
    <script>
        function switchTab(tabIndex) {{
            const allContents = document.querySelectorAll('.tab-content');
            allContents.forEach(content => {{
                content.classList.remove('active');
            }});
            
            const allButtons = document.querySelectorAll('.tab-button');
            allButtons.forEach(button => {{
                button.classList.remove('active');
            }});
            
            document.getElementById('tab-' + tabIndex).classList.add('active');
            allButtons[tabIndex].classList.add('active');
        }}
        
        // ç¦æ­¢é›™æŒ‡ç¸®æ”¾
        document.addEventListener('touchstart', function(e) {{
            if (e.touches.length > 1) {{
                e.preventDefault();
            }}
        }}, {{ passive: false }});
        
        // ç¦æ­¢æ‰‹å‹¢ç¸®æ”¾
        document.addEventListener('gesturestart', function(e) {{
            e.preventDefault();
        }});
    </script>
</body>
</html>"""
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    print(f"âœ“ æ‰‹æ©Ÿå„ªåŒ–ç‰ˆ HTML å·²å„²å­˜: {output_path}")

def beautify_excel(output_path):
    """ç¾åŒ– Excel æ ¼å¼"""
    wb = load_workbook(output_path)

    border = Border(
        left=Side(style='thin', color='000000'),
        right=Side(style='thin', color='000000'),
        top=Side(style='thin', color='000000'),
        bottom=Side(style='thin', color='000000')
    )

    header_fill_buy = PatternFill(start_color="90EE90", end_color="90EE90", fill_type="solid")
    header_fill_sell = PatternFill(start_color="FFB6C1", end_color="FFB6C1", fill_type="solid")
    header_fill_warning = PatternFill(start_color="FFD700", end_color="FFD700", fill_type="solid")
    header_fill_observable = PatternFill(start_color="FFA500", end_color="FFA500", fill_type="solid")
    header_fill_etf = PatternFill(start_color="87CEEB", end_color="87CEEB", fill_type="solid")
    title_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
    title_fill_warning = PatternFill(start_color="FF8C00", end_color="FF8C00", fill_type="solid")
    title_fill_etf = PatternFill(start_color="4169E1", end_color="4169E1", fill_type="solid")
    new_fill = PatternFill(start_color="FF69B4", end_color="FF69B4", fill_type="solid")

    red_font = Font(bold=True, color="FF0000", size=11)
    green_font = Font(bold=True, color="00FF00", size=11)

    title_font = Font(bold=True, size=14, color="FFFFFF")
    center_align = Alignment(horizontal="center", vertical="center")

    for sheet_name in wb.sheetnames:
        ws = wb[sheet_name]

        ws.column_dimensions['A'].width = 10
        ws.column_dimensions['B'].width = 12
        ws.column_dimensions['C'].width = 18
        ws.column_dimensions['D'].width = 20
        ws.column_dimensions['E'].width = 13
        ws.column_dimensions['F'].width = 13
        ws.column_dimensions['G'].width = 13
        ws.column_dimensions['H'].width = 12
        ws.column_dimensions['I'].width = 25
        ws.column_dimensions['J'].width = 20

        for row in ws.iter_rows(min_row=1, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
            for cell in row:
                cell.border = border
                cell.alignment = Alignment(horizontal="center", vertical="center")

        price_diff_col_idx = None
        for row in ws.iter_rows(min_row=1, max_row=1):
            for cell in row:
                if cell.value == 'æ¼²è·Œåƒ¹å·®':
                    price_diff_col_idx = cell.column
                    break

        for row in ws.iter_rows():
            for cell in row:
                if cell.value and isinstance(cell.value, str) and 'ã€' in str(cell.value):
                    if 'ETF' in str(cell.value):
                        cell.fill = title_fill_etf
                    elif 'ç‰¹åˆ¥æ³¨æ„' in str(cell.value):
                        cell.fill = title_fill_warning
                    else:
                        cell.fill = title_fill
                    cell.font = title_font
                    cell.alignment = center_align
                    cell.border = border
                    max_col = ws.max_column
                    ws.merge_cells(start_row=cell.row, start_column=1,
                                  end_row=cell.row, end_column=max_col)
                    for col in range(1, max_col + 1):
                        ws.cell(row=cell.row, column=col).border = border

                elif cell.value in ['è­‰åˆ¸åç¨±', 'è­‰åˆ¸ä»£è™Ÿ', 'è­‰åˆ¸é ˜åŸŸ', 'è²·è¶…ç¸½å’Œ(å¼µ)', 'è³£è¶…ç¸½å’Œ(å¼µ)',
                                   'æ’å', 'è²·è³£è¶…å¼µæ•¸', 'æ³¨æ„äº‹é …', 'æ·¨è²·è³£è¶…(å¼µ)', 'è²·è¶…æ—¥æœŸ', 'è³£è¶…æ—¥æœŸ',
                                   'è²·è¶…æ¬¡æ•¸', 'è³£è¶…æ¬¡æ•¸', 'è²·è¶…ç¸½å’Œ', 'è³£è¶…ç¸½å’Œ', 'æ·¨è²·è³£è¶…',
                                   'æ–°é€²æ¦œ', 'å€¼å¾—è§€å¯Ÿ', 'çµ±è¨ˆæ•¸æ“š(60å¤©)', 'æ”¶ç›¤åƒ¹', 'æ¼²è·Œåƒ¹å·®']:
                    is_buy_section = False
                    is_warning_section = False
                    is_etf_section = False
                    for check_row in range(cell.row, 0, -1):
                        title_cell = ws.cell(row=check_row, column=1).value
                        if title_cell and isinstance(title_cell, str) and 'ã€' in title_cell:
                            if 'ETF' in title_cell:
                                is_etf_section = True
                            elif 'ç‰¹åˆ¥æ³¨æ„' in title_cell:
                                is_warning_section = True
                            elif 'è²·è¶…' in title_cell and 'è³£è¶…' not in title_cell:
                                is_buy_section = True
                            break

                    if cell.value == 'æ–°é€²æ¦œ':
                        cell.fill = new_fill
                    elif cell.value == 'å€¼å¾—è§€å¯Ÿ':
                        cell.fill = header_fill_observable
                    elif cell.value == 'çµ±è¨ˆæ•¸æ“š(60å¤©)':
                        cell.fill = PatternFill(start_color="87CEEB", end_color="87CEEB", fill_type="solid")
                    elif is_etf_section:
                        cell.fill = header_fill_etf
                    elif is_warning_section:
                        cell.fill = header_fill_warning
                    elif is_buy_section:
                        cell.fill = header_fill_buy
                    else:
                        cell.fill = header_fill_sell
                    cell.font = Font(bold=True, size=11)
                    cell.alignment = center_align
                    cell.border = border

                elif cell.value is not None and cell.value != '':
                    if price_diff_col_idx and cell.column == price_diff_col_idx and cell.row > 1:
                        cell_str = str(cell.value).strip()

                        if cell_str and cell_str not in ['', '--', 'X', 'x']:
                            if cell_str.startswith('+'):
                                cell.font = red_font
                            elif cell_str.startswith('-'):
                                cell.font = green_font
                    elif cell.value == 'ğŸ”¥NEW':
                        cell.font = Font(bold=True, color="FF0000", size=11)
                    elif cell.value == 'ğŸ“‰NEW':
                        cell.font = Font(bold=True, color="00A86B", size=11)
                    elif isinstance(cell.value, str) and 'ğŸ‘€' in str(cell.value):
                        cell.font = Font(bold=True, color="FF8C00", size=10)
                    cell.border = border

    wb.save(output_path)

def run_step2_analysis(base_dir, market_type):
    """åŸ·è¡Œç¬¬äºŒæ­¥ï¼šåˆ†æç¨‹å¼ (GitHub Actions ç‰ˆæœ¬)"""
    print(f"\n{'ğŸ”¥'*40}")
    print(f"ç¬¬äºŒæ­¥åˆ†æï¼š{market_type} ({'ä¸Šå¸‚' if market_type == 'TSE' else 'ä¸Šæ«ƒ'})")
    print(f"{'ğŸ”¥'*40}\n")

    # è¨­å®šé…ç½® (ä½¿ç”¨ç•¶å‰ç›®éŒ„ï¼Œä¸ä½¿ç”¨ Google Drive)
    config = setup_config(market_type=market_type)
    
    # æ ¹æ“š TOP_STOCKS_ONLY æ±ºå®š history_folder è·¯å¾‘
    if not TOP_STOCKS_ONLY:
        # ä½¿ç”¨ local_ é–‹é ­çš„è³‡æ–™å¤¾
        if market_type == 'TSE':
            config['history_folder'] = os.path.join(base_dir, 'local_StockTSEHistory')
        else:
            config['history_folder'] = os.path.join(base_dir, 'local_StockOTCHistory')
        
        # ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
        os.makedirs(config['history_folder'], exist_ok=True)
        print(f"ğŸ“ History è³‡æ–™å¤¾: {config['history_folder']}\n")

    # è®€å–è‚¡ç¥¨æ¸…å–®
    allowed_stock_codes, stock_sector_map, etf_stock_codes = load_stock_list(config['market_list_path'])

    # è®€å–åƒ¹æ ¼è³‡æ–™
    stock_daily_prices = load_stock_daily_prices(config['stock_daily_folder'], allowed_stock_codes)

    # å–å¾—æœ€æ–°æª”æ¡ˆ
    latest_61_files = get_latest_files(config['folder_path'], num_files=61)

    # è™•ç†ä¸‰å¤§æ³•äººæ•¸æ“š
    (all_data, daily_buy_sell_data_raw, etf_daily_data, buy_top20_tracker,
     sell_top20_tracker, daily_buy_stocks, daily_sell_stocks,
     daily_all_stocks, all_historical_data, statistics) = process_shares_files(
        latest_61_files, 
        allowed_stock_codes, 
        stock_daily_prices,
        stock_sector_map, 
        etf_stock_codes,
        top_buy_count=config['top_buy_count'],
        top_sell_count=config['top_sell_count']
    )
    

    # ä¿ç•™åŸå§‹ DataFrame list ç”¨æ–¼ Excel
    daily_buy_sell_data = daily_buy_sell_data_raw
    
    # è½‰æ›ç‚ºå­—å…¸æ ¼å¼ç”¨æ–¼ HTML
    daily_buy_sell_data_html = organize_daily_buy_sell_data_for_html(daily_buy_sell_data_raw)
    print(f"\nâœ“ å·²æ•´ç† {len(daily_buy_sell_data_html)} å¤©çš„è²·è³£è¶…æ•¸æ“š")

    # è¨ˆç®—æ¨™æº–å·®
    stock_statistics = calculate_stock_statistics(all_historical_data, config['sigma_threshold'])

    # åˆ†ææ–°é€²æ¦œèˆ‡å€¼å¾—è§€å¯Ÿ
    (new_buy_stocks, new_sell_stocks, observable_buy_stocks, observable_sell_stocks,
     latest_date, latest_buy_stocks_n, latest_sell_stocks_n) = analyze_new_entries_and_observables(
        latest_61_files[0], daily_buy_stocks, daily_sell_stocks,
        daily_all_stocks, stock_statistics, allowed_stock_codes,
        config['sigma_threshold'],
        top_buy_count=config['top_buy_count'],
        top_sell_count=config['top_sell_count']
    )

    # ========== æ ¹æ“š TOP_STOCKS_ONLY flag æ±ºå®šè¦æ”¶é›†æ­·å²çš„è‚¡ç¥¨ ==========
    if TOP_STOCKS_ONLY:
        # åªæ”¶é›†è²·è¶…å‰150 + è³£è¶…å‰50 çš„æ­·å²
        print(f"\n{'='*80}")
        print(f"TOP_STOCKS_ONLY = True: åªæ”¶é›†è²·è¶…å‰150 + è³£è¶…å‰50 çš„æ­·å²æ•¸æ“š")
        print(f"{'='*80}")
        collect_buy_stocks = latest_buy_stocks_n
        collect_sell_stocks = latest_sell_stocks_n
    else:
        # æ”¶é›†æ‰€æœ‰ CSV å…§è‚¡ç¥¨çš„æ­·å²
        print(f"\n{'='*80}")
        print(f"TOP_STOCKS_ONLY = False: æ”¶é›†æ‰€æœ‰ CSV å…§è‚¡ç¥¨çš„æ­·å²æ•¸æ“š")
        print(f"{'='*80}")
        
        # å¾æ‰€æœ‰æ­·å²æ•¸æ“šä¸­å–å¾—æ‰€æœ‰è‚¡ç¥¨ä»£ç¢¼
        all_stocks_in_csv = set(all_historical_data.keys())
        print(f"å¾ CSV æª”æ¡ˆä¸­æ‰¾åˆ° {len(all_stocks_in_csv)} æ”¯è‚¡ç¥¨")
        
        collect_buy_stocks = all_stocks_in_csv
        collect_sell_stocks = set()  # å·²ç¶“åŒ…å«åœ¨ collect_buy_stocks ä¸­

    # æ”¶é›†æ­·å²æ•¸æ“š
    collect_stock_history(collect_buy_stocks, collect_sell_stocks, config['folder_path'],
                      config['stock_daily_folder'], config['history_folder'],
                      allowed_stock_codes)

    # å½™æ•´åˆ†æ
    buy_stocks, sell_stocks, both_stocks_set, both_stocks_df = aggregate_analysis(
        buy_top20_tracker, sell_top20_tracker, stock_sector_map,
        aggregate_threshold=config.get('aggregate_threshold', 10000),
        show_top_n=config.get('show_top_n', None)
    )

    # è¼¸å‡º Excel
    if buy_stocks is not None and sell_stocks is not None:
        export_to_excel(config['output_path'], buy_stocks, sell_stocks, both_stocks_set,
                       both_stocks_df, daily_buy_sell_data, etf_daily_data, latest_date,
                       new_buy_stocks, new_sell_stocks, observable_buy_stocks,
                       observable_sell_stocks, stock_sector_map, etf_stock_codes)

        # ç¾åŒ– Excel
        beautify_excel(config['output_path'])

        # ç”Ÿæˆ HTML å ±å‘Š - ä½¿ç”¨è½‰æ›å¾Œçš„å­—å…¸æ ¼å¼
        html_output_path = config['output_path'].replace('.xlsx', '_complete.html')
        
        print(f"\næº–å‚™ç”Ÿæˆ HTML: {html_output_path}")
        print(f"  - buy_stocks: {len(buy_stocks) if buy_stocks is not None else 0} rows")
        print(f"  - sell_stocks: {len(sell_stocks) if sell_stocks is not None else 0} rows")
        print(f"  - daily_buy_sell_data_html: {len(daily_buy_sell_data_html)} days")
        
        generate_complete_html(
            html_output_path, buy_stocks, sell_stocks, both_stocks_set,
            both_stocks_df, daily_buy_sell_data_html, etf_daily_data, latest_date,
            new_buy_stocks, new_sell_stocks, observable_buy_stocks,
            observable_sell_stocks, stock_sector_map, etf_stock_codes,
            market_type=market_type)

        print(f"\nâœ“ {market_type} åˆ†æå®Œæˆ")
        print(f"âœ“ Excel å·²å„²å­˜: {config['output_path']}")
        print(f"âœ“ HTML å·²å„²å­˜: {html_output_path}")

    # ========== å„²å­˜è²·è¶…æ’åé †åº ==========
    # ========== å„²å­˜è²·è¶…+è³£è¶…æ’åé †åº ==========
    if latest_date and latest_buy_stocks_n:
        try:
            # è®€å–æœ€æ–°ä¸€å¤©çš„è³‡æ–™ä¾†å–å¾—å®Œæ•´æ’å
            latest_file = latest_61_files[0]
            latest_df = pd.read_csv(latest_file, encoding='utf-8')

            if 'è­‰åˆ¸ä»£è™Ÿ' in latest_df.columns:
                latest_df['è­‰åˆ¸ä»£è™Ÿ'] = latest_df['è­‰åˆ¸ä»£è™Ÿ'].apply(normalize_stock_code)

            if allowed_stock_codes is not None:
                latest_df = latest_df[latest_df['è­‰åˆ¸ä»£è™Ÿ'].isin(allowed_stock_codes)]

            latest_df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'] = pd.to_numeric(
                latest_df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'].astype(str).str.replace(',', ''),
                errors='coerce'
            )
            latest_df['è²·è³£è¶…å¼µæ•¸'] = (latest_df['ä¸‰å¤§æ³•äººè²·è³£è¶…è‚¡æ•¸'] / 1000).fillna(0).astype(int)

            # å–å¾—è²·è¶…å‰Nå’Œè³£è¶…å‰Nçš„æ’åé †åº
            top_buy_count = config.get('top_buy_count', 50)
            top_sell_count = config.get('top_sell_count', 20)
            
            buy_top = latest_df[latest_df['è²·è³£è¶…å¼µæ•¸'] > 0].nlargest(top_buy_count, 'è²·è³£è¶…å¼µæ•¸')
            sell_top = latest_df[latest_df['è²·è³£è¶…å¼µæ•¸'] < 0].nsmallest(top_sell_count, 'è²·è³£è¶…å¼µæ•¸')
            
            buy_ranking = buy_top['è­‰åˆ¸ä»£è™Ÿ'].tolist()
            sell_ranking = sell_top['è­‰åˆ¸ä»£è™Ÿ'].tolist()

            # å„²å­˜æ’ååˆ°æª”æ¡ˆï¼ˆè²·è¶…+è³£è¶…ï¼‰
            ranking_file = os.path.join(config['output_folder'], f'{market_type}_buy_ranking.txt')
            with open(ranking_file, 'w', encoding='utf-8') as f:
                f.write(f"# {market_type} - {latest_date}\n")
                
                # å¯«å…¥è²·è¶…å‰Nå
                for rank, code in enumerate(buy_ranking, 1):
                    stock_name = latest_df[latest_df['è­‰åˆ¸ä»£è™Ÿ'] == code]['è­‰åˆ¸åç¨±'].iloc[0] if len(latest_df[latest_df['è­‰åˆ¸ä»£è™Ÿ'] == code]) > 0 else ''
                    buy_amount = latest_df[latest_df['è­‰åˆ¸ä»£è™Ÿ'] == code]['è²·è³£è¶…å¼µæ•¸'].iloc[0] if len(latest_df[latest_df['è­‰åˆ¸ä»£è™Ÿ'] == code]) > 0 else 0
                    f.write(f"{rank},{code},{stock_name},{buy_amount}\n")
                
                # å¯«å…¥è³£è¶…å‰Nå
                for rank, code in enumerate(sell_ranking, top_buy_count + 1):
                    stock_name = latest_df[latest_df['è­‰åˆ¸ä»£è™Ÿ'] == code]['è­‰åˆ¸åç¨±'].iloc[0] if len(latest_df[latest_df['è­‰åˆ¸ä»£è™Ÿ'] == code]) > 0 else ''
                    sell_amount = latest_df[latest_df['è­‰åˆ¸ä»£è™Ÿ'] == code]['è²·è³£è¶…å¼µæ•¸'].iloc[0] if len(latest_df[latest_df['è­‰åˆ¸ä»£è™Ÿ'] == code]) > 0 else 0
                    f.write(f"{rank},{code},{stock_name},{sell_amount}\n")

            print(f"\nâœ“ æ’åå·²å„²å­˜: {ranking_file}")
            print(f"  è²·è¶…å‰{top_buy_count}å + è³£è¶…å‰{top_sell_count}å = å…±{top_buy_count + top_sell_count}ç­†")
            print(f"  è²·è¶…å‰10å: {', '.join(buy_ranking[:10])}")
            print(f"  è³£è¶…å‰5å: {', '.join(sell_ranking[:5])}")
        except Exception as e:
            print(f"\nâš  å„²å­˜æ’åæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")


# ============================================================================
# ç¬¬ä¸‰æ­¥ï¼šåœ–è¡¨ç”Ÿæˆçš„æ‰€æœ‰é¡åˆ¥å’Œå‡½æ•¸
# ============================================================================

# ============================================================================
# æ¨¡çµ„ 1: é…ç½®ç®¡ç† (Config)
# ============================================================================

class Config:
    """é…ç½®ç®¡ç†é¡åˆ¥"""

    # ========== å…¨åŸŸè¨­å®š ==========
    OVERWRITE_EXISTING = True  # True: è¦†è“‹å·²å­˜åœ¨çš„æª”æ¡ˆ, False: è·³éå·²å­˜åœ¨çš„æª”æ¡ˆ
    MARKET_TYPE = 'TSE'  # 'TSE': ä¸Šå¸‚, 'OTC': ä¸Šæ«ƒ, 'ALL': å…¨éƒ¨
    RUN_ALL = True  # True: æ‰¹æ¬¡è™•ç†æ‰€æœ‰è‚¡ç¥¨, False: æ‰‹å‹•è¼¸å…¥å–®ä¸€è‚¡ç¥¨
    # æ‰¹æ¬¡è™•ç†æœƒåŒæ™‚ç”Ÿæˆ: (1) å€‹åˆ¥HTMLåˆ°StockTSEHTML, (2) åˆä½µHTMLåˆ°StockInfo
    # ==============================

    FONT_PATH = None  # ä¸­æ–‡å­—é«”è·¯å¾‘

    @staticmethod
    def setup_config(market_type='TSE', base_path='.'):
        """
        è¨­å®šæ‰€æœ‰è·¯å¾‘è®Šæ•¸

        Args:
            market_type: 'TSE' (ä¸Šå¸‚) æˆ– 'OTC' (ä¸Šæ«ƒ)
            base_path: åŸºç¤è·¯å¾‘ (é è¨­ç‚ºç•¶å‰ç›®éŒ„)

        Returns:
            dict: åŒ…å«æ‰€æœ‰è·¯å¾‘é…ç½®çš„å­—å…¸
        """

        if market_type == 'TSE':
            config = {
                'market_type': market_type,
                'market_name': 'ä¸Šå¸‚',
                'history_folder': os.path.join(base_path, 'StockTSEHistory'),
                'html_output_folder': os.path.join(base_path, 'StockTSEHTML'),
                'merged_output_folder': os.path.join(base_path, 'StockInfo'),
                'stocklist_folder': os.path.join(base_path, 'StockList'),
            }
        else:  # OTC
            config = {
                'market_type': market_type,
                'market_name': 'ä¸Šæ«ƒ',
                'history_folder': os.path.join(base_path, 'StockOTCHistory'),
                'html_output_folder': os.path.join(base_path, 'StockOTCHTML'),
                'merged_output_folder': os.path.join(base_path, 'StockInfo'),
                'stocklist_folder': os.path.join(base_path, 'StockList'),
            }

        # å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾
        os.makedirs(config['html_output_folder'], exist_ok=True)
        os.makedirs(config['merged_output_folder'], exist_ok=True)

        print(f"{'='*80}")
        print(f"å¸‚å ´é¡å‹: {market_type} ({config['market_name']})")
        print(f"è¼¸å‡ºæ¨¡å¼: å€‹åˆ¥HTML + åˆä½µHTML")
        print(f"æ­·å²æ•¸æ“šè³‡æ–™å¤¾: {config['history_folder']}")
        print(f"å€‹åˆ¥HTMLè¼¸å‡º: {config['html_output_folder']}")
        print(f"åˆä½µHTMLè¼¸å‡º: {config['merged_output_folder']}")
        print(f"{'='*80}\n")

        return config


# ============================================================================
# æ¨¡çµ„ 2: å·¥å…·å‡½æ•¸ (Utils)
# ============================================================================

class Utils:
    """å·¥å…·å‡½æ•¸é¡åˆ¥"""

    @staticmethod
    def setup_chinese_font(base_path='.'):
        """è¨­å®šä¸­æ–‡å­—é«”"""
        font_path = os.path.join(base_path, 'StockList', 'Font.ttf')

        if os.path.exists(font_path):
            Config.FONT_PATH = font_path
            print(f"âœ“ æ‰¾åˆ°å­—é«”æª”æ¡ˆ: {font_path}")
        else:
            print(f"âš  æ‰¾ä¸åˆ°å­—é«”æª”æ¡ˆ: {font_path}")
            print("  HTML åœ–è¡¨å°‡ä½¿ç”¨é è¨­å­—é«”")
            Config.FONT_PATH = None

        return Config.FONT_PATH

    @staticmethod
    def read_csv_auto_encoding(file_path):
        """è‡ªå‹•åµæ¸¬ç·¨ç¢¼è®€å– CSV"""
        encodings = ['utf-8-sig', 'utf-8', 'big5', 'cp950']
        for encoding in encodings:
            try:
                return pd.read_csv(file_path, encoding=encoding)
            except:
                continue
        raise ValueError(f"ç„¡æ³•è®€å–æª”æ¡ˆ: {file_path}")

    @staticmethod
    def get_stock_name(base_path, stock_code):
        """å¾ StockList å–å¾—è‚¡ç¥¨åç¨±"""
        try:
            stocklist_path = os.path.join(base_path, 'StockList', 'StockList_simplified.csv')
            if not os.path.exists(stocklist_path):
                return ''

            df = Utils.read_csv_auto_encoding(stocklist_path)

            for code_col in df.columns:
                if 'ä»£' in code_col or 'code' in code_col.lower():
                    for name_col in df.columns:
                        if 'å' in name_col or 'name' in name_col.lower():
                            matched = df[df[code_col].astype(str) == str(stock_code)]
                            if len(matched) > 0:
                                return str(matched.iloc[0][name_col])
            return ''
        except:
            return ''

    @staticmethod
    def get_all_stock_codes_from_history(history_folder):
        """å¾ History è³‡æ–™å¤¾å–å¾—æ‰€æœ‰è‚¡ç¥¨ä»£ç¢¼"""
        try:
            if not os.path.exists(history_folder):
                print(f"âŒ æ‰¾ä¸åˆ°è³‡æ–™å¤¾: {history_folder}")
                return []

            csv_files = glob.glob(os.path.join(history_folder, "*.csv"))

            if not csv_files:
                print(f"âŒ è³‡æ–™å¤¾ä¸­æ²’æœ‰ CSV æª”æ¡ˆ: {history_folder}")
                return []

            stock_codes = []
            for csv_file in csv_files:
                filename = os.path.basename(csv_file)
                stock_code = os.path.splitext(filename)[0]
                stock_codes.append(stock_code)

            stock_codes.sort()

            print(f"âœ“ å¾ {os.path.basename(history_folder)} æ‰¾åˆ° {len(stock_codes)} æ”¯è‚¡ç¥¨")
            return stock_codes

        except Exception as e:
            print(f"âŒ è®€å–è³‡æ–™å¤¾å¤±æ•—: {str(e)}")
            return []

    @staticmethod
    def prepare_chart_data(df):
        """æº–å‚™åœ–è¡¨æ•¸æ“š"""
        df_chart = df.copy()

        # ç¢ºä¿æ—¥æœŸæ˜¯ datetime æ ¼å¼
        df_chart['æ—¥æœŸ'] = pd.to_datetime(df_chart['æ—¥æœŸ'], errors='coerce')

        # ç§»é™¤æ—¥æœŸç‚º NaT çš„è³‡æ–™
        df_chart = df_chart[df_chart['æ—¥æœŸ'].notna()]

        df_chart = df_chart.sort_values('æ—¥æœŸ')
        df_chart = df_chart.tail(60).copy()

        # ç¢ºä¿æ•¸å€¼æ¬„ä½æ˜¯æ•¸å­—é¡å‹
        numeric_cols = ['é–‹ç›¤åƒ¹', 'æœ€é«˜åƒ¹', 'æœ€ä½åƒ¹', 'æ”¶ç›¤åƒ¹', 'æˆäº¤å¼µæ•¸',
                        'å¤–é™¸è³‡è²·è³£è¶…å¼µæ•¸', 'æŠ•ä¿¡è²·è³£è¶…å¼µæ•¸', 'è‡ªç‡Ÿå•†è²·è³£è¶…å¼µæ•¸']
        for col in numeric_cols:
            if col in df_chart.columns:
                if df_chart[col].dtype == 'object':
                    df_chart[col] = df_chart[col].astype(str).str.replace(',', '').str.replace('--', '0')
                df_chart[col] = pd.to_numeric(df_chart[col], errors='coerce')

        # è¨ˆç®— MA5 å’Œ MA10ï¼ˆç§»å‹•å¹³å‡ç·šï¼‰
        if 'æ”¶ç›¤åƒ¹' in df_chart.columns:
            df_chart['MA5'] = df_chart['æ”¶ç›¤åƒ¹'].rolling(window=5, min_periods=1).mean()
            df_chart['MA10'] = df_chart['æ”¶ç›¤åƒ¹'].rolling(window=10, min_periods=1).mean()

        return df_chart


# ============================================================================
# æ¨¡çµ„ 3: Plotly åœ–è¡¨ç”Ÿæˆ (ChartPlotly)
# ============================================================================

class ChartPlotly:
    """Plotly åœ–è¡¨ç”Ÿæˆé¡åˆ¥"""

    @staticmethod
    def generate_chart(df, stock_code, stock_name, html_output_path=None):
        """
        ä½¿ç”¨ Plotly ç”Ÿæˆäº’å‹•å¼æŠ€è¡“åˆ†æåœ–è¡¨ (HTML)

        Args:
            html_output_path: å¦‚æœç‚º None, å‰‡åªè¿”å› HTML å­—ä¸²ä¸å„²å­˜æª”æ¡ˆ
        """

        df_chart = Utils.prepare_chart_data(df)

        print(f"  åœ–è¡¨æ•¸æ“šç¯„åœ: {df_chart['æ—¥æœŸ'].min().strftime('%Y-%m-%d')} ~ {df_chart['æ—¥æœŸ'].max().strftime('%Y-%m-%d')} (å…± {len(df_chart)} ç­†)")

        latest_date_str = df_chart['æ—¥æœŸ'].max().strftime('%Y-%m-%d')

        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        stats = ChartPlotly._calculate_statistics(df_chart)

        # å‰µå»ºå­åœ–ï¼ˆ4å±¤ï¼šKç·šã€æˆäº¤é‡ã€ç•¶æ—¥è²·è³£è¶…ã€ç´¯ç©è²·è³£è¶…ï¼‰
        fig = make_subplots(
            rows=4, cols=1,
            shared_xaxes=True,
            vertical_spacing=0.03,
            subplot_titles=(
                '',  # ç¬¬ä¸€å±¤æ¨™é¡Œç•™ç©º
                '',  # ç¬¬äºŒå±¤æ¨™é¡Œç•™ç©º
                '',  # ç¬¬ä¸‰å±¤æ¨™é¡Œç•™ç©º
                ''   # ç¬¬å››å±¤æ¨™é¡Œç•™ç©º
            ),
            row_heights=[0.35, 0.15, 0.25, 0.25],
            specs=[[{"secondary_y": False}],
                   [{"secondary_y": False}],
                   [{"secondary_y": False}],
                   [{"secondary_y": False}]]
        )

        # ç¬¬ä¸€å±¤: Kç·šåœ–
        ChartPlotly._add_candlestick(fig, df_chart)
        
        # ç¬¬ä¸€å±¤: ç§»å‹•å¹³å‡ç·š (MA5 å’Œ MA10)
        ChartPlotly._add_moving_averages(fig, df_chart)

        # ç¬¬äºŒå±¤: æˆäº¤é‡
        ChartPlotly._add_volume_traces(fig, df_chart)

        # ç¬¬ä¸‰å±¤: ä¸‰å¤§æ³•äººç•¶æ—¥è²·è³£è¶…
        has_institutional = ChartPlotly._add_institutional_daily(fig, df_chart)

        # ç¬¬å››å±¤: ä¸‰å¤§æ³•äººç´¯ç©è²·è³£è¶…
        if has_institutional:
            ChartPlotly._add_institutional_cumulative(fig, df_chart)

        # æ›´æ–°ä½ˆå±€
        ChartPlotly._update_layout(fig, stock_code, stock_name, latest_date_str, df_chart, stats)

        # ç”Ÿæˆ HTML å­—ä¸²
        html_string = fig.to_html(include_plotlyjs='cdn', div_id=f'chart_{stock_code}')

        # å¦‚æœæŒ‡å®šäº†è¼¸å‡ºè·¯å¾‘,å‰‡å„²å­˜å®Œæ•´çš„ HTML æª”æ¡ˆ
        if html_output_path:
            full_html = ChartPlotly._wrap_html(html_string, f"{stock_code} {stock_name}")
            with open(html_output_path, 'w', encoding='utf-8') as f:
                f.write(full_html)
            print(f"  âœ“ HTMLåœ–è¡¨å·²å„²å­˜: {html_output_path}")

        return html_string

    @staticmethod
    def _wrap_html(chart_html, title="è‚¡ç¥¨åœ–è¡¨"):
        """åŒ…è£å®Œæ•´çš„ HTML çµæ§‹"""
        viewport_meta = '<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0, user-scalable=no">'

        touch_action_css = '''
    <style>
        html {
            -webkit-text-size-adjust: 100%;
            -ms-text-size-adjust: 100%;
        }

        body {
            margin: 0;
            padding: 0;
            overflow-y: auto;
            overflow-x: hidden;
            -webkit-overflow-scrolling: touch;
            -webkit-user-select: none;
            -moz-user-select: none;
            -ms-user-select: none;
            user-select: none;
        }

        .plotly {
            touch-action: pan-y;
            -ms-touch-action: pan-y;
        }

        * {
            -webkit-tap-highlight-color: transparent;
        }

        .stock-separator {
            height: 30px;
            background: linear-gradient(to bottom, #f0f0f0, #ffffff);
            margin: 20px 0;
            border-top: 2px solid #ddd;
            border-bottom: 2px solid #ddd;
        }
    </style>'''

        disable_gestures_script = '''
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // ç¦æ­¢é›™æŒ‡ç¸®æ”¾
            document.addEventListener('touchstart', function(e) {
                if (e.touches.length > 1) {
                    e.preventDefault();
                }
            }, { passive: false });

            // ç¦æ­¢æ‰‹å‹¢ç¸®æ”¾
            document.addEventListener('gesturestart', function(e) {
                e.preventDefault();
            });

            document.addEventListener('gesturechange', function(e) {
                e.preventDefault();
            });

            document.addEventListener('gestureend', function(e) {
                e.preventDefault();
            });

            // ç¦æ­¢é›™æ“Šç¸®æ”¾
            let lastTouchEnd = 0;
            document.addEventListener('touchend', function(e) {
                const now = Date.now();
                if (now - lastTouchEnd <= 300) {
                    e.preventDefault();
                }
                lastTouchEnd = now;
            }, false);

            // ç¦æ­¢æ»¾è¼ªç¸®æ”¾(Ctrl+æ»¾è¼ª)
            document.addEventListener('wheel', function(e) {
                if (e.ctrlKey) {
                    e.preventDefault();
                }
            }, { passive: false });

            // ç¦æ­¢æ©«å‘æ»¾å‹•
            document.addEventListener('touchmove', function(e) {
                if (!e.target.closest('.plotly')) {
                    const touch = e.touches[0];
                    const deltaX = Math.abs(touch.clientX - (touch.startX || touch.clientX));
                    const deltaY = Math.abs(touch.clientY - (touch.startY || touch.clientY));

                    if (deltaX > deltaY) {
                        e.preventDefault();
                    }
                }
            }, { passive: false });

            document.addEventListener('touchstart', function(e) {
                const touch = e.touches[0];
                touch.startX = touch.clientX;
                touch.startY = touch.clientY;
            }, { passive: true });
        });
    </script>'''

        full_html = f'''<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    {viewport_meta}
    <title>{title}</title>
    {touch_action_css}
</head>
<body>
{chart_html}
{disable_gestures_script}
</body>
</html>'''

        return full_html

    @staticmethod
    def _calculate_statistics(df_chart):
        """è¨ˆç®—çµ±è¨ˆæ•¸æ“š"""
        latest = df_chart.iloc[-1]

        stats = {
            'æˆäº¤é‡': latest['æˆäº¤å¼µæ•¸'] if 'æˆäº¤å¼µæ•¸' in latest and pd.notna(latest['æˆäº¤å¼µæ•¸']) else 0,
        }

        # è¨ˆç®—æ³•äººç´¯ç©
        if 'å¤–é™¸è³‡è²·è³£è¶…å¼µæ•¸' in df_chart.columns:
            foreign_cumsum = df_chart['å¤–é™¸è³‡è²·è³£è¶…å¼µæ•¸'].fillna(0).cumsum()
            stats['å¤–è³‡ç´¯ç©'] = foreign_cumsum.iloc[-1] if len(foreign_cumsum) > 0 else 0
        else:
            stats['å¤–è³‡ç´¯ç©'] = 0

        if 'æŠ•ä¿¡è²·è³£è¶…å¼µæ•¸' in df_chart.columns:
            trust_cumsum = df_chart['æŠ•ä¿¡è²·è³£è¶…å¼µæ•¸'].fillna(0).cumsum()
            stats['æŠ•ä¿¡ç´¯ç©'] = trust_cumsum.iloc[-1] if len(trust_cumsum) > 0 else 0
        else:
            stats['æŠ•ä¿¡ç´¯ç©'] = 0

        if 'è‡ªç‡Ÿå•†è²·è³£è¶…å¼µæ•¸' in df_chart.columns:
            dealer_cumsum = df_chart['è‡ªç‡Ÿå•†è²·è³£è¶…å¼µæ•¸'].fillna(0).cumsum()
            stats['è‡ªç‡Ÿç´¯ç©'] = dealer_cumsum.iloc[-1] if len(dealer_cumsum) > 0 else 0
        else:
            stats['è‡ªç‡Ÿç´¯ç©'] = 0

        return stats

    @staticmethod
    def _add_candlestick(fig, df_chart):
        """æ–°å¢ K ç·šåœ–"""
        fig.add_trace(
            go.Candlestick(
                x=df_chart['æ—¥æœŸ'],
                open=df_chart['é–‹ç›¤åƒ¹'],
                high=df_chart['æœ€é«˜åƒ¹'],
                low=df_chart['æœ€ä½åƒ¹'],
                close=df_chart['æ”¶ç›¤åƒ¹'],
                name='Kç·š',
                increasing_line_color='#FF5252',  # ç©è‚¡ç¶²é¢¨æ ¼çš„ç´…è‰²
                increasing_fillcolor='#FF5252',
                decreasing_line_color='#00C851',  # ç©è‚¡ç¶²é¢¨æ ¼çš„ç¶ è‰²
                decreasing_fillcolor='#00C851',
                line=dict(width=0.8),  # å½±ç·šåŠ ç²—
                xhoverformat="%m-%d",
                yhoverformat=".2f"
            ),
            row=1, col=1
        )

    @staticmethod
    def _add_moving_averages(fig, df_chart):
        """æ–°å¢ç§»å‹•å¹³å‡ç·šï¼ˆåªåŠ  MA5 å’Œ MA10ï¼‰"""
        # åªåŠ  MA5 å’Œ MA10ï¼Œä¸¦æª¢æŸ¥æ¬„ä½æ˜¯å¦å­˜åœ¨
        for ma_name, ma_col, color in [('MA5', 'MA5', 'blue'),
                                         ('MA10', 'MA10', 'orange')]:
            if ma_col in df_chart.columns:
                # ç¢ºä¿è³‡æ–™ä¸å…¨æ˜¯ NaN
                if df_chart[ma_col].notna().sum() > 0:
                    fig.add_trace(
                        go.Scatter(
                            x=df_chart['æ—¥æœŸ'],
                            y=df_chart[ma_col],
                            name=ma_name,
                            line=dict(color=color, width=1.5),
                            mode='lines',
                            hovertemplate=f'{ma_name}: %{{y:.2f}}<extra></extra>'
                        ),
                        row=1, col=1
                    )

    @staticmethod
    def _add_volume_traces(fig, df_chart):
        """æ–°å¢æˆäº¤é‡åœ–è¡¨ï¼ˆç¾åŒ–é•·æ¢åœ–æ¨£å¼ï¼‰"""
        if 'æˆäº¤å¼µæ•¸' in df_chart.columns:
            volume_lots = pd.to_numeric(df_chart['æˆäº¤å¼µæ•¸'], errors='coerce')
            
            # æ ¹æ“šæ¼²è·Œæ±ºå®šé¡è‰²ï¼ˆç´…æ¼²ç¶ è·Œï¼‰
            colors = []
            for i in range(len(df_chart)):
                if i == 0:
                    # ç¬¬ä¸€å¤©ç”¨é–‹ç›¤æ”¶ç›¤æ¯”è¼ƒ
                    if df_chart['æ”¶ç›¤åƒ¹'].iloc[i] >= df_chart['é–‹ç›¤åƒ¹'].iloc[i]:
                        colors.append('rgba(255, 82, 82, 0.8)')  # ç©è‚¡ç¶²é¢¨æ ¼ç´…è‰²
                    else:
                        colors.append('rgba(0, 200, 81, 0.8)')   # ç©è‚¡ç¶²é¢¨æ ¼ç¶ è‰²
                else:
                    # å…¶ä»–å¤©èˆ‡å‰ä¸€å¤©æ”¶ç›¤åƒ¹æ¯”è¼ƒ
                    if df_chart['æ”¶ç›¤åƒ¹'].iloc[i] >= df_chart['æ”¶ç›¤åƒ¹'].iloc[i-1]:
                        colors.append('rgba(255, 82, 82, 0.8)')  # ç©è‚¡ç¶²é¢¨æ ¼ç´…è‰²
                    else:
                        colors.append('rgba(0, 200, 81, 0.8)')   # ç©è‚¡ç¶²é¢¨æ ¼ç¶ è‰²
            
            # æˆäº¤é‡é•·æ¢åœ–
            fig.add_trace(
                go.Bar(
                    x=df_chart['æ—¥æœŸ'],
                    y=volume_lots,
                    name='æˆäº¤é‡',
                    marker=dict(
                        color=colors,
                        line=dict(width=0)  # ç„¡é‚Šæ¡†æ›´ç°¡æ½”
                    ),
                    hovertemplate='æˆäº¤é‡: %{y:,.0f}å¼µ<extra></extra>',
                    showlegend=True
                ),
                row=2, col=1
            )

    @staticmethod
    def _add_institutional_daily(fig, df_chart):
        """æ–°å¢ä¸‰å¤§æ³•äººç•¶æ—¥è²·è³£è¶…"""
        has_institutional_data = False
        if 'å¤–é™¸è³‡è²·è³£è¶…å¼µæ•¸' in df_chart.columns:
            foreign = pd.to_numeric(df_chart['å¤–é™¸è³‡è²·è³£è¶…å¼µæ•¸'], errors='coerce')
            trust = pd.to_numeric(df_chart.get('æŠ•ä¿¡è²·è³£è¶…å¼µæ•¸', 0), errors='coerce')
            dealer = pd.to_numeric(df_chart.get('è‡ªç‡Ÿå•†è²·è³£è¶…å¼µæ•¸', 0), errors='coerce')

            if foreign.notna().sum() > 0 or trust.notna().sum() > 0 or dealer.notna().sum() > 0:
                has_institutional_data = True

                # çµ±ä¸€é¡è‰²é…ç½®èˆ‡åœ–ä¾‹åç¨±
                for name, data, color in [
                    ('å¤–è³‡', foreign, 'rgba(255, 82, 82, 0.75)'),    # é®®æ˜ç´…è‰²
                    ('æŠ•ä¿¡', trust, 'rgba(0, 200, 81, 0.75)'),       # é®®æ˜ç¶ è‰²
                    ('è‡ªç‡Ÿå•†', dealer, 'rgba(0, 191, 255, 0.75)')    # å¤©ç©ºè—
                ]:
                    fig.add_trace(
                        go.Bar(
                            x=df_chart['æ—¥æœŸ'],
                            y=data,
                            name=name,  # åœ–ä¾‹é¡¯ç¤º: å¤–è³‡/æŠ•ä¿¡/è‡ªç‡Ÿå•†
                            marker_color=color,
                            hovertemplate=f'{name}: %{{y:,.0f}}å¼µ<extra></extra>',
                            legendgroup=name,  # å°‡ä¸Šä¸‹åœ–è¡¨çš„åŒé¡å‹åˆ†çµ„
                            showlegend=True
                        ),
                        row=3, col=1
                    )

        return has_institutional_data

    @staticmethod
    def _add_institutional_cumulative(fig, df_chart):
        """æ–°å¢ä¸‰å¤§æ³•äººç´¯ç©è²·è³£è¶…ï¼ˆå¹³æ»‘æ›²ç·šï¼‰"""
        if 'å¤–é™¸è³‡è²·è³£è¶…å¼µæ•¸' in df_chart.columns:
            foreign_cumsum = pd.to_numeric(df_chart['å¤–é™¸è³‡è²·è³£è¶…å¼µæ•¸'], errors='coerce').fillna(0).cumsum()
            trust_cumsum = pd.to_numeric(df_chart.get('æŠ•ä¿¡è²·è³£è¶…å¼µæ•¸', 0), errors='coerce').fillna(0).cumsum()
            dealer_cumsum = pd.to_numeric(df_chart.get('è‡ªç‡Ÿå•†è²·è³£è¶…å¼µæ•¸', 0), errors='coerce').fillna(0).cumsum()

            # çµ±ä¸€é¡è‰²é…ç½®èˆ‡åœ–ä¾‹åç¨±ï¼ˆä½¿ç”¨splineå¹³æ»‘æ›²ç·šï¼‰
            for name, data, color in [
                ('å¤–è³‡', foreign_cumsum, 'rgb(255, 82, 82)'),    # é®®æ˜ç´…è‰²
                ('æŠ•ä¿¡', trust_cumsum, 'rgb(0, 200, 81)'),       # é®®æ˜ç¶ è‰²
                ('è‡ªç‡Ÿå•†', dealer_cumsum, 'rgb(0, 191, 255)')    # å¤©ç©ºè—
            ]:
                fig.add_trace(
                    go.Scatter(
                        x=df_chart['æ—¥æœŸ'],
                        y=data,
                        name=f'{name}ç´¯ç©',  # åœ–ä¾‹é¡¯ç¤º: å¤–è³‡ç´¯ç©/æŠ•ä¿¡ç´¯ç©/è‡ªç‡Ÿå•†ç´¯ç©
                        line=dict(color=color, width=2.5, shape='spline', smoothing=0.8),
                        mode='lines',
                        hovertemplate=f'{name}ç´¯ç©: %{{y:,.0f}}å¼µ<extra></extra>',
                        legendgroup=name,  # èˆ‡ä¸Šå±¤çš„å¤–è³‡/æŠ•ä¿¡/è‡ªç‡Ÿå•†åŒçµ„
                        showlegend=True
                    ),
                    row=4, col=1
                )

    @staticmethod
    def _update_layout(fig, stock_code, stock_name, latest_date_str, df_chart, stats):
        """æ›´æ–°åœ–è¡¨ä½ˆå±€"""
        # å»ºç«‹çµ±è¨ˆè³‡è¨Šæ–‡å­— (ç°¡åŒ–ç‰ˆï¼Œç§»é™¤MA)
        stats_line1 = (
            f"æœ€æ–°è³‡æ–™æ—¥æœŸ: {latest_date_str} | "
            f"å¤–è³‡ç´¯ç©: {stats['å¤–è³‡ç´¯ç©']:,.0f}å¼µ | "
            f"æŠ•ä¿¡ç´¯ç©: {stats['æŠ•ä¿¡ç´¯ç©']:,.0f}å¼µ | "
            f"è‡ªç‡Ÿç´¯ç©: {stats['è‡ªç‡Ÿç´¯ç©']:,.0f}å¼µ"
        )
        stats_line2 = (
            f"è‚¡åƒ¹Kç·šåœ– | "
            f"æˆäº¤é‡: {stats['æˆäº¤é‡']:,.0f}å¼µ"
        )

        fig.update_layout(
            title=dict(
                text=f'{stock_code} {stock_name} æŠ€è¡“åˆ†æåœ–è¡¨ (æœ€è¿‘60ç­†)<br><sub>{stats_line1}</sub><br><sub>{stats_line2}</sub>',
                x=0.5,
                xanchor='center',
                font=dict(size=16, family='Microsoft JhengHei, Arial, sans-serif')
            ),
            xaxis_rangeslider_visible=False,
            height=1500,  # 4å±¤åœ–è¡¨é«˜åº¦
            showlegend=True,
            hovermode='x unified',
            template='plotly_white',
            barmode='relative',
            legend=dict(
                orientation="v",
                yanchor="top",
                y=0.98,
                xanchor="left",
                x=0.01,
                bgcolor="rgba(255, 255, 255, 0.8)",
                bordercolor="lightgray",
                borderwidth=1,
                font=dict(family='Microsoft JhengHei, Arial, sans-serif')
            ),
            font=dict(family='Microsoft JhengHei, Arial, sans-serif'),  # å…¨åŸŸå­—é«”è¨­å®š
            dragmode='pan'  # å…è¨±æ‹–æ›³,ä½†ç”± fixedrange é™åˆ¶è»¸ç¯„åœ
        )

        # ç§»é™¤æ‰€æœ‰å­åœ–æ¨™é¡Œï¼ˆå·²åœ¨ make_subplots ä¸­è¨­ç‚ºç©ºå­—ä¸²ï¼‰
        # ä¸éœ€è¦é¡å¤–çš„ annotations è¨­å®š

        # è¨ˆç®—è‚¡åƒ¹ç¯„åœï¼ˆåªä½¿ç”¨OHLCï¼‰
        price_cols = ['é–‹ç›¤åƒ¹', 'æœ€é«˜åƒ¹', 'æœ€ä½åƒ¹', 'æ”¶ç›¤åƒ¹']
        price_min = df_chart[price_cols].min().min()
        price_max = df_chart[price_cols].max().max()
        price_margin = (price_max - price_min) * 0.05
        price_range = [price_min - price_margin, price_max + price_margin]

        # æ›´æ–°Yè»¸ - ç¦ç”¨ç¸®æ”¾
        fig.update_yaxes(title_text="è‚¡åƒ¹ (å…ƒ)", row=1, col=1, range=price_range, fixedrange=True)
        fig.update_yaxes(title_text="æˆäº¤é‡ (å¼µ)", row=2, col=1, tickformat=",", fixedrange=True)
        fig.update_yaxes(title_text="ç•¶æ—¥è²·è³£è¶… (å¼µ)", row=3, col=1, tickformat=",", fixedrange=True)
        fig.update_yaxes(title_text="ç´¯ç©è²·è³£è¶… (å¼µ)", row=4, col=1, tickformat=",", fixedrange=True)

        # æ›´æ–°Xè»¸ - ç¦ç”¨ç¸®æ”¾
        # æ›´æ–°Xè»¸ - ç§»é™¤éäº¤æ˜“æ—¥ç©ºéš™ï¼Œè®“ K ç·šé¡¯ç¤ºå®Œæ•´
        start_date = df_chart['æ—¥æœŸ'].min()
        end_date = df_chart['æ—¥æœŸ'].max()
        
        # ç²å–å¯¦éš›äº¤æ˜“æ—¥æœŸåˆ—è¡¨
        trading_dates = df_chart['æ—¥æœŸ'].tolist()

        tickvals = []
        current = start_date.replace(day=1)
        while current <= end_date:
            for day in [1, 6, 11, 16, 21, 26]:
                try:
                    tick_date = current.replace(day=day)
                    if start_date <= tick_date <= end_date:
                        tickvals.append(tick_date)
                except:
                    pass
            if current.month == 12:
                current = current.replace(year=current.year + 1, month=1)
            else:
                current = current.replace(month=current.month + 1)

        for i in range(1, 5):
            fig.update_xaxes(
                tickformat="%m-%d",
                tickangle=-45,
                tickmode='array',
                tickvals=tickvals,
                showticklabels=True,
                autorange=True,  # è‡ªå‹•èª¿æ•´ç¯„åœä»¥é¡¯ç¤ºå®Œæ•´è³‡æ–™
                hoverformat="%m-%d",
                fixedrange=True,  # ç¦ç”¨ X è»¸ç¸®æ”¾
                rangebreaks=[
                    dict(values=pd.date_range(start=start_date, end=end_date, freq='D')
                         .difference(pd.DatetimeIndex(trading_dates)).tolist())  # ç§»é™¤æ‰€æœ‰éäº¤æ˜“æ—¥
                ],
                row=i, col=1
            )


# ============================================================================
# æ¨¡çµ„ 4: è‚¡ç¥¨è™•ç†å™¨ (Processor)
# ============================================================================

class Processor:
    """è‚¡ç¥¨è™•ç†é¡åˆ¥"""

    @staticmethod
    def process_stock(stock_code, base_path, config, save_individual=True):
        """
        è™•ç†å–®ä¸€è‚¡ç¥¨

        Args:
            save_individual: True å‰‡å„²å­˜å€‹åˆ¥æª”æ¡ˆ, False å‰‡åªè¿”å› HTML å­—ä¸²
        
        Returns:
            HTML å­—ä¸² (ç”¨æ–¼åˆä½µ), æˆ– True/False (å„²å­˜ç‹€æ…‹)
        """

        print(f"\n{'='*70}")
        print(f"è™•ç†è‚¡ç¥¨: {stock_code}")
        print('='*70)

        csv_file = os.path.join(config['history_folder'], f"{stock_code}.csv")

        if not os.path.exists(csv_file):
            print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ: {csv_file}")
            return None

        print(f"â³ è®€å– {os.path.basename(config['history_folder'])}/{stock_code}.csv...")

        try:
            result = Utils.read_csv_auto_encoding(csv_file)
            print(f"âœ“ æˆåŠŸè®€å– {len(result)} ç­†è³‡æ–™")
        except Exception as e:
            print(f"âŒ è®€å–å¤±æ•—: {str(e)}")
            return None

        stock_name = result['è‚¡ç¥¨åç¨±'].iloc[0] if 'è‚¡ç¥¨åç¨±' in result.columns and len(result) > 0 else ''
        if not stock_name:
            stock_name = Utils.get_stock_name(base_path, stock_code)

        print(f"âœ… è³‡æ–™è¼‰å…¥å®Œæˆ")
        print(f"  è‚¡ç¥¨: {stock_code} {stock_name}")
        print(f"  ç­†æ•¸: {len(result)}")
        if 'æ—¥æœŸ' in result.columns:
            print(f"  æ—¥æœŸç¯„åœ: {result['æ—¥æœŸ'].min()} ~ {result['æ—¥æœŸ'].max()}")

        print(f"â³ ç”ŸæˆæŠ€è¡“åˆ†æåœ–è¡¨...")

        try:
            # ç”Ÿæˆ HTML å­—ä¸² (ç”¨æ–¼åˆä½µ)
            html_string = ChartPlotly.generate_chart(
                result,
                stock_code,
                stock_name,
                html_output_path=None
            )
            
            # å¦‚æœéœ€è¦,åŒæ™‚å„²å­˜å€‹åˆ¥æª”æ¡ˆ
            if save_individual:
                html_output_file = os.path.join(config['html_output_folder'], f"{stock_code}.html")
                
                if not Config.OVERWRITE_EXISTING and os.path.exists(html_output_file):
                    print(f"â­ï¸  å€‹åˆ¥æª”æ¡ˆå·²å­˜åœ¨ï¼Œè·³é: {stock_code}")
                else:
                    ChartPlotly.generate_chart(
                        result,
                        stock_code,
                        stock_name,
                        html_output_path=html_output_file
                    )
                    print(f"âœ… å€‹åˆ¥åœ–è¡¨: {os.path.basename(config['html_output_folder'])}/{stock_code}.html")
            
            print(f"âœ… åœ–è¡¨å·²ç”Ÿæˆ")
            return html_string

        except Exception as e:
            print(f"âŒ åœ–è¡¨ç”Ÿæˆå¤±æ•—: {str(e)}")
            import traceback
            traceback.print_exc()
            return None

    @staticmethod
    def batch_process_all_stocks(base_dir, config):
        """æ‰¹æ¬¡è™•ç†æ‰€æœ‰è‚¡ç¥¨ - æŒ‰ç…§è²·è¶…æ’åé †åºç”Ÿæˆ"""

        print("\n" + "="*70)
        print(f"æ‰¹æ¬¡è™•ç†æ¨¡å¼ - {config['market_name']}")
        print(f"è¼¸å‡ºæ–¹å¼: å€‹åˆ¥HTML + åˆä½µHTML (æŒ‰è²·è¶…æ’åæ’åº)")
        print(f"è¦†è“‹æ¨¡å¼: {'è¦†è“‹å·²å­˜åœ¨æª”æ¡ˆ' if Config.OVERWRITE_EXISTING else 'è·³éå·²å­˜åœ¨æª”æ¡ˆ'}")
        print("="*70)

        # è®€å–è²·è¶…æ’åé †åº
        ranking_file = os.path.join(config['merged_output_folder'], f"{config['market_type']}_buy_ranking.txt")
        ranked_stocks = []
        
        if os.path.exists(ranking_file):
            print(f"\nâœ“ æ‰¾åˆ°è²·è¶…æ’åæª”æ¡ˆ: {os.path.basename(ranking_file)}")
            with open(ranking_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                for line in lines[1:]:  # è·³éæ¨™é¡Œè¡Œ
                    parts = line.strip().split(',')
                    if len(parts) >= 2:
                        ranked_stocks.append(parts[1])  # è­‰åˆ¸ä»£è™Ÿ
            print(f"  å·²è¼‰å…¥ {len(ranked_stocks)} æ”¯æ’åè‚¡ç¥¨")
            if len(ranked_stocks) > 0:
                print(f"  å‰10å: {', '.join(ranked_stocks[:10])}")
        else:
            print(f"\nâš  æ‰¾ä¸åˆ°è²·è¶…æ’åæª”æ¡ˆ: {ranking_file}")
            print("  å°‡ä½¿ç”¨é è¨­é †åºè™•ç†")

        # å–å¾—æ‰€æœ‰è‚¡ç¥¨ä»£ç¢¼
        print("\nâ³ æƒææ­·å²è³‡æ–™å¤¾...")
        all_stocks = Utils.get_all_stock_codes_from_history(config['history_folder'])

        if not all_stocks:
            print("âŒ ç„¡æ³•å–å¾—è‚¡ç¥¨æ¸…å–®")
            return

        # å°‡è‚¡ç¥¨åˆ†ç‚ºå…©çµ„ï¼šæœ‰æ’åçš„ + å…¶ä»–çš„
        if ranked_stocks:
            # ç¢ºä¿æ’åä¸­çš„è‚¡ç¥¨éƒ½å­˜åœ¨æ–¼æ­·å²è³‡æ–™ä¸­
            ranked_stocks = [s for s in ranked_stocks if s in all_stocks]
            # å…¶ä»–æœªæ’åçš„è‚¡ç¥¨ï¼ˆæŒ‰ä»£ç¢¼æ’åºï¼‰
            other_stocks = sorted([s for s in all_stocks if s not in ranked_stocks])
            # åˆä½µï¼šå…ˆæ’åçš„ï¼Œå¾Œå…¶ä»–çš„
            stock_codes = ranked_stocks + other_stocks
            print(f"\nâœ“ è‚¡ç¥¨è™•ç†é †åº:")
            print(f"  - è²·è¶…æ’åè‚¡ç¥¨: {len(ranked_stocks)} æ”¯ (å„ªå…ˆè™•ç†)")
            print(f"  - å…¶ä»–è‚¡ç¥¨: {len(other_stocks)} æ”¯")
        else:
            stock_codes = sorted(all_stocks)
            print(f"\nâœ“ æ‰¾åˆ° {len(stock_codes)} æ”¯è‚¡ç¥¨ (æŒ‰ä»£ç¢¼æ’åº)")

        start_time = datetime.now()

        # åŒæ™‚ç”Ÿæˆå€‹åˆ¥HTMLå’Œæ”¶é›†åˆä½µHTML
        merged_html_parts = []
        success_count = 0
        fail_count = 0

        for idx, stock_code in enumerate(stock_codes, 1):
            print(f"\n{'='*70}")
            
            # é¡¯ç¤ºæ˜¯å¦ç‚ºæ’åè‚¡ç¥¨
            if ranked_stocks and stock_code in ranked_stocks:
                rank = ranked_stocks.index(stock_code) + 1
                print(f"é€²åº¦: [{idx}/{len(stock_codes)}] ğŸ“Š è²·è¶…æ’å #{rank}")
            else:
                print(f"é€²åº¦: [{idx}/{len(stock_codes)}] ({idx/len(stock_codes)*100:.1f}%)")
            
            print(f"{'='*70}")

            html_string = Processor.process_stock(
                stock_code,
                base_dir,  # ä¿®æ”¹ï¼šæ”¹ç”¨ base_dir
                config,
                save_individual=True  # åŒæ™‚å„²å­˜å€‹åˆ¥æª”æ¡ˆ
            )

            if html_string:
                merged_html_parts.append(html_string)
                # åœ¨æ¯å€‹åœ–è¡¨ä¹‹é–“åŠ å…¥åˆ†éš”ç·š
                if idx < len(stock_codes):
                    merged_html_parts.append('<div class="stock-separator"></div>')
                success_count += 1
            else:
                fail_count += 1

        # ç”Ÿæˆåˆä½µçš„ HTML
        if merged_html_parts:
            print(f"\n{'='*70}")
            print("â³ ç”Ÿæˆåˆä½µHTML...")
            print(f"{'='*70}")

            all_charts_html = '\n'.join(merged_html_parts)

            # åŒ…è£æˆå®Œæ•´çš„ HTML
            full_html = ChartPlotly._wrap_html(
                all_charts_html,
                f"{config['market_name']}è‚¡ç¥¨æŠ€è¡“åˆ†æåœ–è¡¨åˆé›†"
            )

            # å„²å­˜åˆä½µå¾Œçš„ HTML åˆ° StockInfo è³‡æ–™å¤¾
            merged_filename = f"ALL_{config['market_type']}.html"
            merged_output_path = os.path.join(config['merged_output_folder'], merged_filename)

            with open(merged_output_path, 'w', encoding='utf-8') as f:
                f.write(full_html)

            print(f"\nâœ… åˆä½µHTMLå·²å„²å­˜!")
            print(f"  æª”æ¡ˆ: {merged_filename}")
            print(f"  è·¯å¾‘: {merged_output_path}")
            print(f"  æª”æ¡ˆå¤§å°: {os.path.getsize(merged_output_path) / 1024 / 1024:.2f} MB")

        end_time = datetime.now()
        elapsed_time = (end_time - start_time).total_seconds()

        print("\n" + "="*70)
        print("æ‰¹æ¬¡è™•ç†å®Œæˆ")
        print("="*70)
        print(f"ç¸½è‚¡ç¥¨æ•¸: {len(stock_codes)}")
        print(f"æˆåŠŸè™•ç†: {success_count}")
        print(f"è™•ç†å¤±æ•—: {fail_count}")
        print(f"è™•ç†æ™‚é–“: {elapsed_time:.1f} ç§’ ({elapsed_time/60:.1f} åˆ†é˜)")
        print("="*70)
        print(f"å€‹åˆ¥HTMLä½ç½®: {config['html_output_folder']}")
        print(f"åˆä½µHTMLä½ç½®: {config['merged_output_folder']}")
        print("="*70)

def run_step3_chart_generation(base_dir, market_type):
    """åŸ·è¡Œç¬¬ä¸‰æ­¥ï¼šåœ–è¡¨ç”Ÿæˆ"""
    print(f"\n{'ğŸ”¥'*40}")
    print(f"ç¬¬ä¸‰æ­¥åœ–è¡¨ç”Ÿæˆï¼š{market_type} ({'ä¸Šå¸‚' if market_type == 'TSE' else 'ä¸Šæ«ƒ'})")
    print(f"{'ğŸ”¥'*40}\n")
    
    # è¨­å®šé…ç½®
    config = Config.setup_config(base_path=base_dir, market_type=market_type)
    
    # æ ¹æ“š TOP_STOCKS_ONLY æ±ºå®šè³‡æ–™å¤¾è·¯å¾‘
    if not TOP_STOCKS_ONLY:
        # ä½¿ç”¨ local_ é–‹é ­çš„è³‡æ–™å¤¾
        if market_type == 'TSE':
            config['history_folder'] = os.path.join(base_dir, 'local_StockTSEHistory')
            config['html_output_folder'] = os.path.join(base_dir, 'local_StockTSEHTML')
        else:
            config['history_folder'] = os.path.join(base_dir, 'local_StockOTCHistory')
            config['html_output_folder'] = os.path.join(base_dir, 'local_StockOTCHTML')
        
        # ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
        os.makedirs(config['history_folder'], exist_ok=True)
        os.makedirs(config['html_output_folder'], exist_ok=True)
        print(f"ğŸ“ History è³‡æ–™å¤¾: {config['history_folder']}")
        print(f"ğŸ“ HTML è³‡æ–™å¤¾: {config['html_output_folder']}\n")
    # è¨­å®šå­—é«” (GitHub Actions ç’°å¢ƒ)
    Utils.setup_chinese_font(base_dir)
    
    # æ‰¹æ¬¡è™•ç†æ‰€æœ‰è‚¡ç¥¨
    Processor.batch_process_all_stocks(base_dir, config)
    
    print(f"\nâœ“ {market_type} åœ–è¡¨ç”Ÿæˆå®Œæˆ")

# ============================================================================
# ä¸»ç¨‹å¼æµç¨‹
# ============================================================================

def copy_data_to_repo(base_dir, repo_data_dir='data'):
    """
    å°‡ä¸‹è¼‰å’Œè™•ç†çš„è³‡æ–™è¤‡è£½åˆ° repo çš„ data è³‡æ–™å¤¾
    
    Args:
        base_dir: å·¥ä½œç›®éŒ„
        repo_data_dir: repo ä¸­çš„ data è³‡æ–™å¤¾è·¯å¾‘
    """
    print("\n" + "ğŸ“¦"*40)
    print("è¤‡è£½è³‡æ–™åˆ° Repository")
    print("ğŸ“¦"*40 + "\n")
    
    # ç¢ºä¿ repo data ç›®éŒ„å­˜åœ¨
    os.makedirs(repo_data_dir, exist_ok=True)
    
    # å®šç¾©éœ€è¦è¤‡è£½çš„è³‡æ–™å¤¾
    folders_to_copy = [
        'StockTSEDaily',      # ä¸Šå¸‚æ¯æ—¥äº¤æ˜“
        'StockTSEShares',     # ä¸Šå¸‚ä¸‰å¤§æ³•äºº
        'StockOTCDaily',   # ä¸Šæ«ƒæ¯æ—¥äº¤æ˜“
        'StockOTCShares',  # ä¸Šæ«ƒä¸‰å¤§æ³•äºº
        'StockTSEHistory',    # ä¸Šå¸‚æ­·å²è³‡æ–™
        'StockOTCHistory', # ä¸Šæ«ƒæ­·å²è³‡æ–™
        'StockInfo',       # åˆ†æå ±å‘Š
        'StockTSEHTML',       # ä¸Šå¸‚åœ–è¡¨ HTML
        'StockOTCHTML'    # ä¸Šæ«ƒåœ–è¡¨ HTML
    ]
    
    copied_count = 0
    skipped_count = 0
    
    for folder_name in folders_to_copy:
        source_path = os.path.join(base_dir, folder_name)
        dest_path = os.path.join(repo_data_dir, folder_name)
        
        if os.path.exists(source_path):
            try:
                # å¦‚æœç›®æ¨™è³‡æ–™å¤¾å­˜åœ¨,å…ˆåˆªé™¤
                if os.path.exists(dest_path):
                    shutil.rmtree(dest_path)
                
                # è¤‡è£½æ•´å€‹è³‡æ–™å¤¾
                shutil.copytree(source_path, dest_path)
                
                # è¨ˆç®—æª”æ¡ˆæ•¸é‡
                file_count = len([f for f in os.listdir(dest_path) if os.path.isfile(os.path.join(dest_path, f))])
                print(f"âœ“ {folder_name:<20} â†’ {file_count} å€‹æª”æ¡ˆ")
                copied_count += 1
            except Exception as e:
                print(f"âœ— {folder_name:<20} è¤‡è£½å¤±æ•—: {e}")
        else:
            print(f"âŠ˜ {folder_name:<20} ä¾†æºä¸å­˜åœ¨")
            skipped_count += 1
    
    print("\n" + "="*80)
    print(f"è¤‡è£½å®Œæˆ: {copied_count} å€‹è³‡æ–™å¤¾, è·³é: {skipped_count} å€‹")
    print("="*80 + "\n")

def main():
    """ä¸»ç¨‹å¼ - å®Œæ•´è‡ªå‹•åŒ–æµç¨‹"""
    
    # è§£æå‘½ä»¤åˆ—åƒæ•¸
    parser = argparse.ArgumentParser(description='å°ç£è‚¡å¸‚è³‡æ–™å®Œæ•´è™•ç†æµç¨‹')
    parser.add_argument('--base-dir', type=str, default=None,
                       help='æŒ‡å®šå·¥ä½œç›®éŒ„ (é è¨­: ç•¶å‰ç›®éŒ„)')
    parser.add_argument('--repo-data-dir', type=str, default='data',
                       help='Repository çš„ data è³‡æ–™å¤¾è·¯å¾‘ (é è¨­: data)')
    parser.add_argument('--copy-to-repo', action='store_true',
                       help='å®Œæˆå¾Œå°‡è³‡æ–™è¤‡è£½åˆ° repo çš„ data è³‡æ–™å¤¾')
    parser.add_argument('--start-date', type=str, default='2025-01-01',
                       help='çˆ¬èŸ²èµ·å§‹æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)')
    parser.add_argument('--skip-crawler', action='store_true',
                       help='è·³éçˆ¬èŸ²æ­¥é©Ÿ')
    parser.add_argument('--skip-analysis', action='store_true',
                       help='è·³éåˆ†ææ­¥é©Ÿ')
    parser.add_argument('--skip-charts', action='store_true',
                       help='è·³éåœ–è¡¨ç”Ÿæˆæ­¥é©Ÿ')
    parser.add_argument('--market', type=str, choices=['TSE', 'OTC', 'BOTH'], 
                       default='BOTH', help='è™•ç†å¸‚å ´é¡å‹')
    parser.add_argument('--debug-skip-data-processing', action='store_true',
                       help='é™¤éŒ¯æ¨¡å¼ï¼šè·³éçˆ¬èŸ²å’Œ History ç”Ÿæˆ,ç›´æ¥æ¸¬è©¦å ±è¡¨å’Œä¸Šå‚³')
    
    args = parser.parse_args()
    
    print("\n" + "="*80)
    print("å°ç£è‚¡å¸‚è³‡æ–™å®Œæ•´è™•ç†æµç¨‹ - GitHub Actions ç‰ˆæœ¬")
    print("="*80)
    print("æµç¨‹èªªæ˜ï¼š")
    if not args.skip_crawler:
        print("  1. åŸ·è¡Œçˆ¬èŸ²ç¨‹å¼ (ä¸Šå¸‚/ä¸Šæ«ƒæ¯æ—¥äº¤æ˜“èˆ‡ä¸‰å¤§æ³•äºº)")
    if not args.skip_analysis:
        print("  2. æ¸…ç†èˆŠçš„ History è³‡æ–™å¤¾")
        print("  3. åŸ·è¡Œåˆ†æç¨‹å¼ - TSE (ä¸Šå¸‚)" if args.market in ['TSE', 'BOTH'] else "")
        print("  4. åŸ·è¡Œåˆ†æç¨‹å¼ - OTC (ä¸Šæ«ƒ)" if args.market in ['OTC', 'BOTH'] else "")
    if not args.skip_charts:
        print("  5. æ¸…ç†èˆŠçš„åœ–è¡¨è³‡æ–™å¤¾")
        print("  6. åŸ·è¡Œåœ–è¡¨ç”Ÿæˆ - TSE (ä¸Šå¸‚)" if args.market in ['TSE', 'BOTH'] else "")
        print("  7. åŸ·è¡Œåœ–è¡¨ç”Ÿæˆ - OTC (ä¸Šæ«ƒ)" if args.market in ['OTC', 'BOTH'] else "")
    print("="*80 + "\n")
    
    # è¨­å®šåŸºç¤ç›®éŒ„
    if args.base_dir:
        os.environ['STOCK_DATA_DIR'] = args.base_dir
    base_dir = setup_base_directory()
    
    # å»ºç«‹å¿…è¦çš„è³‡æ–™å¤¾çµæ§‹
    create_required_directories(base_dir)
    
    # ========== æ­¥é©Ÿ 1ï¼šçˆ¬èŸ² ==========
    if args.debug_skip_data_processing:
        print("\n" + "âš¡"*40)
        print("é™¤éŒ¯æ¨¡å¼ï¼šè·³éçˆ¬èŸ²å’Œ History ç”Ÿæˆæ­¥é©Ÿ")
        print("âš¡"*40 + "\n")
    else:
        if not args.skip_crawler:
            start_date = datetime.strptime(args.start_date, '%Y-%m-%d')
            run_step1_crawler(base_dir, start_date=start_date)
        
        # ========== æ­¥é©Ÿ 2-4ï¼šåˆ†æ ==========
        if not args.skip_analysis:
            # åˆªé™¤ History è³‡æ–™å¤¾
            print("\n" + "ğŸ”¥"*40)
            print("æ­¥é©Ÿ 2ï¼šæ¸…ç† History è³‡æ–™å¤¾")
            print("ğŸ”¥"*40)
            # æ ¹æ“š TOP_STOCKS_ONLY æ±ºå®šè¦æ¸…ç†çš„è³‡æ–™å¤¾
            if TOP_STOCKS_ONLY:
                delete_folders(base_dir, ['StockTSEHistory', 'StockOTCHistory'])
            else:
                delete_folders(base_dir, ['local_StockTSEHistory', 'local_StockOTCHistory'])
            
            # åŸ·è¡Œåˆ†æ
            if args.market in ['TSE', 'BOTH']:
                run_step2_analysis(base_dir, 'TSE')
            
            if args.market in ['OTC', 'BOTH']:
                run_step2_analysis(base_dir, 'OTC')
    
    # ========== æ­¥é©Ÿ 5-7ï¼šåœ–è¡¨ç”Ÿæˆ ==========
    if not args.skip_charts:
        # åˆªé™¤åœ–è¡¨è³‡æ–™å¤¾
        print("\n" + "ğŸ”¥"*40)
        print("æ­¥é©Ÿ 5ï¼šæ¸…ç†åœ–è¡¨è³‡æ–™å¤¾")
        print("ğŸ”¥"*40)
        # æ ¹æ“š TOP_STOCKS_ONLY æ±ºå®šè¦æ¸…ç†çš„è³‡æ–™å¤¾
        if TOP_STOCKS_ONLY:
            delete_folders(base_dir, ['StockTSEHTML', 'StockOTCHTML'])
        else:
            delete_folders(base_dir, ['local_StockTSEHTML', 'local_StockOTCHTML'])
        
        # åŸ·è¡Œåœ–è¡¨ç”Ÿæˆ
        if args.market in ['TSE', 'BOTH']:
            run_step3_chart_generation(base_dir, 'TSE')
        
        if args.market in ['OTC', 'BOTH']:
            run_step3_chart_generation(base_dir, 'OTC')
    
    # ========== æ­¥é©Ÿ 7.5ï¼šè¤‡è£½å¸¶æ—¥æœŸçš„æª”æ¡ˆ ==========
    print("\n" + "ğŸ“…"*40)
    print("æ­¥é©Ÿ 7.5ï¼šå‚™ä»½å¸¶æ—¥æœŸçš„åˆ†ææª”æ¡ˆ")
    print("ğŸ“…"*40 + "\n")
    
    
    stock_info_dir = os.path.join(base_dir, 'StockInfo')
    
    # å…ˆå¾ Excel æª”æ¡ˆæå–æ—¥æœŸ
    def extract_date_from_excel(excel_file_path):
        """å¾ Excel æª”æ¡ˆçš„ç¬¬äºŒå€‹åˆ†é åç¨±æå–æ—¥æœŸ"""
        try:
            wb = load_workbook(excel_file_path)
            sheet_names = wb.sheetnames
            if len(sheet_names) >= 2:
                second_sheet_name = sheet_names[1]
                match = re.search(r'(\d{8})', second_sheet_name)
                wb.close()
                if match:
                    return match.group(1)
            wb.close()
        except Exception as e:
            print(f"  âš ï¸  æå–æ—¥æœŸå¤±æ•—: {e}")
        return None
    
    # å…ˆå‚™ä»½ Excel æª”æ¡ˆä¸¦æå–æ—¥æœŸ
    tse_date_str = None
    otc_date_str = None
    
    excel_files_to_backup = [
        ('tse_analysis_result.xlsx', 'TSE'),
        ('otc_analysis_result.xlsx', 'OTC'),
    ]
    
    backup_count = 0
    
    for source_name, market_type in excel_files_to_backup:
        source_path = os.path.join(stock_info_dir, source_name)
        
        if os.path.exists(source_path):
            # æå–æ—¥æœŸ
            date_str = extract_date_from_excel(source_path)
            
            if date_str:
                # å„²å­˜æ—¥æœŸä¾›å¾ŒçºŒ HTML ä½¿ç”¨
                if market_type == 'TSE':
                    tse_date_str = date_str
                else:
                    otc_date_str = date_str
                
                # å‚™ä»½ Excel
                backup_name = f'{source_name.replace(".xlsx", "")}_{date_str}.xlsx'
                backup_path = os.path.join(stock_info_dir, backup_name)
                
                try:
                    shutil.copy2(source_path, backup_path)
                    file_size = os.path.getsize(backup_path) / 1024  # KB
                    print(f"âœ… å·²å‚™ä»½: {source_name} â†’ {backup_name} ({file_size:.1f} KB, æ—¥æœŸ: {date_str})")
                    backup_count += 1
                except Exception as e:
                    print(f"âŒ å‚™ä»½å¤±æ•—: {source_name} - {e}")
            else:
                print(f"âš ï¸  ç„¡æ³•å¾ {source_name} æå–æ—¥æœŸ,ä½¿ç”¨ç•¶å‰æ—¥æœŸ")
                # å¦‚æœç„¡æ³•æå–æ—¥æœŸ,ä½¿ç”¨å°ç£æ™‚é–“
                from datetime import timezone, timedelta as td
                taiwan_tz = timezone(td(hours=8))
                taiwan_time = datetime.now(taiwan_tz)
                date_str = taiwan_time.strftime('%Y%m%d')
                
                if market_type == 'TSE':
                    tse_date_str = date_str
                else:
                    otc_date_str = date_str
        else:
            print(f"âš ï¸  Excel æª”æ¡ˆä¸å­˜åœ¨: {source_name}")
    
    # ä½¿ç”¨æå–çš„æ—¥æœŸå‚™ä»½ HTML
    html_files_to_backup = [
        ('ALL_TSE.html', tse_date_str),
        ('ALL_OTC.html', otc_date_str),
    ]
    
    for source_name, date_str in html_files_to_backup:
        if date_str:
            source_path = os.path.join(stock_info_dir, source_name)
            backup_name = f'{source_name.replace(".html", "")}_{date_str}.html'
            backup_path = os.path.join(stock_info_dir, backup_name)
            
            if os.path.exists(source_path):
                try:
                    shutil.copy2(source_path, backup_path)
                    file_size = os.path.getsize(backup_path) / 1024  # KB
                    print(f"âœ… å·²å‚™ä»½: {source_name} â†’ {backup_name} ({file_size:.1f} KB, æ—¥æœŸ: {date_str})")
                    backup_count += 1
                except Exception as e:
                    print(f"âŒ å‚™ä»½å¤±æ•—: {source_name} - {e}")
            else:
                print(f"âš ï¸  HTML æª”æ¡ˆä¸å­˜åœ¨: {source_name}")
        else:
            print(f"âš ï¸  ç„¡æ³•å–å¾— {source_name} çš„æ—¥æœŸ,è·³éå‚™ä»½")
    
    print(f"\nâœ“ å…±å‚™ä»½ {backup_count} å€‹æª”æ¡ˆ")
    print("="*80 + "\n")
    
    # ========== æ­¥é©Ÿ 7.6ï¼šæ¸…ç† Excel åˆ†é  ==========
    print("\n" + "ğŸ“"*40)
    print("æ­¥é©Ÿ 7.6ï¼šæ¸…ç† Excel åˆ†é ï¼ˆåªä¿ç•™æœ€è¿‘äº¤æ˜“æ—¥ï¼‰")
    print("ğŸ“"*40 + "\n")
    
    # è™•ç†å¸¶æ—¥æœŸçš„ Excel æª”æ¡ˆ - ä½¿ç”¨å¾ Excel æå–çš„æ—¥æœŸ
    excel_files_to_clean = []
    if tse_date_str:
        excel_files_to_clean.append(f'tse_analysis_result_{tse_date_str}.xlsx')
    if otc_date_str:
        excel_files_to_clean.append(f'otc_analysis_result_{otc_date_str}.xlsx')

    
    cleaned_count = 0
    for excel_file in excel_files_to_clean:
        excel_path = os.path.join(stock_info_dir, excel_file)
        
        if os.path.exists(excel_path):
            print(f"è™•ç†æª”æ¡ˆ: {excel_file}")
            result = clean_excel_keep_second_sheet(excel_path)
            if result:
                cleaned_count += 1
                print()
        else:
            print(f"âŠ˜ æª”æ¡ˆä¸å­˜åœ¨: {excel_file}\n")
    
    print(f"âœ“ å…±è™•ç† {cleaned_count} å€‹ Excel æª”æ¡ˆ")
    print("="*80 + "\n")
    
    # ========== æ­¥é©Ÿ 8ï¼šè¤‡è£½åˆ° Repository ==========
    if args.copy_to_repo:
        copy_data_to_repo(base_dir, args.repo_data_dir)
    
    # ========== å®Œæˆ ==========
    print("\n" + "ğŸ‰"*40)
    print("æ‰€æœ‰æµç¨‹å·²å®Œæˆï¼")
    print("ğŸ‰"*40 + "\n")
    
    print("è™•ç†çµæœï¼š")
    if not args.skip_crawler:
        print("  âœ“ ä¸Šå¸‚/ä¸Šæ«ƒæ¯æ—¥äº¤æ˜“è³‡æ–™å·²æ›´æ–°")
        print("  âœ“ ä¸‰å¤§æ³•äººè²·è³£è¶…è³‡æ–™å·²æ›´æ–°")
    if not args.skip_analysis:
        if args.market in ['TSE', 'BOTH']:
            print("  âœ“ TSE åˆ†æå ±å‘Š (Excel) å·²ç”Ÿæˆ")
        if args.market in ['OTC', 'BOTH']:
            print("  âœ“ OTC åˆ†æå ±å‘Š (Excel) å·²ç”Ÿæˆ")
    if not args.skip_charts:
        if args.market in ['TSE', 'BOTH']:
            print("  âœ“ TSE æŠ€è¡“åˆ†æåœ–è¡¨ (HTML) å·²ç”Ÿæˆ")
        if args.market in ['OTC', 'BOTH']:
            print("  âœ“ OTC æŠ€è¡“åˆ†æåœ–è¡¨ (HTML) å·²ç”Ÿæˆ")
    print("\n" + "="*80)

if __name__ == "__main__":
    main()